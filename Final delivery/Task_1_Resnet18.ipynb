{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feYfIxWrOEiX",
        "outputId": "adeb67df-5da7-42dd-88f6-a43ba0a80ae4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXndySvRUejt"
      },
      "source": [
        "## Data Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGXpe-CKUjf9"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "from tqdm import *\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import torch\n",
        "from torchvision.models import resnet18, resnet34, resnet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-ia4Lr_UmdL"
      },
      "outputs": [],
      "source": [
        "data_root = '/content/drive/MyDrive/Dataset/Ham10000/ham10000'\n",
        "label_list = []\n",
        "label_to_image_path_list = {}\n",
        "label_to_int = {}\n",
        "int_to_label = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y3ydc6PUrGf",
        "outputId": "52b98bba-a46e-434c-9971-043795c48e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_to_int={'df': 0, 'akiec': 1, 'bcc': 2, 'bkl': 3, 'nv': 4, 'vasc': 5, 'mel': 6}\n",
            "int_to_label={0: 'df', 1: 'akiec', 2: 'bcc', 3: 'bkl', 4: 'nv', 5: 'vasc', 6: 'mel'}\n"
          ]
        }
      ],
      "source": [
        "# load meta data\n",
        "cnt = 0\n",
        "for f1 in os.listdir(data_root):\n",
        "    label = f1\n",
        "    label_to_int[label] = cnt\n",
        "    int_to_label[cnt] = label\n",
        "    cnt += 1\n",
        "    label_list.append(label)\n",
        "    f2 = os.path.join(data_root, f1)\n",
        "    label_to_image_path_list[label] = []\n",
        "    for f3 in os.listdir(f2):\n",
        "        f4 = os.path.join(f2, f3)\n",
        "        label_to_image_path_list[label].append(f4)\n",
        "\n",
        "print( 'label_to_int={0}'.format(label_to_int) )\n",
        "print( 'int_to_label={0}'.format(int_to_label) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kYqW8bJUuRf",
        "outputId": "9786ad24-36e3-4437-e83a-5120bd5b1cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 115/115 [00:10<00:00, 10.72it/s]\n",
            "100%|██████████| 327/327 [01:34<00:00,  3.46it/s]\n",
            "100%|██████████| 514/514 [02:40<00:00,  3.21it/s]\n",
            "100%|██████████| 1099/1099 [05:48<00:00,  3.15it/s]\n",
            "100%|██████████| 6705/6705 [37:06<00:00,  3.01it/s]\n",
            "100%|██████████| 142/142 [00:45<00:00,  3.15it/s]\n",
            "100%|██████████| 1113/1113 [06:28<00:00,  2.86it/s]\n"
          ]
        }
      ],
      "source": [
        "# load image to color images\n",
        "label_to_image_arr_list = {}\n",
        "for label in label_to_image_path_list:\n",
        "    label_to_image_arr_list[label] = []\n",
        "    image_path_list = label_to_image_path_list[label]\n",
        "    for image_path in tqdm(image_path_list):\n",
        "        img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "        label_to_image_arr_list[label].append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaMJqX-aUw0s",
        "outputId": "55c73673-1aef-4dde-8c08-9d8ce3532c71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df: loaded 115 images, shape=(450, 600, 3)\n",
            "akiec: loaded 327 images, shape=(450, 600, 3)\n",
            "bcc: loaded 514 images, shape=(450, 600, 3)\n",
            "bkl: loaded 1099 images, shape=(450, 600, 3)\n",
            "nv: loaded 6705 images, shape=(450, 600, 3)\n",
            "vasc: loaded 142 images, shape=(450, 600, 3)\n",
            "mel: loaded 1113 images, shape=(450, 600, 3)\n"
          ]
        }
      ],
      "source": [
        "# print load status\n",
        "for label in label_to_image_arr_list:\n",
        "    print('{0}: loaded {1} images, shape={2}'.format(label, len(label_to_image_arr_list[label]),\n",
        "                                                    label_to_image_arr_list[label][0].shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUKvlIfuUzAO",
        "outputId": "f724d593-0a21-4e1a-a348-c1f13e459d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_data.shape=(10015, 450, 600, 3), y_data.shape=(10015,)\n"
          ]
        }
      ],
      "source": [
        "# images to x_data, y_data\n",
        "x_data = []\n",
        "y_data = []\n",
        "for label in label_to_image_arr_list:\n",
        "    for img in label_to_image_arr_list[label]:\n",
        "        x_data.append( img )\n",
        "        y_data.append( label_to_int[label] )\n",
        "\n",
        "\n",
        "x_data = numpy.array(x_data)\n",
        "y_data = numpy.array(y_data)\n",
        "print( 'x_data.shape={0}, y_data.shape={1}'.format(x_data.shape, y_data.shape) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9Q7OP3NU5OO"
      },
      "source": [
        "## Resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbT-MFftU6RP"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "class_count = len(label_to_image_arr_list)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEYuTR2HVDT9"
      },
      "outputs": [],
      "source": [
        "model = resnet18().to(device)\n",
        "model.fc = torch.nn.Linear(512,class_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPwQCZodi6zx"
      },
      "outputs": [],
      "source": [
        "# loss and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer_function = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e-nMQsEi7jF"
      },
      "outputs": [],
      "source": [
        "n_channel = x_data.shape[-1]\n",
        "n_count = x_data.shape[0]\n",
        "width = x_data.shape[1]\n",
        "height = x_data.shape[2]\n",
        "\n",
        "x_data = x_data.reshape( (n_count,n_channel,width,height)  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLLCBzNXPXNE"
      },
      "source": [
        "## The training and test sets are split in the ratio of 8:2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME5N_Ow4jLZ_"
      },
      "outputs": [],
      "source": [
        "# train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV3vsK6ljQhq",
        "outputId": "04490d49-2b08-48df-da0a-0cbb49b9b47f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train=(8012, 3, 450, 600), x_test=(2003, 3, 450, 600), y_train=(8012,), y_test=(2003,)\n"
          ]
        }
      ],
      "source": [
        "# output\n",
        "print('x_train={0}, x_test={1}, y_train={2}, y_test={3}'.format(\n",
        "    x_train.shape, x_test.shape, y_train.shape, y_test.shape\n",
        ") )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs-0EIjqjYGJ"
      },
      "outputs": [],
      "source": [
        "def get_acc(y_pred, y_true):\n",
        "    asum = 0\n",
        "    for i in range(len(y_pred)):\n",
        "        if y_pred[i]==y_true[i]:\n",
        "            asum += 1\n",
        "    return asum/len(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcXX3Sf5FfbR",
        "outputId": "9af33349-af85-4a81-fc04-19b705423fdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "completed batch 103 of epoch 116. loss=0.04286640137434006. train batch time cost=0.10180425643920898s\n",
            "completed batch 104 of epoch 116. loss=0.015677202492952347. train batch time cost=0.10231566429138184s\n",
            "completed batch 105 of epoch 116. loss=0.023114468902349472. train batch time cost=0.10125350952148438s\n",
            "completed batch 106 of epoch 116. loss=0.08870118111371994. train batch time cost=0.10152649879455566s\n",
            "completed batch 107 of epoch 116. loss=0.023600000888109207. train batch time cost=0.10115838050842285s\n",
            "completed batch 108 of epoch 116. loss=0.0714862123131752. train batch time cost=0.10099053382873535s\n",
            "completed batch 109 of epoch 116. loss=0.03860117122530937. train batch time cost=0.10059261322021484s\n",
            "completed batch 110 of epoch 116. loss=0.06232021003961563. train batch time cost=0.09921669960021973s\n",
            "completed batch 111 of epoch 116. loss=0.020567530766129494. train batch time cost=0.10095906257629395s\n",
            "completed batch 112 of epoch 116. loss=0.09391249716281891. train batch time cost=0.10203933715820312s\n",
            "completed batch 113 of epoch 116. loss=0.008476227521896362. train batch time cost=0.10264396667480469s\n",
            "completed batch 114 of epoch 116. loss=0.012814021669328213. train batch time cost=0.09620881080627441s\n",
            "completed batch 115 of epoch 116. loss=0.049585118889808655. train batch time cost=0.10157251358032227s\n",
            "completed batch 116 of epoch 116. loss=0.06315258145332336. train batch time cost=0.10138845443725586s\n",
            "completed batch 117 of epoch 116. loss=0.039610374718904495. train batch time cost=0.10011768341064453s\n",
            "completed batch 118 of epoch 116. loss=0.03198200464248657. train batch time cost=0.10137724876403809s\n",
            "completed batch 119 of epoch 116. loss=0.02252364531159401. train batch time cost=0.10136628150939941s\n",
            "completed batch 120 of epoch 116. loss=0.037980541586875916. train batch time cost=0.0997309684753418s\n",
            "completed batch 121 of epoch 116. loss=0.016377827152609825. train batch time cost=0.10072493553161621s\n",
            "completed batch 122 of epoch 116. loss=0.03610309213399887. train batch time cost=0.10083842277526855s\n",
            "completed batch 123 of epoch 116. loss=0.05090758204460144. train batch time cost=0.10077166557312012s\n",
            "completed batch 124 of epoch 116. loss=0.23062770068645477. train batch time cost=0.10020947456359863s\n",
            "completed batch 125 of epoch 116. loss=0.01529126800596714. train batch time cost=0.10146856307983398s\n",
            "completed batch 126 of epoch 116. loss=0.0002860173990484327. train batch time cost=0.02935504913330078s\n",
            "completed test of epoch 116. loss=0.0002860173990484327. accuracy=0.7024463305042437. train one epoch time cost=27.541008710861206s, test validation time cost=3.8970932960510254\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437]\n",
            "completed batch 1 of epoch 117. loss=0.49083369970321655. train batch time cost=0.10702323913574219s\n",
            "completed batch 2 of epoch 117. loss=0.04715126380324364. train batch time cost=0.10203909873962402s\n",
            "completed batch 3 of epoch 117. loss=0.02921151928603649. train batch time cost=0.10266566276550293s\n",
            "completed batch 4 of epoch 117. loss=0.024039724841713905. train batch time cost=0.10258674621582031s\n",
            "completed batch 5 of epoch 117. loss=0.023521484807133675. train batch time cost=0.09457921981811523s\n",
            "completed batch 6 of epoch 117. loss=0.07206173241138458. train batch time cost=0.09450244903564453s\n",
            "completed batch 7 of epoch 117. loss=0.18016678094863892. train batch time cost=0.09523725509643555s\n",
            "completed batch 8 of epoch 117. loss=0.04902014136314392. train batch time cost=0.0950460433959961s\n",
            "completed batch 9 of epoch 117. loss=0.11433520913124084. train batch time cost=0.09452033042907715s\n",
            "completed batch 10 of epoch 117. loss=0.02822986990213394. train batch time cost=0.09574103355407715s\n",
            "completed batch 11 of epoch 117. loss=0.02494504302740097. train batch time cost=0.09613204002380371s\n",
            "completed batch 12 of epoch 117. loss=0.13354866206645966. train batch time cost=0.0955359935760498s\n",
            "completed batch 13 of epoch 117. loss=0.02064059115946293. train batch time cost=0.09586000442504883s\n",
            "completed batch 14 of epoch 117. loss=0.07684478908777237. train batch time cost=0.09642505645751953s\n",
            "completed batch 15 of epoch 117. loss=0.08557294309139252. train batch time cost=0.09779953956604004s\n",
            "completed batch 16 of epoch 117. loss=0.033446572721004486. train batch time cost=0.09512448310852051s\n",
            "completed batch 17 of epoch 117. loss=0.029446003958582878. train batch time cost=0.09556293487548828s\n",
            "completed batch 18 of epoch 117. loss=0.018962105736136436. train batch time cost=0.09594607353210449s\n",
            "completed batch 19 of epoch 117. loss=0.03480422869324684. train batch time cost=0.09914159774780273s\n",
            "completed batch 20 of epoch 117. loss=0.03558879345655441. train batch time cost=0.09917020797729492s\n",
            "completed batch 21 of epoch 117. loss=0.059286929666996. train batch time cost=0.09414529800415039s\n",
            "completed batch 22 of epoch 117. loss=0.00464462349191308. train batch time cost=0.09656286239624023s\n",
            "completed batch 23 of epoch 117. loss=0.04315916821360588. train batch time cost=0.09833765029907227s\n",
            "completed batch 24 of epoch 117. loss=0.005752574652433395. train batch time cost=0.0982198715209961s\n",
            "completed batch 25 of epoch 117. loss=0.0066282618790864944. train batch time cost=0.09927630424499512s\n",
            "completed batch 26 of epoch 117. loss=0.02143951691687107. train batch time cost=0.09880518913269043s\n",
            "completed batch 27 of epoch 117. loss=0.007906759157776833. train batch time cost=0.09822678565979004s\n",
            "completed batch 28 of epoch 117. loss=0.10454008728265762. train batch time cost=0.09710311889648438s\n",
            "completed batch 29 of epoch 117. loss=0.03550494834780693. train batch time cost=0.09954380989074707s\n",
            "completed batch 30 of epoch 117. loss=0.001770602772012353. train batch time cost=0.09855532646179199s\n",
            "completed batch 31 of epoch 117. loss=0.05926225706934929. train batch time cost=0.10286569595336914s\n",
            "completed batch 32 of epoch 117. loss=0.025846082717180252. train batch time cost=0.10196280479431152s\n",
            "completed batch 33 of epoch 117. loss=0.023863762617111206. train batch time cost=0.10190677642822266s\n",
            "completed batch 34 of epoch 117. loss=0.08805523067712784. train batch time cost=0.10145926475524902s\n",
            "completed batch 35 of epoch 117. loss=0.0589735247194767. train batch time cost=0.10086727142333984s\n",
            "completed batch 36 of epoch 117. loss=0.005813105497509241. train batch time cost=0.1020195484161377s\n",
            "completed batch 37 of epoch 117. loss=0.021856898441910744. train batch time cost=0.10085296630859375s\n",
            "completed batch 38 of epoch 117. loss=0.009240754880011082. train batch time cost=0.10034990310668945s\n",
            "completed batch 39 of epoch 117. loss=0.043679092079401016. train batch time cost=0.10132026672363281s\n",
            "completed batch 40 of epoch 117. loss=0.010035242885351181. train batch time cost=0.1004629135131836s\n",
            "completed batch 41 of epoch 117. loss=0.029319308698177338. train batch time cost=0.10241889953613281s\n",
            "completed batch 42 of epoch 117. loss=0.0641578733921051. train batch time cost=0.10047411918640137s\n",
            "completed batch 43 of epoch 117. loss=0.01503440085798502. train batch time cost=0.10049891471862793s\n",
            "completed batch 44 of epoch 117. loss=0.0042229206301271915. train batch time cost=0.09946417808532715s\n",
            "completed batch 45 of epoch 117. loss=0.05983354151248932. train batch time cost=0.10057282447814941s\n",
            "completed batch 46 of epoch 117. loss=0.010454511269927025. train batch time cost=0.10099411010742188s\n",
            "completed batch 47 of epoch 117. loss=0.028102178126573563. train batch time cost=0.10097885131835938s\n",
            "completed batch 48 of epoch 117. loss=0.06089288741350174. train batch time cost=0.09970211982727051s\n",
            "completed batch 49 of epoch 117. loss=0.04188533499836922. train batch time cost=0.09969830513000488s\n",
            "completed batch 50 of epoch 117. loss=0.09455867856740952. train batch time cost=0.1005258560180664s\n",
            "completed batch 51 of epoch 117. loss=0.015220594592392445. train batch time cost=0.1005558967590332s\n",
            "completed batch 52 of epoch 117. loss=0.09285460412502289. train batch time cost=0.10016942024230957s\n",
            "completed batch 53 of epoch 117. loss=0.03546462580561638. train batch time cost=0.10085248947143555s\n",
            "completed batch 54 of epoch 117. loss=0.05496829375624657. train batch time cost=0.1015937328338623s\n",
            "completed batch 55 of epoch 117. loss=0.0763266533613205. train batch time cost=0.10162925720214844s\n",
            "completed batch 56 of epoch 117. loss=0.02744077891111374. train batch time cost=0.10010361671447754s\n",
            "completed batch 57 of epoch 117. loss=0.017309824004769325. train batch time cost=0.10169100761413574s\n",
            "completed batch 58 of epoch 117. loss=0.12661057710647583. train batch time cost=0.10138797760009766s\n",
            "completed batch 59 of epoch 117. loss=0.06679444015026093. train batch time cost=0.10700058937072754s\n",
            "completed batch 60 of epoch 117. loss=0.08455808460712433. train batch time cost=0.10095691680908203s\n",
            "completed batch 61 of epoch 117. loss=0.01165673229843378. train batch time cost=0.10180926322937012s\n",
            "completed batch 62 of epoch 117. loss=0.009380458854138851. train batch time cost=0.10126614570617676s\n",
            "completed batch 63 of epoch 117. loss=0.027734115719795227. train batch time cost=0.10085082054138184s\n",
            "completed batch 64 of epoch 117. loss=0.0158150102943182. train batch time cost=0.10167574882507324s\n",
            "completed batch 65 of epoch 117. loss=0.006040411535650492. train batch time cost=0.10037851333618164s\n",
            "completed batch 66 of epoch 117. loss=0.036521200090646744. train batch time cost=0.10074424743652344s\n",
            "completed batch 67 of epoch 117. loss=0.006620641332119703. train batch time cost=0.10150551795959473s\n",
            "completed batch 68 of epoch 117. loss=0.016833005473017693. train batch time cost=0.10062932968139648s\n",
            "completed batch 69 of epoch 117. loss=0.029424874112010002. train batch time cost=0.10043883323669434s\n",
            "completed batch 70 of epoch 117. loss=0.036786794662475586. train batch time cost=0.09480905532836914s\n",
            "completed batch 71 of epoch 117. loss=0.025817716494202614. train batch time cost=0.0958409309387207s\n",
            "completed batch 72 of epoch 117. loss=0.013298152945935726. train batch time cost=0.09489679336547852s\n",
            "completed batch 73 of epoch 117. loss=0.00643988186493516. train batch time cost=0.09622311592102051s\n",
            "completed batch 74 of epoch 117. loss=0.04180219769477844. train batch time cost=0.1009058952331543s\n",
            "completed batch 75 of epoch 117. loss=0.04314052313566208. train batch time cost=0.10076451301574707s\n",
            "completed batch 76 of epoch 117. loss=0.05120978504419327. train batch time cost=0.10130643844604492s\n",
            "completed batch 77 of epoch 117. loss=0.0032858795020729303. train batch time cost=0.10282111167907715s\n",
            "completed batch 78 of epoch 117. loss=0.008634149096906185. train batch time cost=0.10173964500427246s\n",
            "completed batch 79 of epoch 117. loss=0.05018558353185654. train batch time cost=0.10017180442810059s\n",
            "completed batch 80 of epoch 117. loss=0.08676307648420334. train batch time cost=0.09992194175720215s\n",
            "completed batch 81 of epoch 117. loss=0.027004478499293327. train batch time cost=0.1003730297088623s\n",
            "completed batch 82 of epoch 117. loss=0.054234813898801804. train batch time cost=0.1024930477142334s\n",
            "completed batch 83 of epoch 117. loss=0.015067540109157562. train batch time cost=0.10122203826904297s\n",
            "completed batch 84 of epoch 117. loss=0.004227498080581427. train batch time cost=0.10284185409545898s\n",
            "completed batch 85 of epoch 117. loss=0.014197981916368008. train batch time cost=0.10445094108581543s\n",
            "completed batch 86 of epoch 117. loss=0.04344555363059044. train batch time cost=0.10251617431640625s\n",
            "completed batch 87 of epoch 117. loss=0.02522467076778412. train batch time cost=0.10273504257202148s\n",
            "completed batch 88 of epoch 117. loss=0.0488487146794796. train batch time cost=0.10237264633178711s\n",
            "completed batch 89 of epoch 117. loss=0.0601779967546463. train batch time cost=0.10120630264282227s\n",
            "completed batch 90 of epoch 117. loss=0.005664460361003876. train batch time cost=0.1019589900970459s\n",
            "completed batch 91 of epoch 117. loss=0.025471989065408707. train batch time cost=0.10106134414672852s\n",
            "completed batch 92 of epoch 117. loss=0.007605494931340218. train batch time cost=0.10201907157897949s\n",
            "completed batch 93 of epoch 117. loss=0.027817821130156517. train batch time cost=0.09981942176818848s\n",
            "completed batch 94 of epoch 117. loss=0.06941334158182144. train batch time cost=0.10051703453063965s\n",
            "completed batch 95 of epoch 117. loss=0.022597508504986763. train batch time cost=0.10103821754455566s\n",
            "completed batch 96 of epoch 117. loss=0.11188746988773346. train batch time cost=0.10456013679504395s\n",
            "completed batch 97 of epoch 117. loss=0.0038913257885724306. train batch time cost=0.09514403343200684s\n",
            "completed batch 98 of epoch 117. loss=0.024075258523225784. train batch time cost=0.10082554817199707s\n",
            "completed batch 99 of epoch 117. loss=0.08241697400808334. train batch time cost=0.10072946548461914s\n",
            "completed batch 100 of epoch 117. loss=0.011852964758872986. train batch time cost=0.10291671752929688s\n",
            "completed batch 101 of epoch 117. loss=0.04683062806725502. train batch time cost=0.10178780555725098s\n",
            "completed batch 102 of epoch 117. loss=0.054050758481025696. train batch time cost=0.09501528739929199s\n",
            "completed batch 103 of epoch 117. loss=0.03209209069609642. train batch time cost=0.09587550163269043s\n",
            "completed batch 104 of epoch 117. loss=0.0033626863732934. train batch time cost=0.09636902809143066s\n",
            "completed batch 105 of epoch 117. loss=0.025841763243079185. train batch time cost=0.10177111625671387s\n",
            "completed batch 106 of epoch 117. loss=0.11111585050821304. train batch time cost=0.10265707969665527s\n",
            "completed batch 107 of epoch 117. loss=0.020754778757691383. train batch time cost=0.10197901725769043s\n",
            "completed batch 108 of epoch 117. loss=0.03618350252509117. train batch time cost=0.10375070571899414s\n",
            "completed batch 109 of epoch 117. loss=0.01007577870041132. train batch time cost=0.10131621360778809s\n",
            "completed batch 110 of epoch 117. loss=0.03391491249203682. train batch time cost=0.10065031051635742s\n",
            "completed batch 111 of epoch 117. loss=0.006119303405284882. train batch time cost=0.10106730461120605s\n",
            "completed batch 112 of epoch 117. loss=0.02690507471561432. train batch time cost=0.10133695602416992s\n",
            "completed batch 113 of epoch 117. loss=0.00993824377655983. train batch time cost=0.10155487060546875s\n",
            "completed batch 114 of epoch 117. loss=0.2324715554714203. train batch time cost=0.10186553001403809s\n",
            "completed batch 115 of epoch 117. loss=0.07012993097305298. train batch time cost=0.10233068466186523s\n",
            "completed batch 116 of epoch 117. loss=0.0158182755112648. train batch time cost=0.09519433975219727s\n",
            "completed batch 117 of epoch 117. loss=0.002557209460064769. train batch time cost=0.09590268135070801s\n",
            "completed batch 118 of epoch 117. loss=0.011922851204872131. train batch time cost=0.0959622859954834s\n",
            "completed batch 119 of epoch 117. loss=0.013796997256577015. train batch time cost=0.09469127655029297s\n",
            "completed batch 120 of epoch 117. loss=0.030114714056253433. train batch time cost=0.09620499610900879s\n",
            "completed batch 121 of epoch 117. loss=0.03956313803792. train batch time cost=0.1012265682220459s\n",
            "completed batch 122 of epoch 117. loss=0.01188951451331377. train batch time cost=0.1021583080291748s\n",
            "completed batch 123 of epoch 117. loss=0.03427806869149208. train batch time cost=0.10234618186950684s\n",
            "completed batch 124 of epoch 117. loss=0.11963571608066559. train batch time cost=0.11008667945861816s\n",
            "completed batch 125 of epoch 117. loss=0.015249695628881454. train batch time cost=0.0999290943145752s\n",
            "completed batch 126 of epoch 117. loss=0.00015764065028633922. train batch time cost=0.03059101104736328s\n",
            "completed test of epoch 117. loss=0.00015764065028633922. accuracy=0.7219171243135297. train one epoch time cost=27.43474316596985s, test validation time cost=3.956444025039673\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297]\n",
            "completed batch 1 of epoch 118. loss=0.04817052185535431. train batch time cost=0.10119795799255371s\n",
            "completed batch 2 of epoch 118. loss=0.07650952786207199. train batch time cost=0.10292482376098633s\n",
            "completed batch 3 of epoch 118. loss=0.05159720778465271. train batch time cost=0.10271906852722168s\n",
            "completed batch 4 of epoch 118. loss=0.040336724370718. train batch time cost=0.10129117965698242s\n",
            "completed batch 5 of epoch 118. loss=0.026275943964719772. train batch time cost=0.10026979446411133s\n",
            "completed batch 6 of epoch 118. loss=0.0054108272306621075. train batch time cost=0.10080552101135254s\n",
            "completed batch 7 of epoch 118. loss=0.034681107848882675. train batch time cost=0.10120701789855957s\n",
            "completed batch 8 of epoch 118. loss=0.0501885823905468. train batch time cost=0.10071158409118652s\n",
            "completed batch 9 of epoch 118. loss=0.04623189568519592. train batch time cost=0.10184216499328613s\n",
            "completed batch 10 of epoch 118. loss=0.0026742503978312016. train batch time cost=0.10092616081237793s\n",
            "completed batch 11 of epoch 118. loss=0.020334729924798012. train batch time cost=0.10071992874145508s\n",
            "completed batch 12 of epoch 118. loss=0.09757421910762787. train batch time cost=0.10394644737243652s\n",
            "completed batch 13 of epoch 118. loss=0.03850092738866806. train batch time cost=0.1022493839263916s\n",
            "completed batch 14 of epoch 118. loss=0.017659636214375496. train batch time cost=0.10100579261779785s\n",
            "completed batch 15 of epoch 118. loss=0.012945192866027355. train batch time cost=0.10126447677612305s\n",
            "completed batch 16 of epoch 118. loss=0.012169014662504196. train batch time cost=0.10164046287536621s\n",
            "completed batch 17 of epoch 118. loss=0.011713238433003426. train batch time cost=0.10158848762512207s\n",
            "completed batch 18 of epoch 118. loss=0.03167114034295082. train batch time cost=0.10364413261413574s\n",
            "completed batch 19 of epoch 118. loss=0.00919403787702322. train batch time cost=0.10152506828308105s\n",
            "completed batch 20 of epoch 118. loss=0.07639092952013016. train batch time cost=0.10300612449645996s\n",
            "completed batch 21 of epoch 118. loss=0.010654589161276817. train batch time cost=0.10321187973022461s\n",
            "completed batch 22 of epoch 118. loss=0.00193612405564636. train batch time cost=0.10291028022766113s\n",
            "completed batch 23 of epoch 118. loss=0.1143670305609703. train batch time cost=0.10094857215881348s\n",
            "completed batch 24 of epoch 118. loss=0.041478078812360764. train batch time cost=0.10155963897705078s\n",
            "completed batch 25 of epoch 118. loss=0.009951659478247166. train batch time cost=0.10253381729125977s\n",
            "completed batch 26 of epoch 118. loss=0.013620284385979176. train batch time cost=0.10347342491149902s\n",
            "completed batch 27 of epoch 118. loss=0.010969975031912327. train batch time cost=0.10413765907287598s\n",
            "completed batch 28 of epoch 118. loss=0.025608154013752937. train batch time cost=0.1006777286529541s\n",
            "completed batch 29 of epoch 118. loss=0.007956219837069511. train batch time cost=0.10061240196228027s\n",
            "completed batch 30 of epoch 118. loss=0.0022940482012927532. train batch time cost=0.10123157501220703s\n",
            "completed batch 31 of epoch 118. loss=0.00460366765037179. train batch time cost=0.10196566581726074s\n",
            "completed batch 32 of epoch 118. loss=0.0115797258913517. train batch time cost=0.10185122489929199s\n",
            "completed batch 33 of epoch 118. loss=0.01264052651822567. train batch time cost=0.10190868377685547s\n",
            "completed batch 34 of epoch 118. loss=0.010190920904278755. train batch time cost=0.1029355525970459s\n",
            "completed batch 35 of epoch 118. loss=0.036616262048482895. train batch time cost=0.10206866264343262s\n",
            "completed batch 36 of epoch 118. loss=0.017077839002013206. train batch time cost=0.10095977783203125s\n",
            "completed batch 37 of epoch 118. loss=0.031684376299381256. train batch time cost=0.1008448600769043s\n",
            "completed batch 38 of epoch 118. loss=0.013217323459684849. train batch time cost=0.10146570205688477s\n",
            "completed batch 39 of epoch 118. loss=0.08077733218669891. train batch time cost=0.10119390487670898s\n",
            "completed batch 40 of epoch 118. loss=0.03770298883318901. train batch time cost=0.10072755813598633s\n",
            "completed batch 41 of epoch 118. loss=0.06867820024490356. train batch time cost=0.10147380828857422s\n",
            "completed batch 42 of epoch 118. loss=0.031082119792699814. train batch time cost=0.10050463676452637s\n",
            "completed batch 43 of epoch 118. loss=0.025209223851561546. train batch time cost=0.10130453109741211s\n",
            "completed batch 44 of epoch 118. loss=0.0028928357642143965. train batch time cost=0.1017918586730957s\n",
            "completed batch 45 of epoch 118. loss=0.03267649561166763. train batch time cost=0.10176205635070801s\n",
            "completed batch 46 of epoch 118. loss=0.30251601338386536. train batch time cost=0.1012887954711914s\n",
            "completed batch 47 of epoch 118. loss=0.011751176789402962. train batch time cost=0.10261416435241699s\n",
            "completed batch 48 of epoch 118. loss=0.004166533704847097. train batch time cost=0.10204529762268066s\n",
            "completed batch 49 of epoch 118. loss=0.003977607935667038. train batch time cost=0.10130095481872559s\n",
            "completed batch 50 of epoch 118. loss=0.046734366565942764. train batch time cost=0.1001732349395752s\n",
            "completed batch 51 of epoch 118. loss=0.06616799533367157. train batch time cost=0.10077595710754395s\n",
            "completed batch 52 of epoch 118. loss=0.04047596454620361. train batch time cost=0.10133957862854004s\n",
            "completed batch 53 of epoch 118. loss=0.006796820554882288. train batch time cost=0.10379409790039062s\n",
            "completed batch 54 of epoch 118. loss=0.14236924052238464. train batch time cost=0.10317301750183105s\n",
            "completed batch 55 of epoch 118. loss=0.03398745507001877. train batch time cost=0.10329604148864746s\n",
            "completed batch 56 of epoch 118. loss=0.028839513659477234. train batch time cost=0.1027534008026123s\n",
            "completed batch 57 of epoch 118. loss=0.04853285849094391. train batch time cost=0.10321164131164551s\n",
            "completed batch 58 of epoch 118. loss=0.01735200732946396. train batch time cost=0.10357093811035156s\n",
            "completed batch 59 of epoch 118. loss=0.005138542503118515. train batch time cost=0.10177779197692871s\n",
            "completed batch 60 of epoch 118. loss=0.049601826816797256. train batch time cost=0.10269331932067871s\n",
            "completed batch 61 of epoch 118. loss=0.019329166039824486. train batch time cost=0.10283899307250977s\n",
            "completed batch 62 of epoch 118. loss=0.06468560546636581. train batch time cost=0.1014399528503418s\n",
            "completed batch 63 of epoch 118. loss=0.003468517679721117. train batch time cost=0.10145354270935059s\n",
            "completed batch 64 of epoch 118. loss=0.019374046474695206. train batch time cost=0.0999596118927002s\n",
            "completed batch 65 of epoch 118. loss=0.02709447778761387. train batch time cost=0.09974098205566406s\n",
            "completed batch 66 of epoch 118. loss=0.12527458369731903. train batch time cost=0.10494446754455566s\n",
            "completed batch 67 of epoch 118. loss=0.009069413878023624. train batch time cost=0.10281729698181152s\n",
            "completed batch 68 of epoch 118. loss=0.023103566840291023. train batch time cost=0.10177397727966309s\n",
            "completed batch 69 of epoch 118. loss=0.003149335505440831. train batch time cost=0.10085105895996094s\n",
            "completed batch 70 of epoch 118. loss=0.030995234847068787. train batch time cost=0.10144639015197754s\n",
            "completed batch 71 of epoch 118. loss=0.03388410061597824. train batch time cost=0.10193514823913574s\n",
            "completed batch 72 of epoch 118. loss=0.10058794915676117. train batch time cost=0.10203385353088379s\n",
            "completed batch 73 of epoch 118. loss=0.00597363943234086. train batch time cost=0.1007537841796875s\n",
            "completed batch 74 of epoch 118. loss=0.10940467566251755. train batch time cost=0.10134553909301758s\n",
            "completed batch 75 of epoch 118. loss=0.026128239929676056. train batch time cost=0.10172080993652344s\n",
            "completed batch 76 of epoch 118. loss=0.03579080477356911. train batch time cost=0.10241508483886719s\n",
            "completed batch 77 of epoch 118. loss=0.010473433881998062. train batch time cost=0.10136008262634277s\n",
            "completed batch 78 of epoch 118. loss=0.035997845232486725. train batch time cost=0.10006880760192871s\n",
            "completed batch 79 of epoch 118. loss=0.043327029794454575. train batch time cost=0.10280990600585938s\n",
            "completed batch 80 of epoch 118. loss=0.02717261202633381. train batch time cost=0.09711408615112305s\n",
            "completed batch 81 of epoch 118. loss=0.11497853696346283. train batch time cost=0.10257697105407715s\n",
            "completed batch 82 of epoch 118. loss=0.02976679615676403. train batch time cost=0.10204267501831055s\n",
            "completed batch 83 of epoch 118. loss=0.0952962338924408. train batch time cost=0.1040036678314209s\n",
            "completed batch 84 of epoch 118. loss=0.007392754312604666. train batch time cost=0.10114431381225586s\n",
            "completed batch 85 of epoch 118. loss=0.01933867111802101. train batch time cost=0.10198783874511719s\n",
            "completed batch 86 of epoch 118. loss=0.05348389595746994. train batch time cost=0.10075235366821289s\n",
            "completed batch 87 of epoch 118. loss=0.08019935339689255. train batch time cost=0.10029721260070801s\n",
            "completed batch 88 of epoch 118. loss=0.009783395566046238. train batch time cost=0.09520983695983887s\n",
            "completed batch 89 of epoch 118. loss=0.02432878687977791. train batch time cost=0.09820961952209473s\n",
            "completed batch 90 of epoch 118. loss=0.02350246161222458. train batch time cost=0.0950937271118164s\n",
            "completed batch 91 of epoch 118. loss=0.04866042733192444. train batch time cost=0.09429335594177246s\n",
            "completed batch 92 of epoch 118. loss=0.01876160316169262. train batch time cost=0.09497594833374023s\n",
            "completed batch 93 of epoch 118. loss=0.04104023054242134. train batch time cost=0.09511017799377441s\n",
            "completed batch 94 of epoch 118. loss=0.022680120542645454. train batch time cost=0.0962677001953125s\n",
            "completed batch 95 of epoch 118. loss=0.08093378692865372. train batch time cost=0.09502959251403809s\n",
            "completed batch 96 of epoch 118. loss=0.06158532202243805. train batch time cost=0.09463119506835938s\n",
            "completed batch 97 of epoch 118. loss=0.022724471986293793. train batch time cost=0.09990310668945312s\n",
            "completed batch 98 of epoch 118. loss=0.024215098470449448. train batch time cost=0.10063457489013672s\n",
            "completed batch 99 of epoch 118. loss=0.008346110582351685. train batch time cost=0.10171723365783691s\n",
            "completed batch 100 of epoch 118. loss=0.0071944971568882465. train batch time cost=0.09995198249816895s\n",
            "completed batch 101 of epoch 118. loss=0.028087304905056953. train batch time cost=0.09968018531799316s\n",
            "completed batch 102 of epoch 118. loss=0.016645966097712517. train batch time cost=0.1029503345489502s\n",
            "completed batch 103 of epoch 118. loss=0.04039893299341202. train batch time cost=0.10122466087341309s\n",
            "completed batch 104 of epoch 118. loss=0.03505942225456238. train batch time cost=0.10263323783874512s\n",
            "completed batch 105 of epoch 118. loss=0.03170206770300865. train batch time cost=0.1012411117553711s\n",
            "completed batch 106 of epoch 118. loss=0.022845299914479256. train batch time cost=0.10070109367370605s\n",
            "completed batch 107 of epoch 118. loss=0.015600262209773064. train batch time cost=0.10197186470031738s\n",
            "completed batch 108 of epoch 118. loss=0.04591664671897888. train batch time cost=0.1027688980102539s\n",
            "completed batch 109 of epoch 118. loss=0.031900014728307724. train batch time cost=0.10228204727172852s\n",
            "completed batch 110 of epoch 118. loss=0.04075217247009277. train batch time cost=0.1025998592376709s\n",
            "completed batch 111 of epoch 118. loss=0.012746691703796387. train batch time cost=0.102935791015625s\n",
            "completed batch 112 of epoch 118. loss=0.07406947016716003. train batch time cost=0.10564780235290527s\n",
            "completed batch 113 of epoch 118. loss=0.01110529899597168. train batch time cost=0.10363435745239258s\n",
            "completed batch 114 of epoch 118. loss=0.0063636209815740585. train batch time cost=0.10343718528747559s\n",
            "completed batch 115 of epoch 118. loss=0.027091337367892265. train batch time cost=0.1013340950012207s\n",
            "completed batch 116 of epoch 118. loss=0.03926735371351242. train batch time cost=0.1016232967376709s\n",
            "completed batch 117 of epoch 118. loss=0.00830839853733778. train batch time cost=0.1016535758972168s\n",
            "completed batch 118 of epoch 118. loss=0.08621080219745636. train batch time cost=0.10037040710449219s\n",
            "completed batch 119 of epoch 118. loss=0.0031605372205376625. train batch time cost=0.09976911544799805s\n",
            "completed batch 120 of epoch 118. loss=0.016334140673279762. train batch time cost=0.10000920295715332s\n",
            "completed batch 121 of epoch 118. loss=0.004838488530367613. train batch time cost=0.10204720497131348s\n",
            "completed batch 122 of epoch 118. loss=0.008382727392017841. train batch time cost=0.10132312774658203s\n",
            "completed batch 123 of epoch 118. loss=0.025408416986465454. train batch time cost=0.10106277465820312s\n",
            "completed batch 124 of epoch 118. loss=0.009160882793366909. train batch time cost=0.10080385208129883s\n",
            "completed batch 125 of epoch 118. loss=0.04521684721112251. train batch time cost=0.10165953636169434s\n",
            "completed batch 126 of epoch 118. loss=0.00014489352179225534. train batch time cost=0.030734777450561523s\n",
            "completed test of epoch 118. loss=0.00014489352179225534. accuracy=0.7254118821767349. train one epoch time cost=27.608587980270386s, test validation time cost=3.8310317993164062\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349]\n",
            "completed batch 1 of epoch 119. loss=0.010051261633634567. train batch time cost=0.10238957405090332s\n",
            "completed batch 2 of epoch 119. loss=0.02040804736316204. train batch time cost=0.10309791564941406s\n",
            "completed batch 3 of epoch 119. loss=0.007892042398452759. train batch time cost=0.10238456726074219s\n",
            "completed batch 4 of epoch 119. loss=0.05859318748116493. train batch time cost=0.10191583633422852s\n",
            "completed batch 5 of epoch 119. loss=0.037541866302490234. train batch time cost=0.10233402252197266s\n",
            "completed batch 6 of epoch 119. loss=0.010898306965827942. train batch time cost=0.10230350494384766s\n",
            "completed batch 7 of epoch 119. loss=0.01978524774312973. train batch time cost=0.102325439453125s\n",
            "completed batch 8 of epoch 119. loss=0.013818048872053623. train batch time cost=0.10155224800109863s\n",
            "completed batch 9 of epoch 119. loss=0.04014933481812477. train batch time cost=0.1003568172454834s\n",
            "completed batch 10 of epoch 119. loss=0.0050389752723276615. train batch time cost=0.10239768028259277s\n",
            "completed batch 11 of epoch 119. loss=0.06297410279512405. train batch time cost=0.10111665725708008s\n",
            "completed batch 12 of epoch 119. loss=0.004815202206373215. train batch time cost=0.10233211517333984s\n",
            "completed batch 13 of epoch 119. loss=0.011270230636000633. train batch time cost=0.10097074508666992s\n",
            "completed batch 14 of epoch 119. loss=0.010850168764591217. train batch time cost=0.10142993927001953s\n",
            "completed batch 15 of epoch 119. loss=0.018932640552520752. train batch time cost=0.10072612762451172s\n",
            "completed batch 16 of epoch 119. loss=0.03536733612418175. train batch time cost=0.10138249397277832s\n",
            "completed batch 17 of epoch 119. loss=0.006361193489283323. train batch time cost=0.10213756561279297s\n",
            "completed batch 18 of epoch 119. loss=0.009578077122569084. train batch time cost=0.10223007202148438s\n",
            "completed batch 19 of epoch 119. loss=0.010938080959022045. train batch time cost=0.10128641128540039s\n",
            "completed batch 20 of epoch 119. loss=0.006783817894756794. train batch time cost=0.10153436660766602s\n",
            "completed batch 21 of epoch 119. loss=0.012770096771419048. train batch time cost=0.1000218391418457s\n",
            "completed batch 22 of epoch 119. loss=0.013230127282440662. train batch time cost=0.09957647323608398s\n",
            "completed batch 23 of epoch 119. loss=0.011087260209023952. train batch time cost=0.10226273536682129s\n",
            "completed batch 24 of epoch 119. loss=0.027583111077547073. train batch time cost=0.10335969924926758s\n",
            "completed batch 25 of epoch 119. loss=0.030331503599882126. train batch time cost=0.1033179759979248s\n",
            "completed batch 26 of epoch 119. loss=0.005467486567795277. train batch time cost=0.10087203979492188s\n",
            "completed batch 27 of epoch 119. loss=0.012104232795536518. train batch time cost=0.1002492904663086s\n",
            "completed batch 28 of epoch 119. loss=0.01958729885518551. train batch time cost=0.10183143615722656s\n",
            "completed batch 29 of epoch 119. loss=0.006152668036520481. train batch time cost=0.10059785842895508s\n",
            "completed batch 30 of epoch 119. loss=0.003251985413953662. train batch time cost=0.10045313835144043s\n",
            "completed batch 31 of epoch 119. loss=0.0960339829325676. train batch time cost=0.10015702247619629s\n",
            "completed batch 32 of epoch 119. loss=0.04775697737932205. train batch time cost=0.10189008712768555s\n",
            "completed batch 33 of epoch 119. loss=0.03913012519478798. train batch time cost=0.1029353141784668s\n",
            "completed batch 34 of epoch 119. loss=0.01895374245941639. train batch time cost=0.10137295722961426s\n",
            "completed batch 35 of epoch 119. loss=0.013565433211624622. train batch time cost=0.1016702651977539s\n",
            "completed batch 36 of epoch 119. loss=0.01079771388322115. train batch time cost=0.10086774826049805s\n",
            "completed batch 37 of epoch 119. loss=0.008619353175163269. train batch time cost=0.10105514526367188s\n",
            "completed batch 38 of epoch 119. loss=0.004375965800136328. train batch time cost=0.10169291496276855s\n",
            "completed batch 39 of epoch 119. loss=0.020361347123980522. train batch time cost=0.10168743133544922s\n",
            "completed batch 40 of epoch 119. loss=0.07122711837291718. train batch time cost=0.10109925270080566s\n",
            "completed batch 41 of epoch 119. loss=0.008760583586990833. train batch time cost=0.1018381118774414s\n",
            "completed batch 42 of epoch 119. loss=0.008815412409603596. train batch time cost=0.10192370414733887s\n",
            "completed batch 43 of epoch 119. loss=0.015162891708314419. train batch time cost=0.10248851776123047s\n",
            "completed batch 44 of epoch 119. loss=0.09786170721054077. train batch time cost=0.10223817825317383s\n",
            "completed batch 45 of epoch 119. loss=0.029399855062365532. train batch time cost=0.10101199150085449s\n",
            "completed batch 46 of epoch 119. loss=0.0043630520813167095. train batch time cost=0.10078167915344238s\n",
            "completed batch 47 of epoch 119. loss=0.016499217599630356. train batch time cost=0.10097527503967285s\n",
            "completed batch 48 of epoch 119. loss=0.17796139419078827. train batch time cost=0.10225486755371094s\n",
            "completed batch 49 of epoch 119. loss=0.014488935470581055. train batch time cost=0.10015654563903809s\n",
            "completed batch 50 of epoch 119. loss=0.01898173801600933. train batch time cost=0.10096240043640137s\n",
            "completed batch 51 of epoch 119. loss=0.003913900814950466. train batch time cost=0.10031461715698242s\n",
            "completed batch 52 of epoch 119. loss=0.02256869710981846. train batch time cost=0.09435081481933594s\n",
            "completed batch 53 of epoch 119. loss=0.01577882282435894. train batch time cost=0.1016690731048584s\n",
            "completed batch 54 of epoch 119. loss=0.018551379442214966. train batch time cost=0.10142230987548828s\n",
            "completed batch 55 of epoch 119. loss=0.05405465140938759. train batch time cost=0.10088539123535156s\n",
            "completed batch 56 of epoch 119. loss=0.01886635646224022. train batch time cost=0.10321331024169922s\n",
            "completed batch 57 of epoch 119. loss=0.020268850028514862. train batch time cost=0.10120820999145508s\n",
            "completed batch 58 of epoch 119. loss=0.025283483788371086. train batch time cost=0.10067605972290039s\n",
            "completed batch 59 of epoch 119. loss=0.01899547688663006. train batch time cost=0.100189208984375s\n",
            "completed batch 60 of epoch 119. loss=0.03905390948057175. train batch time cost=0.10115456581115723s\n",
            "completed batch 61 of epoch 119. loss=0.022930804640054703. train batch time cost=0.10363459587097168s\n",
            "completed batch 62 of epoch 119. loss=0.043748851865530014. train batch time cost=0.10227346420288086s\n",
            "completed batch 63 of epoch 119. loss=0.009441661648452282. train batch time cost=0.10169839859008789s\n",
            "completed batch 64 of epoch 119. loss=0.04740232601761818. train batch time cost=0.10214495658874512s\n",
            "completed batch 65 of epoch 119. loss=0.01749046891927719. train batch time cost=0.10104823112487793s\n",
            "completed batch 66 of epoch 119. loss=0.014460669830441475. train batch time cost=0.10205769538879395s\n",
            "completed batch 67 of epoch 119. loss=0.057266715914011. train batch time cost=0.10155820846557617s\n",
            "completed batch 68 of epoch 119. loss=0.06612653285264969. train batch time cost=0.10134291648864746s\n",
            "completed batch 69 of epoch 119. loss=0.004762690048664808. train batch time cost=0.10094118118286133s\n",
            "completed batch 70 of epoch 119. loss=0.0602228157222271. train batch time cost=0.10186910629272461s\n",
            "completed batch 71 of epoch 119. loss=0.06147955358028412. train batch time cost=0.10325217247009277s\n",
            "completed batch 72 of epoch 119. loss=0.039127152413129807. train batch time cost=0.10168290138244629s\n",
            "completed batch 73 of epoch 119. loss=0.0843263491988182. train batch time cost=0.10225892066955566s\n",
            "completed batch 74 of epoch 119. loss=0.006606325041502714. train batch time cost=0.10394406318664551s\n",
            "completed batch 75 of epoch 119. loss=0.023902950808405876. train batch time cost=0.1044461727142334s\n",
            "completed batch 76 of epoch 119. loss=0.07338609546422958. train batch time cost=0.10138988494873047s\n",
            "completed batch 77 of epoch 119. loss=0.011702727526426315. train batch time cost=0.10244083404541016s\n",
            "completed batch 78 of epoch 119. loss=0.007882023230195045. train batch time cost=0.10165190696716309s\n",
            "completed batch 79 of epoch 119. loss=0.035844892263412476. train batch time cost=0.10190629959106445s\n",
            "completed batch 80 of epoch 119. loss=0.048123687505722046. train batch time cost=0.10194540023803711s\n",
            "completed batch 81 of epoch 119. loss=0.03375217691063881. train batch time cost=0.09985780715942383s\n",
            "completed batch 82 of epoch 119. loss=0.05018819496035576. train batch time cost=0.10221505165100098s\n",
            "completed batch 83 of epoch 119. loss=0.007526781409978867. train batch time cost=0.09787583351135254s\n",
            "completed batch 84 of epoch 119. loss=0.020324965938925743. train batch time cost=0.10122466087341309s\n",
            "completed batch 85 of epoch 119. loss=0.04124806821346283. train batch time cost=0.10191154479980469s\n",
            "completed batch 86 of epoch 119. loss=0.0186921413987875. train batch time cost=0.10060453414916992s\n",
            "completed batch 87 of epoch 119. loss=0.012504068203270435. train batch time cost=0.10196685791015625s\n",
            "completed batch 88 of epoch 119. loss=0.015717633068561554. train batch time cost=0.10264253616333008s\n",
            "completed batch 89 of epoch 119. loss=0.05358114093542099. train batch time cost=0.1020498275756836s\n",
            "completed batch 90 of epoch 119. loss=0.02254883386194706. train batch time cost=0.10172581672668457s\n",
            "completed batch 91 of epoch 119. loss=0.06099590286612511. train batch time cost=0.10184192657470703s\n",
            "completed batch 92 of epoch 119. loss=0.023520270362496376. train batch time cost=0.10090279579162598s\n",
            "completed batch 93 of epoch 119. loss=0.03241200000047684. train batch time cost=0.10156726837158203s\n",
            "completed batch 94 of epoch 119. loss=0.015816621482372284. train batch time cost=0.10091280937194824s\n",
            "completed batch 95 of epoch 119. loss=0.023237772285938263. train batch time cost=0.09993171691894531s\n",
            "completed batch 96 of epoch 119. loss=0.01009462308138609. train batch time cost=0.10011768341064453s\n",
            "completed batch 97 of epoch 119. loss=0.0027683041989803314. train batch time cost=0.1016383171081543s\n",
            "completed batch 98 of epoch 119. loss=0.020624861121177673. train batch time cost=0.10160350799560547s\n",
            "completed batch 99 of epoch 119. loss=0.022757254540920258. train batch time cost=0.10149312019348145s\n",
            "completed batch 100 of epoch 119. loss=0.005890315398573875. train batch time cost=0.10151886940002441s\n",
            "completed batch 101 of epoch 119. loss=0.016067195683717728. train batch time cost=0.10106086730957031s\n",
            "completed batch 102 of epoch 119. loss=0.01638645865023136. train batch time cost=0.09531998634338379s\n",
            "completed batch 103 of epoch 119. loss=0.029551547020673752. train batch time cost=0.10474109649658203s\n",
            "completed batch 104 of epoch 119. loss=0.001196758123114705. train batch time cost=0.09538459777832031s\n",
            "completed batch 105 of epoch 119. loss=0.011600024998188019. train batch time cost=0.09485387802124023s\n",
            "completed batch 106 of epoch 119. loss=0.023949963971972466. train batch time cost=0.09555220603942871s\n",
            "completed batch 107 of epoch 119. loss=0.03210242837667465. train batch time cost=0.09528255462646484s\n",
            "completed batch 108 of epoch 119. loss=0.044425055384635925. train batch time cost=0.09540581703186035s\n",
            "completed batch 109 of epoch 119. loss=0.028292136266827583. train batch time cost=0.09690380096435547s\n",
            "completed batch 110 of epoch 119. loss=0.007194464094936848. train batch time cost=0.0950009822845459s\n",
            "completed batch 111 of epoch 119. loss=0.011773563921451569. train batch time cost=0.09510207176208496s\n",
            "completed batch 112 of epoch 119. loss=0.1781112104654312. train batch time cost=0.09453058242797852s\n",
            "completed batch 113 of epoch 119. loss=0.032275062054395676. train batch time cost=0.09575724601745605s\n",
            "completed batch 114 of epoch 119. loss=0.011330891400575638. train batch time cost=0.09486746788024902s\n",
            "completed batch 115 of epoch 119. loss=0.012711155228316784. train batch time cost=0.09521889686584473s\n",
            "completed batch 116 of epoch 119. loss=0.06097177416086197. train batch time cost=0.09448075294494629s\n",
            "completed batch 117 of epoch 119. loss=0.006168132182210684. train batch time cost=0.09531736373901367s\n",
            "completed batch 118 of epoch 119. loss=0.05156468600034714. train batch time cost=0.09648513793945312s\n",
            "completed batch 119 of epoch 119. loss=0.033478811383247375. train batch time cost=0.09555220603942871s\n",
            "completed batch 120 of epoch 119. loss=0.05566549673676491. train batch time cost=0.09469723701477051s\n",
            "completed batch 121 of epoch 119. loss=0.023489465937018394. train batch time cost=0.0956583023071289s\n",
            "completed batch 122 of epoch 119. loss=0.028304966166615486. train batch time cost=0.10120463371276855s\n",
            "completed batch 123 of epoch 119. loss=0.012185480445623398. train batch time cost=0.10202288627624512s\n",
            "completed batch 124 of epoch 119. loss=0.14859524369239807. train batch time cost=0.10268855094909668s\n",
            "completed batch 125 of epoch 119. loss=0.03289475291967392. train batch time cost=0.10183119773864746s\n",
            "completed batch 126 of epoch 119. loss=0.012524115853011608. train batch time cost=0.030144453048706055s\n",
            "completed test of epoch 119. loss=0.012524115853011608. accuracy=0.7264103844233649. train one epoch time cost=27.52528738975525s, test validation time cost=3.9338419437408447\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649]\n",
            "completed batch 1 of epoch 120. loss=0.011218205094337463. train batch time cost=0.10028481483459473s\n",
            "completed batch 2 of epoch 120. loss=0.028777144849300385. train batch time cost=0.10140347480773926s\n",
            "completed batch 3 of epoch 120. loss=0.014647351577877998. train batch time cost=0.10265898704528809s\n",
            "completed batch 4 of epoch 120. loss=0.011197162792086601. train batch time cost=0.10087227821350098s\n",
            "completed batch 5 of epoch 120. loss=0.03263712301850319. train batch time cost=0.10135316848754883s\n",
            "completed batch 6 of epoch 120. loss=0.01822449266910553. train batch time cost=0.10011482238769531s\n",
            "completed batch 7 of epoch 120. loss=0.05580616369843483. train batch time cost=0.10040521621704102s\n",
            "completed batch 8 of epoch 120. loss=0.04050523415207863. train batch time cost=0.10118889808654785s\n",
            "completed batch 9 of epoch 120. loss=0.02596142701804638. train batch time cost=0.10150575637817383s\n",
            "completed batch 10 of epoch 120. loss=0.008925466798245907. train batch time cost=0.10248708724975586s\n",
            "completed batch 11 of epoch 120. loss=0.04047680273652077. train batch time cost=0.10201811790466309s\n",
            "completed batch 12 of epoch 120. loss=0.03188968822360039. train batch time cost=0.10212945938110352s\n",
            "completed batch 13 of epoch 120. loss=0.025684403255581856. train batch time cost=0.10204195976257324s\n",
            "completed batch 14 of epoch 120. loss=0.03369780629873276. train batch time cost=0.10270380973815918s\n",
            "completed batch 15 of epoch 120. loss=0.07305234670639038. train batch time cost=0.1013181209564209s\n",
            "completed batch 16 of epoch 120. loss=0.05521833151578903. train batch time cost=0.10201644897460938s\n",
            "completed batch 17 of epoch 120. loss=0.04639742523431778. train batch time cost=0.10251331329345703s\n",
            "completed batch 18 of epoch 120. loss=0.029179831966757774. train batch time cost=0.10122942924499512s\n",
            "completed batch 19 of epoch 120. loss=0.004863729700446129. train batch time cost=0.10088324546813965s\n",
            "completed batch 20 of epoch 120. loss=0.028911324217915535. train batch time cost=0.10066890716552734s\n",
            "completed batch 21 of epoch 120. loss=0.03399304300546646. train batch time cost=0.1038217544555664s\n",
            "completed batch 22 of epoch 120. loss=0.01506087463349104. train batch time cost=0.10210943222045898s\n",
            "completed batch 23 of epoch 120. loss=0.004802383482456207. train batch time cost=0.10246729850769043s\n",
            "completed batch 24 of epoch 120. loss=0.01207833830267191. train batch time cost=0.10122156143188477s\n",
            "completed batch 25 of epoch 120. loss=0.05485257878899574. train batch time cost=0.10100722312927246s\n",
            "completed batch 26 of epoch 120. loss=0.07555030286312103. train batch time cost=0.10158085823059082s\n",
            "completed batch 27 of epoch 120. loss=0.0498763732612133. train batch time cost=0.10070657730102539s\n",
            "completed batch 28 of epoch 120. loss=0.026447631418704987. train batch time cost=0.09996151924133301s\n",
            "completed batch 29 of epoch 120. loss=0.021380219608545303. train batch time cost=0.09423112869262695s\n",
            "completed batch 30 of epoch 120. loss=0.003770129755139351. train batch time cost=0.09705519676208496s\n",
            "completed batch 31 of epoch 120. loss=0.0676528662443161. train batch time cost=0.09391522407531738s\n",
            "completed batch 32 of epoch 120. loss=0.08334309607744217. train batch time cost=0.09555745124816895s\n",
            "completed batch 33 of epoch 120. loss=0.028388002887368202. train batch time cost=0.0945749282836914s\n",
            "completed batch 34 of epoch 120. loss=0.026980681344866753. train batch time cost=0.0948476791381836s\n",
            "completed batch 35 of epoch 120. loss=0.020291127264499664. train batch time cost=0.1019430160522461s\n",
            "completed batch 36 of epoch 120. loss=0.08069878071546555. train batch time cost=0.09425854682922363s\n",
            "completed batch 37 of epoch 120. loss=0.025874421000480652. train batch time cost=0.09652423858642578s\n",
            "completed batch 38 of epoch 120. loss=0.13476115465164185. train batch time cost=0.09599637985229492s\n",
            "completed batch 39 of epoch 120. loss=0.03220120444893837. train batch time cost=0.09638237953186035s\n",
            "completed batch 40 of epoch 120. loss=0.0575999990105629. train batch time cost=0.09555172920227051s\n",
            "completed batch 41 of epoch 120. loss=0.06624241173267365. train batch time cost=0.09547090530395508s\n",
            "completed batch 42 of epoch 120. loss=0.10489164292812347. train batch time cost=0.0952756404876709s\n",
            "completed batch 43 of epoch 120. loss=0.036262962967157364. train batch time cost=0.09744381904602051s\n",
            "completed batch 44 of epoch 120. loss=0.004388746805489063. train batch time cost=0.1013031005859375s\n",
            "completed batch 45 of epoch 120. loss=0.010066410526633263. train batch time cost=0.10341811180114746s\n",
            "completed batch 46 of epoch 120. loss=0.04129587486386299. train batch time cost=0.10443878173828125s\n",
            "completed batch 47 of epoch 120. loss=0.05540848523378372. train batch time cost=0.10325431823730469s\n",
            "completed batch 48 of epoch 120. loss=0.046605199575424194. train batch time cost=0.10245966911315918s\n",
            "completed batch 49 of epoch 120. loss=0.049198638647794724. train batch time cost=0.10287022590637207s\n",
            "completed batch 50 of epoch 120. loss=0.010371396318078041. train batch time cost=0.10244441032409668s\n",
            "completed batch 51 of epoch 120. loss=0.02707563526928425. train batch time cost=0.10104513168334961s\n",
            "completed batch 52 of epoch 120. loss=0.08997317403554916. train batch time cost=0.10168266296386719s\n",
            "completed batch 53 of epoch 120. loss=0.035909682512283325. train batch time cost=0.10277748107910156s\n",
            "completed batch 54 of epoch 120. loss=0.02480481006205082. train batch time cost=0.10235905647277832s\n",
            "completed batch 55 of epoch 120. loss=0.013150720857083797. train batch time cost=0.10155105590820312s\n",
            "completed batch 56 of epoch 120. loss=0.02565602958202362. train batch time cost=0.10310149192810059s\n",
            "completed batch 57 of epoch 120. loss=0.014610453508794308. train batch time cost=0.10196256637573242s\n",
            "completed batch 58 of epoch 120. loss=0.03171131759881973. train batch time cost=0.10295391082763672s\n",
            "completed batch 59 of epoch 120. loss=0.025550616905093193. train batch time cost=0.10236883163452148s\n",
            "completed batch 60 of epoch 120. loss=0.011583547107875347. train batch time cost=0.10176897048950195s\n",
            "completed batch 61 of epoch 120. loss=0.007140384521335363. train batch time cost=0.10253429412841797s\n",
            "completed batch 62 of epoch 120. loss=0.02232235111296177. train batch time cost=0.10363936424255371s\n",
            "completed batch 63 of epoch 120. loss=0.02684640884399414. train batch time cost=0.10521984100341797s\n",
            "completed batch 64 of epoch 120. loss=0.016024943441152573. train batch time cost=0.10272908210754395s\n",
            "completed batch 65 of epoch 120. loss=0.005371835548430681. train batch time cost=0.10372757911682129s\n",
            "completed batch 66 of epoch 120. loss=0.01747615449130535. train batch time cost=0.10271310806274414s\n",
            "completed batch 67 of epoch 120. loss=0.0015562811167910695. train batch time cost=0.10070109367370605s\n",
            "completed batch 68 of epoch 120. loss=0.01653878204524517. train batch time cost=0.10239005088806152s\n",
            "completed batch 69 of epoch 120. loss=0.04184252396225929. train batch time cost=0.1026754379272461s\n",
            "completed batch 70 of epoch 120. loss=0.04024319723248482. train batch time cost=0.10195684432983398s\n",
            "completed batch 71 of epoch 120. loss=0.05546243116259575. train batch time cost=0.10265827178955078s\n",
            "completed batch 72 of epoch 120. loss=0.02261342667043209. train batch time cost=0.10204529762268066s\n",
            "completed batch 73 of epoch 120. loss=0.005036466289311647. train batch time cost=0.10176873207092285s\n",
            "completed batch 74 of epoch 120. loss=0.004769538529217243. train batch time cost=0.10221600532531738s\n",
            "completed batch 75 of epoch 120. loss=0.006742548197507858. train batch time cost=0.10271596908569336s\n",
            "completed batch 76 of epoch 120. loss=0.07479778677225113. train batch time cost=0.10212564468383789s\n",
            "completed batch 77 of epoch 120. loss=0.08219007402658463. train batch time cost=0.10248398780822754s\n",
            "completed batch 78 of epoch 120. loss=0.011457805521786213. train batch time cost=0.10103273391723633s\n",
            "completed batch 79 of epoch 120. loss=0.020640147849917412. train batch time cost=0.10140442848205566s\n",
            "completed batch 80 of epoch 120. loss=0.01927185244858265. train batch time cost=0.10239005088806152s\n",
            "completed batch 81 of epoch 120. loss=0.049678508192300797. train batch time cost=0.10134005546569824s\n",
            "completed batch 82 of epoch 120. loss=0.05300382524728775. train batch time cost=0.10090112686157227s\n",
            "completed batch 83 of epoch 120. loss=0.02320055104792118. train batch time cost=0.10108017921447754s\n",
            "completed batch 84 of epoch 120. loss=0.11832144111394882. train batch time cost=0.10129547119140625s\n",
            "completed batch 85 of epoch 120. loss=0.02788131684064865. train batch time cost=0.10400032997131348s\n",
            "completed batch 86 of epoch 120. loss=0.006112493574619293. train batch time cost=0.10220670700073242s\n",
            "completed batch 87 of epoch 120. loss=0.006636260077357292. train batch time cost=0.0960683822631836s\n",
            "completed batch 88 of epoch 120. loss=0.030338648706674576. train batch time cost=0.09560656547546387s\n",
            "completed batch 89 of epoch 120. loss=0.08558658510446548. train batch time cost=0.10396265983581543s\n",
            "completed batch 90 of epoch 120. loss=0.012384031899273396. train batch time cost=0.10288405418395996s\n",
            "completed batch 91 of epoch 120. loss=0.03989698737859726. train batch time cost=0.10304379463195801s\n",
            "completed batch 92 of epoch 120. loss=0.032034967094659805. train batch time cost=0.10210108757019043s\n",
            "completed batch 93 of epoch 120. loss=0.059066303074359894. train batch time cost=0.10317206382751465s\n",
            "completed batch 94 of epoch 120. loss=0.008314716629683971. train batch time cost=0.10264420509338379s\n",
            "completed batch 95 of epoch 120. loss=0.029764709994196892. train batch time cost=0.10103797912597656s\n",
            "completed batch 96 of epoch 120. loss=0.04212255775928497. train batch time cost=0.10171222686767578s\n",
            "completed batch 97 of epoch 120. loss=0.19578206539154053. train batch time cost=0.10141372680664062s\n",
            "completed batch 98 of epoch 120. loss=0.03390168026089668. train batch time cost=0.10191130638122559s\n",
            "completed batch 99 of epoch 120. loss=0.015988802537322044. train batch time cost=0.10185933113098145s\n",
            "completed batch 100 of epoch 120. loss=0.012008564546704292. train batch time cost=0.10140585899353027s\n",
            "completed batch 101 of epoch 120. loss=0.025804884731769562. train batch time cost=0.10003924369812012s\n",
            "completed batch 102 of epoch 120. loss=0.007712032180279493. train batch time cost=0.09964609146118164s\n",
            "completed batch 103 of epoch 120. loss=0.054908379912376404. train batch time cost=0.0998525619506836s\n",
            "completed batch 104 of epoch 120. loss=0.026902489364147186. train batch time cost=0.1005401611328125s\n",
            "completed batch 105 of epoch 120. loss=0.13901932537555695. train batch time cost=0.10046672821044922s\n",
            "completed batch 106 of epoch 120. loss=0.09217466413974762. train batch time cost=0.10276436805725098s\n",
            "completed batch 107 of epoch 120. loss=0.0141945481300354. train batch time cost=0.10286474227905273s\n",
            "completed batch 108 of epoch 120. loss=0.042253173887729645. train batch time cost=0.10240912437438965s\n",
            "completed batch 109 of epoch 120. loss=0.04604514688253403. train batch time cost=0.10351824760437012s\n",
            "completed batch 110 of epoch 120. loss=0.06616875529289246. train batch time cost=0.10123944282531738s\n",
            "completed batch 111 of epoch 120. loss=0.054557353258132935. train batch time cost=0.10158205032348633s\n",
            "completed batch 112 of epoch 120. loss=0.04858612269163132. train batch time cost=0.10292935371398926s\n",
            "completed batch 113 of epoch 120. loss=0.007689017802476883. train batch time cost=0.10234761238098145s\n",
            "completed batch 114 of epoch 120. loss=0.03630322962999344. train batch time cost=0.10267424583435059s\n",
            "completed batch 115 of epoch 120. loss=0.007814127951860428. train batch time cost=0.1022939682006836s\n",
            "completed batch 116 of epoch 120. loss=0.03003803826868534. train batch time cost=0.10202550888061523s\n",
            "completed batch 117 of epoch 120. loss=0.050237275660037994. train batch time cost=0.10176992416381836s\n",
            "completed batch 118 of epoch 120. loss=0.03070073388516903. train batch time cost=0.10184741020202637s\n",
            "completed batch 119 of epoch 120. loss=0.02895187772810459. train batch time cost=0.10106515884399414s\n",
            "completed batch 120 of epoch 120. loss=0.1109045073390007. train batch time cost=0.10170960426330566s\n",
            "completed batch 121 of epoch 120. loss=0.015702813863754272. train batch time cost=0.10188794136047363s\n",
            "completed batch 122 of epoch 120. loss=0.09884404391050339. train batch time cost=0.10257077217102051s\n",
            "completed batch 123 of epoch 120. loss=0.012316311709582806. train batch time cost=0.10428833961486816s\n",
            "completed batch 124 of epoch 120. loss=0.07598055154085159. train batch time cost=0.10195302963256836s\n",
            "completed batch 125 of epoch 120. loss=0.008720760233700275. train batch time cost=0.1025233268737793s\n",
            "completed batch 126 of epoch 120. loss=0.00020455778576433659. train batch time cost=0.030361652374267578s\n",
            "completed test of epoch 120. loss=0.00020455778576433659. accuracy=0.7084373439840239. train one epoch time cost=27.59601593017578s, test validation time cost=3.828044891357422\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239]\n",
            "completed batch 1 of epoch 121. loss=0.04837321862578392. train batch time cost=0.09808063507080078s\n",
            "completed batch 2 of epoch 121. loss=0.02008857950568199. train batch time cost=0.10194206237792969s\n",
            "completed batch 3 of epoch 121. loss=0.09599242359399796. train batch time cost=0.10361719131469727s\n",
            "completed batch 4 of epoch 121. loss=0.13089707493782043. train batch time cost=0.10137414932250977s\n",
            "completed batch 5 of epoch 121. loss=0.15769603848457336. train batch time cost=0.09588766098022461s\n",
            "completed batch 6 of epoch 121. loss=0.04588649421930313. train batch time cost=0.0971682071685791s\n",
            "completed batch 7 of epoch 121. loss=0.010722557082772255. train batch time cost=0.09556746482849121s\n",
            "completed batch 8 of epoch 121. loss=0.047430817037820816. train batch time cost=0.09551334381103516s\n",
            "completed batch 9 of epoch 121. loss=0.012622179463505745. train batch time cost=0.09531044960021973s\n",
            "completed batch 10 of epoch 121. loss=0.15977686643600464. train batch time cost=0.09691524505615234s\n",
            "completed batch 11 of epoch 121. loss=0.12733933329582214. train batch time cost=0.09689855575561523s\n",
            "completed batch 12 of epoch 121. loss=0.054871730506420135. train batch time cost=0.09392023086547852s\n",
            "completed batch 13 of epoch 121. loss=0.0923425480723381. train batch time cost=0.09749698638916016s\n",
            "completed batch 14 of epoch 121. loss=0.024625973775982857. train batch time cost=0.09917688369750977s\n",
            "completed batch 15 of epoch 121. loss=0.07390996813774109. train batch time cost=0.10151982307434082s\n",
            "completed batch 16 of epoch 121. loss=0.006105201318860054. train batch time cost=0.10233783721923828s\n",
            "completed batch 17 of epoch 121. loss=0.07757561653852463. train batch time cost=0.10246109962463379s\n",
            "completed batch 18 of epoch 121. loss=0.0038272556848824024. train batch time cost=0.10186147689819336s\n",
            "completed batch 19 of epoch 121. loss=0.07198832184076309. train batch time cost=0.10467386245727539s\n",
            "completed batch 20 of epoch 121. loss=0.006755884736776352. train batch time cost=0.10349202156066895s\n",
            "completed batch 21 of epoch 121. loss=0.06436660885810852. train batch time cost=0.1042783260345459s\n",
            "completed batch 22 of epoch 121. loss=0.0074618179351091385. train batch time cost=0.10155034065246582s\n",
            "completed batch 23 of epoch 121. loss=0.02645367942750454. train batch time cost=0.09885144233703613s\n",
            "completed batch 24 of epoch 121. loss=0.08637119829654694. train batch time cost=0.0949239730834961s\n",
            "completed batch 25 of epoch 121. loss=0.09958625584840775. train batch time cost=0.09578490257263184s\n",
            "completed batch 26 of epoch 121. loss=0.1375638246536255. train batch time cost=0.09700441360473633s\n",
            "completed batch 27 of epoch 121. loss=0.1578001230955124. train batch time cost=0.09495925903320312s\n",
            "completed batch 28 of epoch 121. loss=0.16427081823349. train batch time cost=0.09491968154907227s\n",
            "completed batch 29 of epoch 121. loss=0.037621889263391495. train batch time cost=0.0966031551361084s\n",
            "completed batch 30 of epoch 121. loss=0.0059753223322331905. train batch time cost=0.09432268142700195s\n",
            "completed batch 31 of epoch 121. loss=0.11518985033035278. train batch time cost=0.10041570663452148s\n",
            "completed batch 32 of epoch 121. loss=0.09790290147066116. train batch time cost=0.10051131248474121s\n",
            "completed batch 33 of epoch 121. loss=0.15619394183158875. train batch time cost=0.10271215438842773s\n",
            "completed batch 34 of epoch 121. loss=0.10548967868089676. train batch time cost=0.10210371017456055s\n",
            "completed batch 35 of epoch 121. loss=0.09876048564910889. train batch time cost=0.1003415584564209s\n",
            "completed batch 36 of epoch 121. loss=0.05368722602725029. train batch time cost=0.10058426856994629s\n",
            "completed batch 37 of epoch 121. loss=0.29659026861190796. train batch time cost=0.1012420654296875s\n",
            "completed batch 38 of epoch 121. loss=0.24678468704223633. train batch time cost=0.10137104988098145s\n",
            "completed batch 39 of epoch 121. loss=0.16595900058746338. train batch time cost=0.10210824012756348s\n",
            "completed batch 40 of epoch 121. loss=0.015035196207463741. train batch time cost=0.10159468650817871s\n",
            "completed batch 41 of epoch 121. loss=0.030273444950580597. train batch time cost=0.10200667381286621s\n",
            "completed batch 42 of epoch 121. loss=0.0980377122759819. train batch time cost=0.1027517318725586s\n",
            "completed batch 43 of epoch 121. loss=0.11907345801591873. train batch time cost=0.10233616828918457s\n",
            "completed batch 44 of epoch 121. loss=0.045255035161972046. train batch time cost=0.10244226455688477s\n",
            "completed batch 45 of epoch 121. loss=0.11978714913129807. train batch time cost=0.10154604911804199s\n",
            "completed batch 46 of epoch 121. loss=0.054700396955013275. train batch time cost=0.10419988632202148s\n",
            "completed batch 47 of epoch 121. loss=0.21316996216773987. train batch time cost=0.10121607780456543s\n",
            "completed batch 48 of epoch 121. loss=0.031020693480968475. train batch time cost=0.1022038459777832s\n",
            "completed batch 49 of epoch 121. loss=0.039319850504398346. train batch time cost=0.10274791717529297s\n",
            "completed batch 50 of epoch 121. loss=0.13041533529758453. train batch time cost=0.10250616073608398s\n",
            "completed batch 51 of epoch 121. loss=0.040534328669309616. train batch time cost=0.10304737091064453s\n",
            "completed batch 52 of epoch 121. loss=0.005124702583998442. train batch time cost=0.10385704040527344s\n",
            "completed batch 53 of epoch 121. loss=0.045074377208948135. train batch time cost=0.1030721664428711s\n",
            "completed batch 54 of epoch 121. loss=0.059006527066230774. train batch time cost=0.10341477394104004s\n",
            "completed batch 55 of epoch 121. loss=0.11319383233785629. train batch time cost=0.10339522361755371s\n",
            "completed batch 56 of epoch 121. loss=0.06308792531490326. train batch time cost=0.10256791114807129s\n",
            "completed batch 57 of epoch 121. loss=0.04318491369485855. train batch time cost=0.10659146308898926s\n",
            "completed batch 58 of epoch 121. loss=0.07818940281867981. train batch time cost=0.10531997680664062s\n",
            "completed batch 59 of epoch 121. loss=0.12527048587799072. train batch time cost=0.10103273391723633s\n",
            "completed batch 60 of epoch 121. loss=0.018750479444861412. train batch time cost=0.10262441635131836s\n",
            "completed batch 61 of epoch 121. loss=0.0705975666642189. train batch time cost=0.10342764854431152s\n",
            "completed batch 62 of epoch 121. loss=0.05360923334956169. train batch time cost=0.10144281387329102s\n",
            "completed batch 63 of epoch 121. loss=0.1401970088481903. train batch time cost=0.10066056251525879s\n",
            "completed batch 64 of epoch 121. loss=0.014251310378313065. train batch time cost=0.10199284553527832s\n",
            "completed batch 65 of epoch 121. loss=0.01474496629089117. train batch time cost=0.10268187522888184s\n",
            "completed batch 66 of epoch 121. loss=0.05409203842282295. train batch time cost=0.10224485397338867s\n",
            "completed batch 67 of epoch 121. loss=0.02891416661441326. train batch time cost=0.10226798057556152s\n",
            "completed batch 68 of epoch 121. loss=0.026442278176546097. train batch time cost=0.1006937026977539s\n",
            "completed batch 69 of epoch 121. loss=0.008687235414981842. train batch time cost=0.10294938087463379s\n",
            "completed batch 70 of epoch 121. loss=0.009832095354795456. train batch time cost=0.10176324844360352s\n",
            "completed batch 71 of epoch 121. loss=0.055934976786375046. train batch time cost=0.10245203971862793s\n",
            "completed batch 72 of epoch 121. loss=0.09654320031404495. train batch time cost=0.10148215293884277s\n",
            "completed batch 73 of epoch 121. loss=0.027366293594241142. train batch time cost=0.1031026840209961s\n",
            "completed batch 74 of epoch 121. loss=0.1042151004076004. train batch time cost=0.10217142105102539s\n",
            "completed batch 75 of epoch 121. loss=0.02259903773665428. train batch time cost=0.10498213768005371s\n",
            "completed batch 76 of epoch 121. loss=0.0418313629925251. train batch time cost=0.10374307632446289s\n",
            "completed batch 77 of epoch 121. loss=0.029343729838728905. train batch time cost=0.10217833518981934s\n",
            "completed batch 78 of epoch 121. loss=0.07743704319000244. train batch time cost=0.10434389114379883s\n",
            "completed batch 79 of epoch 121. loss=0.027242586016654968. train batch time cost=0.11003875732421875s\n",
            "completed batch 80 of epoch 121. loss=0.07377927005290985. train batch time cost=0.10286951065063477s\n",
            "completed batch 81 of epoch 121. loss=0.13539952039718628. train batch time cost=0.10158276557922363s\n",
            "completed batch 82 of epoch 121. loss=0.025063632056117058. train batch time cost=0.10167527198791504s\n",
            "completed batch 83 of epoch 121. loss=0.11011198163032532. train batch time cost=0.10222697257995605s\n",
            "completed batch 84 of epoch 121. loss=0.031024165451526642. train batch time cost=0.10214924812316895s\n",
            "completed batch 85 of epoch 121. loss=0.1129789799451828. train batch time cost=0.10174822807312012s\n",
            "completed batch 86 of epoch 121. loss=0.011837465688586235. train batch time cost=0.10213398933410645s\n",
            "completed batch 87 of epoch 121. loss=0.05662047490477562. train batch time cost=0.1033329963684082s\n",
            "completed batch 88 of epoch 121. loss=0.09031779319047928. train batch time cost=0.1031491756439209s\n",
            "completed batch 89 of epoch 121. loss=0.03806416317820549. train batch time cost=0.10189247131347656s\n",
            "completed batch 90 of epoch 121. loss=0.012209630571305752. train batch time cost=0.10179376602172852s\n",
            "completed batch 91 of epoch 121. loss=0.03195185959339142. train batch time cost=0.10123467445373535s\n",
            "completed batch 92 of epoch 121. loss=0.018884137272834778. train batch time cost=0.10142350196838379s\n",
            "completed batch 93 of epoch 121. loss=0.10151956230401993. train batch time cost=0.10139322280883789s\n",
            "completed batch 94 of epoch 121. loss=0.2238510400056839. train batch time cost=0.10131144523620605s\n",
            "completed batch 95 of epoch 121. loss=0.008346443064510822. train batch time cost=0.1011505126953125s\n",
            "completed batch 96 of epoch 121. loss=0.0275066327303648. train batch time cost=0.10138463973999023s\n",
            "completed batch 97 of epoch 121. loss=0.07630304992198944. train batch time cost=0.10188627243041992s\n",
            "completed batch 98 of epoch 121. loss=0.016472894698381424. train batch time cost=0.10161375999450684s\n",
            "completed batch 99 of epoch 121. loss=0.018767433241009712. train batch time cost=0.10123300552368164s\n",
            "completed batch 100 of epoch 121. loss=0.07685133069753647. train batch time cost=0.1010293960571289s\n",
            "completed batch 101 of epoch 121. loss=0.023281985893845558. train batch time cost=0.09537792205810547s\n",
            "completed batch 102 of epoch 121. loss=0.049054551869630814. train batch time cost=0.09564423561096191s\n",
            "completed batch 103 of epoch 121. loss=0.027358921244740486. train batch time cost=0.09527420997619629s\n",
            "completed batch 104 of epoch 121. loss=0.020088735967874527. train batch time cost=0.0944211483001709s\n",
            "completed batch 105 of epoch 121. loss=0.014048056676983833. train batch time cost=0.09444713592529297s\n",
            "completed batch 106 of epoch 121. loss=0.0983428806066513. train batch time cost=0.09611940383911133s\n",
            "completed batch 107 of epoch 121. loss=0.035995595157146454. train batch time cost=0.0952603816986084s\n",
            "completed batch 108 of epoch 121. loss=0.0668821632862091. train batch time cost=0.09554743766784668s\n",
            "completed batch 109 of epoch 121. loss=0.03230324387550354. train batch time cost=0.09553122520446777s\n",
            "completed batch 110 of epoch 121. loss=0.010953226126730442. train batch time cost=0.09536266326904297s\n",
            "completed batch 111 of epoch 121. loss=0.04558253288269043. train batch time cost=0.09699630737304688s\n",
            "completed batch 112 of epoch 121. loss=0.09507466107606888. train batch time cost=0.09628176689147949s\n",
            "completed batch 113 of epoch 121. loss=0.03848037123680115. train batch time cost=0.09586358070373535s\n",
            "completed batch 114 of epoch 121. loss=0.02410782501101494. train batch time cost=0.09492683410644531s\n",
            "completed batch 115 of epoch 121. loss=0.058406196534633636. train batch time cost=0.09549617767333984s\n",
            "completed batch 116 of epoch 121. loss=0.039720553904771805. train batch time cost=0.09647536277770996s\n",
            "completed batch 117 of epoch 121. loss=0.006591585464775562. train batch time cost=0.09527206420898438s\n",
            "completed batch 118 of epoch 121. loss=0.029986875131726265. train batch time cost=0.09589767456054688s\n",
            "completed batch 119 of epoch 121. loss=0.045144349336624146. train batch time cost=0.09569168090820312s\n",
            "completed batch 120 of epoch 121. loss=0.04286367446184158. train batch time cost=0.0962979793548584s\n",
            "completed batch 121 of epoch 121. loss=0.0066239782609045506. train batch time cost=0.09642767906188965s\n",
            "completed batch 122 of epoch 121. loss=0.031360335648059845. train batch time cost=0.09621906280517578s\n",
            "completed batch 123 of epoch 121. loss=0.05904461070895195. train batch time cost=0.09485387802124023s\n",
            "completed batch 124 of epoch 121. loss=0.11408116668462753. train batch time cost=0.09647750854492188s\n",
            "completed batch 125 of epoch 121. loss=0.020184990018606186. train batch time cost=0.09559392929077148s\n",
            "completed batch 126 of epoch 121. loss=0.0004600588872563094. train batch time cost=0.03134441375732422s\n",
            "completed test of epoch 121. loss=0.0004600588872563094. accuracy=0.6859710434348477. train one epoch time cost=27.448449850082397s, test validation time cost=3.8993091583251953\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477]\n",
            "completed batch 1 of epoch 122. loss=0.029210224747657776. train batch time cost=0.1009376049041748s\n",
            "completed batch 2 of epoch 122. loss=0.11520956456661224. train batch time cost=0.09581208229064941s\n",
            "completed batch 3 of epoch 122. loss=0.10505741089582443. train batch time cost=0.09550666809082031s\n",
            "completed batch 4 of epoch 122. loss=0.08212190121412277. train batch time cost=0.09508585929870605s\n",
            "completed batch 5 of epoch 122. loss=0.02902817726135254. train batch time cost=0.09565353393554688s\n",
            "completed batch 6 of epoch 122. loss=0.015417462214827538. train batch time cost=0.09488058090209961s\n",
            "completed batch 7 of epoch 122. loss=0.03768825903534889. train batch time cost=0.09491205215454102s\n",
            "completed batch 8 of epoch 122. loss=0.06383972615003586. train batch time cost=0.09499907493591309s\n",
            "completed batch 9 of epoch 122. loss=0.09650816768407822. train batch time cost=0.09612655639648438s\n",
            "completed batch 10 of epoch 122. loss=0.006638230290263891. train batch time cost=0.10303568840026855s\n",
            "completed batch 11 of epoch 122. loss=0.00567992590367794. train batch time cost=0.10184097290039062s\n",
            "completed batch 12 of epoch 122. loss=0.09801202267408371. train batch time cost=0.09553098678588867s\n",
            "completed batch 13 of epoch 122. loss=0.5010261535644531. train batch time cost=0.09701776504516602s\n",
            "completed batch 14 of epoch 122. loss=0.11269821226596832. train batch time cost=0.09532308578491211s\n",
            "completed batch 15 of epoch 122. loss=0.12352889031171799. train batch time cost=0.09789323806762695s\n",
            "completed batch 16 of epoch 122. loss=0.014179924502968788. train batch time cost=0.0946359634399414s\n",
            "completed batch 17 of epoch 122. loss=0.018034854903817177. train batch time cost=0.09614992141723633s\n",
            "completed batch 18 of epoch 122. loss=0.03135410696268082. train batch time cost=0.09474849700927734s\n",
            "completed batch 19 of epoch 122. loss=0.193791463971138. train batch time cost=0.09632635116577148s\n",
            "completed batch 20 of epoch 122. loss=0.06953917443752289. train batch time cost=0.0949716567993164s\n",
            "completed batch 21 of epoch 122. loss=0.17205610871315002. train batch time cost=0.09466004371643066s\n",
            "completed batch 22 of epoch 122. loss=0.03577012941241264. train batch time cost=0.09482979774475098s\n",
            "completed batch 23 of epoch 122. loss=0.053120337426662445. train batch time cost=0.09438872337341309s\n",
            "completed batch 24 of epoch 122. loss=0.017736000940203667. train batch time cost=0.09632015228271484s\n",
            "completed batch 25 of epoch 122. loss=0.2284221351146698. train batch time cost=0.09613442420959473s\n",
            "completed batch 26 of epoch 122. loss=0.023164186626672745. train batch time cost=0.09659552574157715s\n",
            "completed batch 27 of epoch 122. loss=0.22128364443778992. train batch time cost=0.09649109840393066s\n",
            "completed batch 28 of epoch 122. loss=0.15795587003231049. train batch time cost=0.0953531265258789s\n",
            "completed batch 29 of epoch 122. loss=0.18669776618480682. train batch time cost=0.09509611129760742s\n",
            "completed batch 30 of epoch 122. loss=0.007133052218705416. train batch time cost=0.09556078910827637s\n",
            "completed batch 31 of epoch 122. loss=0.263012558221817. train batch time cost=0.09479475021362305s\n",
            "completed batch 32 of epoch 122. loss=0.05395063757896423. train batch time cost=0.09504842758178711s\n",
            "completed batch 33 of epoch 122. loss=0.055390745401382446. train batch time cost=0.09643816947937012s\n",
            "completed batch 34 of epoch 122. loss=0.06030481681227684. train batch time cost=0.10084080696105957s\n",
            "completed batch 35 of epoch 122. loss=0.05275829881429672. train batch time cost=0.0952606201171875s\n",
            "completed batch 36 of epoch 122. loss=0.055517181754112244. train batch time cost=0.09503841400146484s\n",
            "completed batch 37 of epoch 122. loss=0.10378914326429367. train batch time cost=0.09581518173217773s\n",
            "completed batch 38 of epoch 122. loss=0.06608104705810547. train batch time cost=0.09760570526123047s\n",
            "completed batch 39 of epoch 122. loss=0.09151243418455124. train batch time cost=0.09690308570861816s\n",
            "completed batch 40 of epoch 122. loss=0.050010424107313156. train batch time cost=0.0972604751586914s\n",
            "completed batch 41 of epoch 122. loss=0.07070232927799225. train batch time cost=0.10262274742126465s\n",
            "completed batch 42 of epoch 122. loss=0.022648951038718224. train batch time cost=0.1014852523803711s\n",
            "completed batch 43 of epoch 122. loss=0.08343295007944107. train batch time cost=0.10193347930908203s\n",
            "completed batch 44 of epoch 122. loss=0.061591047793626785. train batch time cost=0.1043710708618164s\n",
            "completed batch 45 of epoch 122. loss=0.034938037395477295. train batch time cost=0.10174942016601562s\n",
            "completed batch 46 of epoch 122. loss=0.08053932338953018. train batch time cost=0.10051131248474121s\n",
            "completed batch 47 of epoch 122. loss=0.1505645215511322. train batch time cost=0.10251474380493164s\n",
            "completed batch 48 of epoch 122. loss=0.09536772966384888. train batch time cost=0.1032721996307373s\n",
            "completed batch 49 of epoch 122. loss=0.0825713723897934. train batch time cost=0.10185718536376953s\n",
            "completed batch 50 of epoch 122. loss=0.01960132084786892. train batch time cost=0.09905076026916504s\n",
            "completed batch 51 of epoch 122. loss=0.005039725918322802. train batch time cost=0.10115194320678711s\n",
            "completed batch 52 of epoch 122. loss=0.026651039719581604. train batch time cost=0.10220789909362793s\n",
            "completed batch 53 of epoch 122. loss=0.04340723529458046. train batch time cost=0.10195112228393555s\n",
            "completed batch 54 of epoch 122. loss=0.054556071758270264. train batch time cost=0.10094499588012695s\n",
            "completed batch 55 of epoch 122. loss=0.059986013919115067. train batch time cost=0.10127687454223633s\n",
            "completed batch 56 of epoch 122. loss=0.03691873326897621. train batch time cost=0.10142779350280762s\n",
            "completed batch 57 of epoch 122. loss=0.03299644961953163. train batch time cost=0.10377049446105957s\n",
            "completed batch 58 of epoch 122. loss=0.09776309877634048. train batch time cost=0.1031193733215332s\n",
            "completed batch 59 of epoch 122. loss=0.03512192144989967. train batch time cost=0.10181808471679688s\n",
            "completed batch 60 of epoch 122. loss=0.11098868399858475. train batch time cost=0.10323572158813477s\n",
            "completed batch 61 of epoch 122. loss=0.020340636372566223. train batch time cost=0.1020967960357666s\n",
            "completed batch 62 of epoch 122. loss=0.09731744974851608. train batch time cost=0.10187053680419922s\n",
            "completed batch 63 of epoch 122. loss=0.035199712961912155. train batch time cost=0.10148739814758301s\n",
            "completed batch 64 of epoch 122. loss=0.060437560081481934. train batch time cost=0.10085344314575195s\n",
            "completed batch 65 of epoch 122. loss=0.014296149834990501. train batch time cost=0.10223078727722168s\n",
            "completed batch 66 of epoch 122. loss=0.18293997645378113. train batch time cost=0.10112738609313965s\n",
            "completed batch 67 of epoch 122. loss=0.037787746638059616. train batch time cost=0.1017458438873291s\n",
            "completed batch 68 of epoch 122. loss=0.020756464451551437. train batch time cost=0.10291790962219238s\n",
            "completed batch 69 of epoch 122. loss=0.05780130624771118. train batch time cost=0.10158467292785645s\n",
            "completed batch 70 of epoch 122. loss=0.020794976502656937. train batch time cost=0.1013040542602539s\n",
            "completed batch 71 of epoch 122. loss=0.02055712789297104. train batch time cost=0.10216212272644043s\n",
            "completed batch 72 of epoch 122. loss=0.02697575092315674. train batch time cost=0.10158014297485352s\n",
            "completed batch 73 of epoch 122. loss=0.04199378564953804. train batch time cost=0.10344839096069336s\n",
            "completed batch 74 of epoch 122. loss=0.03133530542254448. train batch time cost=0.10136961936950684s\n",
            "completed batch 75 of epoch 122. loss=0.09267644584178925. train batch time cost=0.10197210311889648s\n",
            "completed batch 76 of epoch 122. loss=0.016237996518611908. train batch time cost=0.11007022857666016s\n",
            "completed batch 77 of epoch 122. loss=0.07705877721309662. train batch time cost=0.09626555442810059s\n",
            "completed batch 78 of epoch 122. loss=0.02449839375913143. train batch time cost=0.09613585472106934s\n",
            "completed batch 79 of epoch 122. loss=0.03763769567012787. train batch time cost=0.09553337097167969s\n",
            "completed batch 80 of epoch 122. loss=0.018276356160640717. train batch time cost=0.09805417060852051s\n",
            "completed batch 81 of epoch 122. loss=0.10512940585613251. train batch time cost=0.09479308128356934s\n",
            "completed batch 82 of epoch 122. loss=0.019274543970823288. train batch time cost=0.09679841995239258s\n",
            "completed batch 83 of epoch 122. loss=0.0997815728187561. train batch time cost=0.09560918807983398s\n",
            "completed batch 84 of epoch 122. loss=0.01103043183684349. train batch time cost=0.09379315376281738s\n",
            "completed batch 85 of epoch 122. loss=0.035777702927589417. train batch time cost=0.09689664840698242s\n",
            "completed batch 86 of epoch 122. loss=0.021453088149428368. train batch time cost=0.09379744529724121s\n",
            "completed batch 87 of epoch 122. loss=0.11806120723485947. train batch time cost=0.09418392181396484s\n",
            "completed batch 88 of epoch 122. loss=0.03508354723453522. train batch time cost=0.09461259841918945s\n",
            "completed batch 89 of epoch 122. loss=0.02026624232530594. train batch time cost=0.0948340892791748s\n",
            "completed batch 90 of epoch 122. loss=0.07899665087461472. train batch time cost=0.09557104110717773s\n",
            "completed batch 91 of epoch 122. loss=0.06639797240495682. train batch time cost=0.0943305492401123s\n",
            "completed batch 92 of epoch 122. loss=0.012406538240611553. train batch time cost=0.09581136703491211s\n",
            "completed batch 93 of epoch 122. loss=0.011992862448096275. train batch time cost=0.09375524520874023s\n",
            "completed batch 94 of epoch 122. loss=0.023708930239081383. train batch time cost=0.09730648994445801s\n",
            "completed batch 95 of epoch 122. loss=0.007850073277950287. train batch time cost=0.1013033390045166s\n",
            "completed batch 96 of epoch 122. loss=0.02723056450486183. train batch time cost=0.10190153121948242s\n",
            "completed batch 97 of epoch 122. loss=0.01755194552242756. train batch time cost=0.10088539123535156s\n",
            "completed batch 98 of epoch 122. loss=0.05868489295244217. train batch time cost=0.10235857963562012s\n",
            "completed batch 99 of epoch 122. loss=0.06733520328998566. train batch time cost=0.10208845138549805s\n",
            "completed batch 100 of epoch 122. loss=0.03462538868188858. train batch time cost=0.10104537010192871s\n",
            "completed batch 101 of epoch 122. loss=0.09801673144102097. train batch time cost=0.10104012489318848s\n",
            "completed batch 102 of epoch 122. loss=0.03237680718302727. train batch time cost=0.10086774826049805s\n",
            "completed batch 103 of epoch 122. loss=0.06187545508146286. train batch time cost=0.10463547706604004s\n",
            "completed batch 104 of epoch 122. loss=0.010272329673171043. train batch time cost=0.10309839248657227s\n",
            "completed batch 105 of epoch 122. loss=0.02079225704073906. train batch time cost=0.1034395694732666s\n",
            "completed batch 106 of epoch 122. loss=0.04943416267633438. train batch time cost=0.10266470909118652s\n",
            "completed batch 107 of epoch 122. loss=0.06882103532552719. train batch time cost=0.10396552085876465s\n",
            "completed batch 108 of epoch 122. loss=0.03780990093946457. train batch time cost=0.10186600685119629s\n",
            "completed batch 109 of epoch 122. loss=0.026420995593070984. train batch time cost=0.09566640853881836s\n",
            "completed batch 110 of epoch 122. loss=0.01915721595287323. train batch time cost=0.09521818161010742s\n",
            "completed batch 111 of epoch 122. loss=0.039291612803936005. train batch time cost=0.0973808765411377s\n",
            "completed batch 112 of epoch 122. loss=0.017063384875655174. train batch time cost=0.0951077938079834s\n",
            "completed batch 113 of epoch 122. loss=0.009220071136951447. train batch time cost=0.09477019309997559s\n",
            "completed batch 114 of epoch 122. loss=0.007869691587984562. train batch time cost=0.09537482261657715s\n",
            "completed batch 115 of epoch 122. loss=0.08761784434318542. train batch time cost=0.09365725517272949s\n",
            "completed batch 116 of epoch 122. loss=0.04372125491499901. train batch time cost=0.09403729438781738s\n",
            "completed batch 117 of epoch 122. loss=0.006837364751845598. train batch time cost=0.09413957595825195s\n",
            "completed batch 118 of epoch 122. loss=0.017698705196380615. train batch time cost=0.0947256088256836s\n",
            "completed batch 119 of epoch 122. loss=0.01785901002585888. train batch time cost=0.09435367584228516s\n",
            "completed batch 120 of epoch 122. loss=0.12703438103199005. train batch time cost=0.09473276138305664s\n",
            "completed batch 121 of epoch 122. loss=0.04059029370546341. train batch time cost=0.09615302085876465s\n",
            "completed batch 122 of epoch 122. loss=0.016414398327469826. train batch time cost=0.09665870666503906s\n",
            "completed batch 123 of epoch 122. loss=0.09473808854818344. train batch time cost=0.09474515914916992s\n",
            "completed batch 124 of epoch 122. loss=0.01084478385746479. train batch time cost=0.09390044212341309s\n",
            "completed batch 125 of epoch 122. loss=0.023305442184209824. train batch time cost=0.09425020217895508s\n",
            "completed batch 126 of epoch 122. loss=0.00023721340403426439. train batch time cost=0.028411865234375s\n",
            "completed test of epoch 122. loss=0.00023721340403426439. accuracy=0.7299051422865701. train one epoch time cost=27.225508213043213s, test validation time cost=3.8375844955444336\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701]\n",
            "completed batch 1 of epoch 123. loss=0.028064731508493423. train batch time cost=0.0940408706665039s\n",
            "completed batch 2 of epoch 123. loss=0.08955809473991394. train batch time cost=0.10161614418029785s\n",
            "completed batch 3 of epoch 123. loss=0.013269699178636074. train batch time cost=0.10193967819213867s\n",
            "completed batch 4 of epoch 123. loss=0.02282090298831463. train batch time cost=0.10177993774414062s\n",
            "completed batch 5 of epoch 123. loss=0.011440050788223743. train batch time cost=0.09617733955383301s\n",
            "completed batch 6 of epoch 123. loss=0.007777942810207605. train batch time cost=0.09485578536987305s\n",
            "completed batch 7 of epoch 123. loss=0.03544219955801964. train batch time cost=0.09663176536560059s\n",
            "completed batch 8 of epoch 123. loss=0.11437021195888519. train batch time cost=0.09551024436950684s\n",
            "completed batch 9 of epoch 123. loss=0.019010618329048157. train batch time cost=0.09630560874938965s\n",
            "completed batch 10 of epoch 123. loss=0.07352874428033829. train batch time cost=0.09518218040466309s\n",
            "completed batch 11 of epoch 123. loss=0.026112360879778862. train batch time cost=0.10050010681152344s\n",
            "completed batch 12 of epoch 123. loss=0.013112146407365799. train batch time cost=0.10222625732421875s\n",
            "completed batch 13 of epoch 123. loss=0.03472214564681053. train batch time cost=0.10245656967163086s\n",
            "completed batch 14 of epoch 123. loss=0.04512108117341995. train batch time cost=0.10128045082092285s\n",
            "completed batch 15 of epoch 123. loss=0.03928440809249878. train batch time cost=0.0995323657989502s\n",
            "completed batch 16 of epoch 123. loss=0.018632059916853905. train batch time cost=0.1017916202545166s\n",
            "completed batch 17 of epoch 123. loss=0.01253478229045868. train batch time cost=0.10222935676574707s\n",
            "completed batch 18 of epoch 123. loss=0.011833813972771168. train batch time cost=0.09987592697143555s\n",
            "completed batch 19 of epoch 123. loss=0.04741954058408737. train batch time cost=0.10030198097229004s\n",
            "completed batch 20 of epoch 123. loss=0.20582227408885956. train batch time cost=0.0998232364654541s\n",
            "completed batch 21 of epoch 123. loss=0.011278667487204075. train batch time cost=0.10057830810546875s\n",
            "completed batch 22 of epoch 123. loss=0.005047861486673355. train batch time cost=0.1003866195678711s\n",
            "completed batch 23 of epoch 123. loss=0.017493952065706253. train batch time cost=0.1013803482055664s\n",
            "completed batch 24 of epoch 123. loss=0.13747254014015198. train batch time cost=0.10086870193481445s\n",
            "completed batch 25 of epoch 123. loss=0.01575573720037937. train batch time cost=0.10120129585266113s\n",
            "completed batch 26 of epoch 123. loss=0.01866573467850685. train batch time cost=0.10250020027160645s\n",
            "completed batch 27 of epoch 123. loss=0.045452866703271866. train batch time cost=0.10117435455322266s\n",
            "completed batch 28 of epoch 123. loss=0.10794074833393097. train batch time cost=0.10062050819396973s\n",
            "completed batch 29 of epoch 123. loss=0.07875921577215195. train batch time cost=0.10125875473022461s\n",
            "completed batch 30 of epoch 123. loss=0.016906391829252243. train batch time cost=0.10401153564453125s\n",
            "completed batch 31 of epoch 123. loss=0.00699726864695549. train batch time cost=0.10204005241394043s\n",
            "completed batch 32 of epoch 123. loss=0.044610679149627686. train batch time cost=0.10125160217285156s\n",
            "completed batch 33 of epoch 123. loss=0.03517594933509827. train batch time cost=0.10185408592224121s\n",
            "completed batch 34 of epoch 123. loss=0.13847260177135468. train batch time cost=0.10242843627929688s\n",
            "completed batch 35 of epoch 123. loss=0.041623152792453766. train batch time cost=0.1013486385345459s\n",
            "completed batch 36 of epoch 123. loss=0.009718265384435654. train batch time cost=0.1024169921875s\n",
            "completed batch 37 of epoch 123. loss=0.03142976388335228. train batch time cost=0.10286355018615723s\n",
            "completed batch 38 of epoch 123. loss=0.11377362161874771. train batch time cost=0.10146403312683105s\n",
            "completed batch 39 of epoch 123. loss=0.00527031859382987. train batch time cost=0.1014261245727539s\n",
            "completed batch 40 of epoch 123. loss=0.002298471750691533. train batch time cost=0.10256433486938477s\n",
            "completed batch 41 of epoch 123. loss=0.0062887780368328094. train batch time cost=0.10234761238098145s\n",
            "completed batch 42 of epoch 123. loss=0.029999511316418648. train batch time cost=0.09548234939575195s\n",
            "completed batch 43 of epoch 123. loss=0.010816016234457493. train batch time cost=0.09503507614135742s\n",
            "completed batch 44 of epoch 123. loss=0.008074390701949596. train batch time cost=0.09500980377197266s\n",
            "completed batch 45 of epoch 123. loss=0.041418880224227905. train batch time cost=0.09692168235778809s\n",
            "completed batch 46 of epoch 123. loss=0.0676533430814743. train batch time cost=0.09527015686035156s\n",
            "completed batch 47 of epoch 123. loss=0.015283494256436825. train batch time cost=0.09418106079101562s\n",
            "completed batch 48 of epoch 123. loss=0.10833805799484253. train batch time cost=0.09609293937683105s\n",
            "completed batch 49 of epoch 123. loss=0.008478865027427673. train batch time cost=0.09552669525146484s\n",
            "completed batch 50 of epoch 123. loss=0.05612622946500778. train batch time cost=0.09511470794677734s\n",
            "completed batch 51 of epoch 123. loss=0.08947955071926117. train batch time cost=0.09709620475769043s\n",
            "completed batch 52 of epoch 123. loss=0.02389048971235752. train batch time cost=0.09520649909973145s\n",
            "completed batch 53 of epoch 123. loss=0.005237284582108259. train batch time cost=0.09741950035095215s\n",
            "completed batch 54 of epoch 123. loss=0.1307367980480194. train batch time cost=0.09536027908325195s\n",
            "completed batch 55 of epoch 123. loss=0.027860362082719803. train batch time cost=0.0952301025390625s\n",
            "completed batch 56 of epoch 123. loss=0.033202629536390305. train batch time cost=0.0952003002166748s\n",
            "completed batch 57 of epoch 123. loss=0.03673925995826721. train batch time cost=0.09501791000366211s\n",
            "completed batch 58 of epoch 123. loss=0.01256517507135868. train batch time cost=0.09496068954467773s\n",
            "completed batch 59 of epoch 123. loss=0.024775393307209015. train batch time cost=0.10217475891113281s\n",
            "completed batch 60 of epoch 123. loss=0.017119236290454865. train batch time cost=0.10068607330322266s\n",
            "completed batch 61 of epoch 123. loss=0.005692313425242901. train batch time cost=0.10132217407226562s\n",
            "completed batch 62 of epoch 123. loss=0.00947119016200304. train batch time cost=0.10146021842956543s\n",
            "completed batch 63 of epoch 123. loss=0.0026500674430280924. train batch time cost=0.10192561149597168s\n",
            "completed batch 64 of epoch 123. loss=0.011895929463207722. train batch time cost=0.10292387008666992s\n",
            "completed batch 65 of epoch 123. loss=0.015871483832597733. train batch time cost=0.10164427757263184s\n",
            "completed batch 66 of epoch 123. loss=0.0062047624960541725. train batch time cost=0.10142683982849121s\n",
            "completed batch 67 of epoch 123. loss=0.04496686905622482. train batch time cost=0.10148358345031738s\n",
            "completed batch 68 of epoch 123. loss=0.03218545764684677. train batch time cost=0.10363340377807617s\n",
            "completed batch 69 of epoch 123. loss=0.011090122163295746. train batch time cost=0.10532188415527344s\n",
            "completed batch 70 of epoch 123. loss=0.008363936096429825. train batch time cost=0.10191631317138672s\n",
            "completed batch 71 of epoch 123. loss=0.031555626541376114. train batch time cost=0.10311317443847656s\n",
            "completed batch 72 of epoch 123. loss=0.008273385465145111. train batch time cost=0.10348844528198242s\n",
            "completed batch 73 of epoch 123. loss=0.008219118230044842. train batch time cost=0.10567450523376465s\n",
            "completed batch 74 of epoch 123. loss=0.0411427766084671. train batch time cost=0.10205984115600586s\n",
            "completed batch 75 of epoch 123. loss=0.012727922759950161. train batch time cost=0.10370635986328125s\n",
            "completed batch 76 of epoch 123. loss=0.009116576053202152. train batch time cost=0.10416388511657715s\n",
            "completed batch 77 of epoch 123. loss=0.035139866173267365. train batch time cost=0.10384035110473633s\n",
            "completed batch 78 of epoch 123. loss=0.01197575032711029. train batch time cost=0.10238027572631836s\n",
            "completed batch 79 of epoch 123. loss=0.006737586110830307. train batch time cost=0.10196781158447266s\n",
            "completed batch 80 of epoch 123. loss=0.055208753794431686. train batch time cost=0.1037294864654541s\n",
            "completed batch 81 of epoch 123. loss=0.12184733152389526. train batch time cost=0.10257816314697266s\n",
            "completed batch 82 of epoch 123. loss=0.019233735278248787. train batch time cost=0.10274100303649902s\n",
            "completed batch 83 of epoch 123. loss=0.03151072561740875. train batch time cost=0.10222363471984863s\n",
            "completed batch 84 of epoch 123. loss=0.004865601193159819. train batch time cost=0.1022789478302002s\n",
            "completed batch 85 of epoch 123. loss=0.030435366556048393. train batch time cost=0.10189580917358398s\n",
            "completed batch 86 of epoch 123. loss=0.0028442521579563618. train batch time cost=0.10031652450561523s\n",
            "completed batch 87 of epoch 123. loss=0.007425624877214432. train batch time cost=0.10122060775756836s\n",
            "completed batch 88 of epoch 123. loss=0.008208896033465862. train batch time cost=0.10030150413513184s\n",
            "completed batch 89 of epoch 123. loss=0.009144492447376251. train batch time cost=0.10102105140686035s\n",
            "completed batch 90 of epoch 123. loss=0.023619692772626877. train batch time cost=0.10169005393981934s\n",
            "completed batch 91 of epoch 123. loss=0.02376590296626091. train batch time cost=0.10370469093322754s\n",
            "completed batch 92 of epoch 123. loss=0.032941460609436035. train batch time cost=0.10140490531921387s\n",
            "completed batch 93 of epoch 123. loss=0.007857379503548145. train batch time cost=0.10134720802307129s\n",
            "completed batch 94 of epoch 123. loss=0.05481899157166481. train batch time cost=0.10289192199707031s\n",
            "completed batch 95 of epoch 123. loss=0.07198872417211533. train batch time cost=0.10110878944396973s\n",
            "completed batch 96 of epoch 123. loss=0.01393613126128912. train batch time cost=0.10139584541320801s\n",
            "completed batch 97 of epoch 123. loss=0.024665966629981995. train batch time cost=0.10051941871643066s\n",
            "completed batch 98 of epoch 123. loss=0.03557589650154114. train batch time cost=0.10138678550720215s\n",
            "completed batch 99 of epoch 123. loss=0.009682469069957733. train batch time cost=0.10288214683532715s\n",
            "completed batch 100 of epoch 123. loss=0.002982407109811902. train batch time cost=0.10245490074157715s\n",
            "completed batch 101 of epoch 123. loss=0.013341682963073254. train batch time cost=0.10221457481384277s\n",
            "completed batch 102 of epoch 123. loss=0.01223941519856453. train batch time cost=0.10272407531738281s\n",
            "completed batch 103 of epoch 123. loss=0.03173818811774254. train batch time cost=0.10309290885925293s\n",
            "completed batch 104 of epoch 123. loss=0.0059500038623809814. train batch time cost=0.10280346870422363s\n",
            "completed batch 105 of epoch 123. loss=0.009091073647141457. train batch time cost=0.10100841522216797s\n",
            "completed batch 106 of epoch 123. loss=0.017260944470763206. train batch time cost=0.10105180740356445s\n",
            "completed batch 107 of epoch 123. loss=0.17797018587589264. train batch time cost=0.10199666023254395s\n",
            "completed batch 108 of epoch 123. loss=0.0789363905787468. train batch time cost=0.10138559341430664s\n",
            "completed batch 109 of epoch 123. loss=0.013531186617910862. train batch time cost=0.10153675079345703s\n",
            "completed batch 110 of epoch 123. loss=0.01483215857297182. train batch time cost=0.10190582275390625s\n",
            "completed batch 111 of epoch 123. loss=0.005632956512272358. train batch time cost=0.10697436332702637s\n",
            "completed batch 112 of epoch 123. loss=0.013111594133079052. train batch time cost=0.10457491874694824s\n",
            "completed batch 113 of epoch 123. loss=0.012155912816524506. train batch time cost=0.10255241394042969s\n",
            "completed batch 114 of epoch 123. loss=0.014585412107408047. train batch time cost=0.10138249397277832s\n",
            "completed batch 115 of epoch 123. loss=0.016161831095814705. train batch time cost=0.10048556327819824s\n",
            "completed batch 116 of epoch 123. loss=0.017096376046538353. train batch time cost=0.09613871574401855s\n",
            "completed batch 117 of epoch 123. loss=0.0027086741756647825. train batch time cost=0.09549975395202637s\n",
            "completed batch 118 of epoch 123. loss=0.005190571770071983. train batch time cost=0.09593939781188965s\n",
            "completed batch 119 of epoch 123. loss=0.01672651618719101. train batch time cost=0.09482288360595703s\n",
            "completed batch 120 of epoch 123. loss=0.14444474875926971. train batch time cost=0.09467720985412598s\n",
            "completed batch 121 of epoch 123. loss=0.004896052181720734. train batch time cost=0.09518837928771973s\n",
            "completed batch 122 of epoch 123. loss=0.013749264180660248. train batch time cost=0.09717822074890137s\n",
            "completed batch 123 of epoch 123. loss=0.011755898594856262. train batch time cost=0.09553694725036621s\n",
            "completed batch 124 of epoch 123. loss=0.037861138582229614. train batch time cost=0.10377335548400879s\n",
            "completed batch 125 of epoch 123. loss=0.015508397482335567. train batch time cost=0.10467314720153809s\n",
            "completed batch 126 of epoch 123. loss=0.00024857805692590773. train batch time cost=0.030258655548095703s\n",
            "completed test of epoch 123. loss=0.00024857805692590773. accuracy=0.6989515726410385. train one epoch time cost=27.484641790390015s, test validation time cost=3.9734275341033936\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385]\n",
            "completed batch 1 of epoch 124. loss=0.014205751940608025. train batch time cost=0.10171198844909668s\n",
            "completed batch 2 of epoch 124. loss=0.046785105019807816. train batch time cost=0.1022958755493164s\n",
            "completed batch 3 of epoch 124. loss=0.01961727812886238. train batch time cost=0.10121536254882812s\n",
            "completed batch 4 of epoch 124. loss=0.010654688812792301. train batch time cost=0.1009070873260498s\n",
            "completed batch 5 of epoch 124. loss=0.056190744042396545. train batch time cost=0.10099267959594727s\n",
            "completed batch 6 of epoch 124. loss=0.06625248491764069. train batch time cost=0.10200881958007812s\n",
            "completed batch 7 of epoch 124. loss=0.03388984501361847. train batch time cost=0.1035921573638916s\n",
            "completed batch 8 of epoch 124. loss=0.03454088419675827. train batch time cost=0.10265517234802246s\n",
            "completed batch 9 of epoch 124. loss=0.08691781759262085. train batch time cost=0.10171985626220703s\n",
            "completed batch 10 of epoch 124. loss=0.002870874712243676. train batch time cost=0.10698986053466797s\n",
            "completed batch 11 of epoch 124. loss=0.029451483860611916. train batch time cost=0.10142374038696289s\n",
            "completed batch 12 of epoch 124. loss=0.048377811908721924. train batch time cost=0.1015939712524414s\n",
            "completed batch 13 of epoch 124. loss=0.10113820433616638. train batch time cost=0.10229802131652832s\n",
            "completed batch 14 of epoch 124. loss=0.1137186735868454. train batch time cost=0.10225677490234375s\n",
            "completed batch 15 of epoch 124. loss=0.0065948632545769215. train batch time cost=0.10225367546081543s\n",
            "completed batch 16 of epoch 124. loss=0.0522465780377388. train batch time cost=0.10290074348449707s\n",
            "completed batch 17 of epoch 124. loss=0.005173957906663418. train batch time cost=0.10212850570678711s\n",
            "completed batch 18 of epoch 124. loss=0.023495571687817574. train batch time cost=0.10147714614868164s\n",
            "completed batch 19 of epoch 124. loss=0.03504318743944168. train batch time cost=0.10235476493835449s\n",
            "completed batch 20 of epoch 124. loss=0.06934966892004013. train batch time cost=0.10228800773620605s\n",
            "completed batch 21 of epoch 124. loss=0.011779614724218845. train batch time cost=0.10233402252197266s\n",
            "completed batch 22 of epoch 124. loss=0.02437991090118885. train batch time cost=0.10197997093200684s\n",
            "completed batch 23 of epoch 124. loss=0.018034253269433975. train batch time cost=0.10202932357788086s\n",
            "completed batch 24 of epoch 124. loss=0.023789366707205772. train batch time cost=0.10358619689941406s\n",
            "completed batch 25 of epoch 124. loss=0.008756482973694801. train batch time cost=0.10427379608154297s\n",
            "completed batch 26 of epoch 124. loss=0.002455084351822734. train batch time cost=0.10244226455688477s\n",
            "completed batch 27 of epoch 124. loss=0.020588507875800133. train batch time cost=0.10158967971801758s\n",
            "completed batch 28 of epoch 124. loss=0.02273344248533249. train batch time cost=0.10247468948364258s\n",
            "completed batch 29 of epoch 124. loss=0.010241898708045483. train batch time cost=0.10277676582336426s\n",
            "completed batch 30 of epoch 124. loss=0.03858773037791252. train batch time cost=0.10113644599914551s\n",
            "completed batch 31 of epoch 124. loss=0.011203532107174397. train batch time cost=0.10105752944946289s\n",
            "completed batch 32 of epoch 124. loss=0.02980703115463257. train batch time cost=0.10102319717407227s\n",
            "completed batch 33 of epoch 124. loss=0.0674850195646286. train batch time cost=0.10345649719238281s\n",
            "completed batch 34 of epoch 124. loss=0.02040165662765503. train batch time cost=0.10079073905944824s\n",
            "completed batch 35 of epoch 124. loss=0.011037962511181831. train batch time cost=0.09557294845581055s\n",
            "completed batch 36 of epoch 124. loss=0.011950443498790264. train batch time cost=0.09531545639038086s\n",
            "completed batch 37 of epoch 124. loss=0.008394257165491581. train batch time cost=0.0993037223815918s\n",
            "completed batch 38 of epoch 124. loss=0.0039077880792319775. train batch time cost=0.1006767749786377s\n",
            "completed batch 39 of epoch 124. loss=0.21504366397857666. train batch time cost=0.10037946701049805s\n",
            "completed batch 40 of epoch 124. loss=0.012558070942759514. train batch time cost=0.09957551956176758s\n",
            "completed batch 41 of epoch 124. loss=0.012077202089130878. train batch time cost=0.09926080703735352s\n",
            "completed batch 42 of epoch 124. loss=0.011161195114254951. train batch time cost=0.09992003440856934s\n",
            "completed batch 43 of epoch 124. loss=0.011748690158128738. train batch time cost=0.09881830215454102s\n",
            "completed batch 44 of epoch 124. loss=0.04115724191069603. train batch time cost=0.09757304191589355s\n",
            "completed batch 45 of epoch 124. loss=0.007197868544608355. train batch time cost=0.0988013744354248s\n",
            "completed batch 46 of epoch 124. loss=0.04412699118256569. train batch time cost=0.10315895080566406s\n",
            "completed batch 47 of epoch 124. loss=0.033998165279626846. train batch time cost=0.10245227813720703s\n",
            "completed batch 48 of epoch 124. loss=0.012822133488953114. train batch time cost=0.10273146629333496s\n",
            "completed batch 49 of epoch 124. loss=0.006671994924545288. train batch time cost=0.10146689414978027s\n",
            "completed batch 50 of epoch 124. loss=0.011707805097103119. train batch time cost=0.10227131843566895s\n",
            "completed batch 51 of epoch 124. loss=0.005072937346994877. train batch time cost=0.10145854949951172s\n",
            "completed batch 52 of epoch 124. loss=0.007400823757052422. train batch time cost=0.10269641876220703s\n",
            "completed batch 53 of epoch 124. loss=0.013598650693893433. train batch time cost=0.1022176742553711s\n",
            "completed batch 54 of epoch 124. loss=0.007899893447756767. train batch time cost=0.10158085823059082s\n",
            "completed batch 55 of epoch 124. loss=0.01592470146715641. train batch time cost=0.10253548622131348s\n",
            "completed batch 56 of epoch 124. loss=0.01811961457133293. train batch time cost=0.1047210693359375s\n",
            "completed batch 57 of epoch 124. loss=0.14768631756305695. train batch time cost=0.10287952423095703s\n",
            "completed batch 58 of epoch 124. loss=0.09897493571043015. train batch time cost=0.10166430473327637s\n",
            "completed batch 59 of epoch 124. loss=0.040602654218673706. train batch time cost=0.10194873809814453s\n",
            "completed batch 60 of epoch 124. loss=0.07622343301773071. train batch time cost=0.10159182548522949s\n",
            "completed batch 61 of epoch 124. loss=0.016024017706513405. train batch time cost=0.11046338081359863s\n",
            "completed batch 62 of epoch 124. loss=0.020663931965827942. train batch time cost=0.10146379470825195s\n",
            "completed batch 63 of epoch 124. loss=0.0770740658044815. train batch time cost=0.10105347633361816s\n",
            "completed batch 64 of epoch 124. loss=0.05965228006243706. train batch time cost=0.10094618797302246s\n",
            "completed batch 65 of epoch 124. loss=0.15408094227313995. train batch time cost=0.10267448425292969s\n",
            "completed batch 66 of epoch 124. loss=0.018743470311164856. train batch time cost=0.10384345054626465s\n",
            "completed batch 67 of epoch 124. loss=0.05437704548239708. train batch time cost=0.10110020637512207s\n",
            "completed batch 68 of epoch 124. loss=0.08139127492904663. train batch time cost=0.10065817832946777s\n",
            "completed batch 69 of epoch 124. loss=0.013459328562021255. train batch time cost=0.1015620231628418s\n",
            "completed batch 70 of epoch 124. loss=0.012221715413033962. train batch time cost=0.10087728500366211s\n",
            "completed batch 71 of epoch 124. loss=0.08067697286605835. train batch time cost=0.10045242309570312s\n",
            "completed batch 72 of epoch 124. loss=0.11918342113494873. train batch time cost=0.10004734992980957s\n",
            "completed batch 73 of epoch 124. loss=0.025968609377741814. train batch time cost=0.10030007362365723s\n",
            "completed batch 74 of epoch 124. loss=0.10075235366821289. train batch time cost=0.10218477249145508s\n",
            "completed batch 75 of epoch 124. loss=0.047765377908945084. train batch time cost=0.10248112678527832s\n",
            "completed batch 76 of epoch 124. loss=0.0325428806245327. train batch time cost=0.10013198852539062s\n",
            "completed batch 77 of epoch 124. loss=0.019050614908337593. train batch time cost=0.10033941268920898s\n",
            "completed batch 78 of epoch 124. loss=0.01840023510158062. train batch time cost=0.09946966171264648s\n",
            "completed batch 79 of epoch 124. loss=0.1642647087574005. train batch time cost=0.10095381736755371s\n",
            "completed batch 80 of epoch 124. loss=0.044976670295000076. train batch time cost=0.10034394264221191s\n",
            "completed batch 81 of epoch 124. loss=0.025220677256584167. train batch time cost=0.1006007194519043s\n",
            "completed batch 82 of epoch 124. loss=0.03090748004615307. train batch time cost=0.10047411918640137s\n",
            "completed batch 83 of epoch 124. loss=0.10739398002624512. train batch time cost=0.10097861289978027s\n",
            "completed batch 84 of epoch 124. loss=0.07148166000843048. train batch time cost=0.10147976875305176s\n",
            "completed batch 85 of epoch 124. loss=0.062247712165117264. train batch time cost=0.10210347175598145s\n",
            "completed batch 86 of epoch 124. loss=0.08625343441963196. train batch time cost=0.10191798210144043s\n",
            "completed batch 87 of epoch 124. loss=0.04267687723040581. train batch time cost=0.1009666919708252s\n",
            "completed batch 88 of epoch 124. loss=0.13557228446006775. train batch time cost=0.10137438774108887s\n",
            "completed batch 89 of epoch 124. loss=0.14128737151622772. train batch time cost=0.1034703254699707s\n",
            "completed batch 90 of epoch 124. loss=0.10118867456912994. train batch time cost=0.10161113739013672s\n",
            "completed batch 91 of epoch 124. loss=0.008940392173826694. train batch time cost=0.10163021087646484s\n",
            "completed batch 92 of epoch 124. loss=0.01414104737341404. train batch time cost=0.10113072395324707s\n",
            "completed batch 93 of epoch 124. loss=0.08316279947757721. train batch time cost=0.10331082344055176s\n",
            "completed batch 94 of epoch 124. loss=0.052036747336387634. train batch time cost=0.10316801071166992s\n",
            "completed batch 95 of epoch 124. loss=0.05695679038763046. train batch time cost=0.10241389274597168s\n",
            "completed batch 96 of epoch 124. loss=0.026139894500374794. train batch time cost=0.10376787185668945s\n",
            "completed batch 97 of epoch 124. loss=0.12058913707733154. train batch time cost=0.10403752326965332s\n",
            "completed batch 98 of epoch 124. loss=0.05964578688144684. train batch time cost=0.1012721061706543s\n",
            "completed batch 99 of epoch 124. loss=0.10294948518276215. train batch time cost=0.1017916202545166s\n",
            "completed batch 100 of epoch 124. loss=0.0379701666533947. train batch time cost=0.10183334350585938s\n",
            "completed batch 101 of epoch 124. loss=0.25677669048309326. train batch time cost=0.10162043571472168s\n",
            "completed batch 102 of epoch 124. loss=0.014098080806434155. train batch time cost=0.10167193412780762s\n",
            "completed batch 103 of epoch 124. loss=0.08420474827289581. train batch time cost=0.10220742225646973s\n",
            "completed batch 104 of epoch 124. loss=0.02726455219089985. train batch time cost=0.10118246078491211s\n",
            "completed batch 105 of epoch 124. loss=0.11225450783967972. train batch time cost=0.10030508041381836s\n",
            "completed batch 106 of epoch 124. loss=0.027852030470967293. train batch time cost=0.10174775123596191s\n",
            "completed batch 107 of epoch 124. loss=0.063227578997612. train batch time cost=0.10037851333618164s\n",
            "completed batch 108 of epoch 124. loss=0.052188772708177567. train batch time cost=0.10095095634460449s\n",
            "completed batch 109 of epoch 124. loss=0.025230295956134796. train batch time cost=0.10025906562805176s\n",
            "completed batch 110 of epoch 124. loss=0.06058899313211441. train batch time cost=0.10065531730651855s\n",
            "completed batch 111 of epoch 124. loss=0.06374713778495789. train batch time cost=0.10133242607116699s\n",
            "completed batch 112 of epoch 124. loss=0.18781626224517822. train batch time cost=0.10302853584289551s\n",
            "completed batch 113 of epoch 124. loss=0.03916404768824577. train batch time cost=0.10195088386535645s\n",
            "completed batch 114 of epoch 124. loss=0.15921156108379364. train batch time cost=0.1019124984741211s\n",
            "completed batch 115 of epoch 124. loss=0.022145740687847137. train batch time cost=0.1029517650604248s\n",
            "completed batch 116 of epoch 124. loss=0.16599519550800323. train batch time cost=0.09694266319274902s\n",
            "completed batch 117 of epoch 124. loss=0.02488875947892666. train batch time cost=0.0951235294342041s\n",
            "completed batch 118 of epoch 124. loss=0.055282074958086014. train batch time cost=0.0946645736694336s\n",
            "completed batch 119 of epoch 124. loss=0.030923979356884956. train batch time cost=0.09781813621520996s\n",
            "completed batch 120 of epoch 124. loss=0.04117845371365547. train batch time cost=0.09604001045227051s\n",
            "completed batch 121 of epoch 124. loss=0.04195684194564819. train batch time cost=0.09702110290527344s\n",
            "completed batch 122 of epoch 124. loss=0.03681798651814461. train batch time cost=0.09526586532592773s\n",
            "completed batch 123 of epoch 124. loss=0.051642656326293945. train batch time cost=0.09519267082214355s\n",
            "completed batch 124 of epoch 124. loss=0.05851412191987038. train batch time cost=0.09595465660095215s\n",
            "completed batch 125 of epoch 124. loss=0.0448538139462471. train batch time cost=0.09597277641296387s\n",
            "completed batch 126 of epoch 124. loss=0.0018481031293049455. train batch time cost=0.02907705307006836s\n",
            "completed test of epoch 124. loss=0.0018481031293049455. accuracy=0.72690963554668. train one epoch time cost=27.601280450820923s, test validation time cost=3.7624475955963135\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668]\n",
            "completed batch 1 of epoch 125. loss=0.025749221444129944. train batch time cost=0.09443426132202148s\n",
            "completed batch 2 of epoch 125. loss=0.08428622037172318. train batch time cost=0.10386157035827637s\n",
            "completed batch 3 of epoch 125. loss=0.11264348775148392. train batch time cost=0.10208773612976074s\n",
            "completed batch 4 of epoch 125. loss=0.04551461711525917. train batch time cost=0.10146236419677734s\n",
            "completed batch 5 of epoch 125. loss=0.02068253792822361. train batch time cost=0.10193967819213867s\n",
            "completed batch 6 of epoch 125. loss=0.01804531179368496. train batch time cost=0.10113239288330078s\n",
            "completed batch 7 of epoch 125. loss=0.024750377982854843. train batch time cost=0.10298633575439453s\n",
            "completed batch 8 of epoch 125. loss=0.05671892315149307. train batch time cost=0.10108399391174316s\n",
            "completed batch 9 of epoch 125. loss=0.1407388299703598. train batch time cost=0.10115623474121094s\n",
            "completed batch 10 of epoch 125. loss=0.08301191031932831. train batch time cost=0.10060405731201172s\n",
            "completed batch 11 of epoch 125. loss=0.0848146304488182. train batch time cost=0.10277533531188965s\n",
            "completed batch 12 of epoch 125. loss=0.05700374394655228. train batch time cost=0.1031196117401123s\n",
            "completed batch 13 of epoch 125. loss=0.03444179892539978. train batch time cost=0.10208535194396973s\n",
            "completed batch 14 of epoch 125. loss=0.037789929658174515. train batch time cost=0.10602378845214844s\n",
            "completed batch 15 of epoch 125. loss=0.0723937526345253. train batch time cost=0.10281825065612793s\n",
            "completed batch 16 of epoch 125. loss=0.06387732177972794. train batch time cost=0.10253381729125977s\n",
            "completed batch 17 of epoch 125. loss=0.03290871903300285. train batch time cost=0.10378098487854004s\n",
            "completed batch 18 of epoch 125. loss=0.01748552918434143. train batch time cost=0.10116171836853027s\n",
            "completed batch 19 of epoch 125. loss=0.008663909509778023. train batch time cost=0.10288524627685547s\n",
            "completed batch 20 of epoch 125. loss=0.009535402059555054. train batch time cost=0.10333704948425293s\n",
            "completed batch 21 of epoch 125. loss=0.005925644189119339. train batch time cost=0.10148143768310547s\n",
            "completed batch 22 of epoch 125. loss=0.013182427734136581. train batch time cost=0.10061407089233398s\n",
            "completed batch 23 of epoch 125. loss=0.10647623240947723. train batch time cost=0.10217738151550293s\n",
            "completed batch 24 of epoch 125. loss=0.05520396679639816. train batch time cost=0.10123014450073242s\n",
            "completed batch 25 of epoch 125. loss=0.006613287143409252. train batch time cost=0.10111260414123535s\n",
            "completed batch 26 of epoch 125. loss=0.005966163240373135. train batch time cost=0.10179018974304199s\n",
            "completed batch 27 of epoch 125. loss=0.046315502375364304. train batch time cost=0.1007530689239502s\n",
            "completed batch 28 of epoch 125. loss=0.021708056330680847. train batch time cost=0.1008448600769043s\n",
            "completed batch 29 of epoch 125. loss=0.042345233261585236. train batch time cost=0.1005403995513916s\n",
            "completed batch 30 of epoch 125. loss=0.05102111026644707. train batch time cost=0.1010122299194336s\n",
            "completed batch 31 of epoch 125. loss=0.10122179239988327. train batch time cost=0.10116243362426758s\n",
            "completed batch 32 of epoch 125. loss=0.04173138365149498. train batch time cost=0.10150337219238281s\n",
            "completed batch 33 of epoch 125. loss=0.009158879518508911. train batch time cost=0.10214805603027344s\n",
            "completed batch 34 of epoch 125. loss=0.02522585541009903. train batch time cost=0.10208797454833984s\n",
            "completed batch 35 of epoch 125. loss=0.02725869044661522. train batch time cost=0.1021275520324707s\n",
            "completed batch 36 of epoch 125. loss=0.006946312729269266. train batch time cost=0.10299158096313477s\n",
            "completed batch 37 of epoch 125. loss=0.05442358925938606. train batch time cost=0.10239624977111816s\n",
            "completed batch 38 of epoch 125. loss=0.09141109883785248. train batch time cost=0.10203766822814941s\n",
            "completed batch 39 of epoch 125. loss=0.03998372331261635. train batch time cost=0.10185527801513672s\n",
            "completed batch 40 of epoch 125. loss=0.02221107669174671. train batch time cost=0.09637570381164551s\n",
            "completed batch 41 of epoch 125. loss=0.02130751684308052. train batch time cost=0.09400391578674316s\n",
            "completed batch 42 of epoch 125. loss=0.019782228395342827. train batch time cost=0.1023399829864502s\n",
            "completed batch 43 of epoch 125. loss=0.01562543585896492. train batch time cost=0.10311031341552734s\n",
            "completed batch 44 of epoch 125. loss=0.03293916955590248. train batch time cost=0.1114952564239502s\n",
            "completed batch 45 of epoch 125. loss=0.04240508750081062. train batch time cost=0.10301375389099121s\n",
            "completed batch 46 of epoch 125. loss=0.010751944035291672. train batch time cost=0.09830236434936523s\n",
            "completed batch 47 of epoch 125. loss=0.007575090043246746. train batch time cost=0.09529900550842285s\n",
            "completed batch 48 of epoch 125. loss=0.008585482835769653. train batch time cost=0.09534430503845215s\n",
            "completed batch 49 of epoch 125. loss=0.011146355420351028. train batch time cost=0.0958561897277832s\n",
            "completed batch 50 of epoch 125. loss=0.0356469489634037. train batch time cost=0.09483671188354492s\n",
            "completed batch 51 of epoch 125. loss=0.03224305063486099. train batch time cost=0.09483909606933594s\n",
            "completed batch 52 of epoch 125. loss=0.0436406210064888. train batch time cost=0.09482645988464355s\n",
            "completed batch 53 of epoch 125. loss=0.018973754718899727. train batch time cost=0.09476566314697266s\n",
            "completed batch 54 of epoch 125. loss=0.05499221757054329. train batch time cost=0.09625887870788574s\n",
            "completed batch 55 of epoch 125. loss=0.22500000894069672. train batch time cost=0.09545302391052246s\n",
            "completed batch 56 of epoch 125. loss=0.013260586187243462. train batch time cost=0.09579133987426758s\n",
            "completed batch 57 of epoch 125. loss=0.03937486559152603. train batch time cost=0.09448909759521484s\n",
            "completed batch 58 of epoch 125. loss=0.03286122903227806. train batch time cost=0.09502530097961426s\n",
            "completed batch 59 of epoch 125. loss=0.0016440695617347956. train batch time cost=0.09738826751708984s\n",
            "completed batch 60 of epoch 125. loss=0.09124691039323807. train batch time cost=0.10102510452270508s\n",
            "completed batch 61 of epoch 125. loss=0.010612821206450462. train batch time cost=0.10123300552368164s\n",
            "completed batch 62 of epoch 125. loss=0.024590281769633293. train batch time cost=0.10136032104492188s\n",
            "completed batch 63 of epoch 125. loss=0.024960331618785858. train batch time cost=0.10061454772949219s\n",
            "completed batch 64 of epoch 125. loss=0.013408767990767956. train batch time cost=0.10251212120056152s\n",
            "completed batch 65 of epoch 125. loss=0.004637879319489002. train batch time cost=0.10400128364562988s\n",
            "completed batch 66 of epoch 125. loss=0.018868112936615944. train batch time cost=0.1035921573638916s\n",
            "completed batch 67 of epoch 125. loss=0.02716759592294693. train batch time cost=0.10174179077148438s\n",
            "completed batch 68 of epoch 125. loss=0.01235648151487112. train batch time cost=0.1013329029083252s\n",
            "completed batch 69 of epoch 125. loss=0.0077895610593259335. train batch time cost=0.10202360153198242s\n",
            "completed batch 70 of epoch 125. loss=0.015454302541911602. train batch time cost=0.10268640518188477s\n",
            "completed batch 71 of epoch 125. loss=0.028372319415211678. train batch time cost=0.10291481018066406s\n",
            "completed batch 72 of epoch 125. loss=0.053801409900188446. train batch time cost=0.10283088684082031s\n",
            "completed batch 73 of epoch 125. loss=0.04804566130042076. train batch time cost=0.1023411750793457s\n",
            "completed batch 74 of epoch 125. loss=0.005757056176662445. train batch time cost=0.10280513763427734s\n",
            "completed batch 75 of epoch 125. loss=0.0801776796579361. train batch time cost=0.10312771797180176s\n",
            "completed batch 76 of epoch 125. loss=0.00762412091717124. train batch time cost=0.10343122482299805s\n",
            "completed batch 77 of epoch 125. loss=0.03688310086727142. train batch time cost=0.10323858261108398s\n",
            "completed batch 78 of epoch 125. loss=0.06344327330589294. train batch time cost=0.1045536994934082s\n",
            "completed batch 79 of epoch 125. loss=0.06146556884050369. train batch time cost=0.10241842269897461s\n",
            "completed batch 80 of epoch 125. loss=0.0036924630403518677. train batch time cost=0.10283279418945312s\n",
            "completed batch 81 of epoch 125. loss=0.07226785272359848. train batch time cost=0.10136675834655762s\n",
            "completed batch 82 of epoch 125. loss=0.026992762461304665. train batch time cost=0.10145354270935059s\n",
            "completed batch 83 of epoch 125. loss=0.08430209755897522. train batch time cost=0.10215306282043457s\n",
            "completed batch 84 of epoch 125. loss=0.013059559278190136. train batch time cost=0.10119152069091797s\n",
            "completed batch 85 of epoch 125. loss=0.03282126784324646. train batch time cost=0.10223698616027832s\n",
            "completed batch 86 of epoch 125. loss=0.02291852980852127. train batch time cost=0.10191488265991211s\n",
            "completed batch 87 of epoch 125. loss=0.028395820409059525. train batch time cost=0.10103678703308105s\n",
            "completed batch 88 of epoch 125. loss=0.028751114383339882. train batch time cost=0.10233855247497559s\n",
            "completed batch 89 of epoch 125. loss=0.05363214761018753. train batch time cost=0.10198330879211426s\n",
            "completed batch 90 of epoch 125. loss=0.03396552801132202. train batch time cost=0.1019754409790039s\n",
            "completed batch 91 of epoch 125. loss=0.08459289371967316. train batch time cost=0.1013336181640625s\n",
            "completed batch 92 of epoch 125. loss=0.02105071395635605. train batch time cost=0.10197782516479492s\n",
            "completed batch 93 of epoch 125. loss=0.026858029887080193. train batch time cost=0.10195064544677734s\n",
            "completed batch 94 of epoch 125. loss=0.04409770667552948. train batch time cost=0.10266757011413574s\n",
            "completed batch 95 of epoch 125. loss=0.018588121980428696. train batch time cost=0.10196137428283691s\n",
            "completed batch 96 of epoch 125. loss=0.018307551741600037. train batch time cost=0.10303783416748047s\n",
            "completed batch 97 of epoch 125. loss=0.02301551215350628. train batch time cost=0.10082030296325684s\n",
            "completed batch 98 of epoch 125. loss=0.03501707315444946. train batch time cost=0.10031604766845703s\n",
            "completed batch 99 of epoch 125. loss=0.017478862777352333. train batch time cost=0.10038304328918457s\n",
            "completed batch 100 of epoch 125. loss=0.00928445253521204. train batch time cost=0.10095643997192383s\n",
            "completed batch 101 of epoch 125. loss=0.069880910217762. train batch time cost=0.100250244140625s\n",
            "completed batch 102 of epoch 125. loss=0.06913987547159195. train batch time cost=0.10099673271179199s\n",
            "completed batch 103 of epoch 125. loss=0.012909962795674801. train batch time cost=0.10039305686950684s\n",
            "completed batch 104 of epoch 125. loss=0.050865866243839264. train batch time cost=0.10375046730041504s\n",
            "completed batch 105 of epoch 125. loss=0.039584092795848846. train batch time cost=0.10202264785766602s\n",
            "completed batch 106 of epoch 125. loss=0.027880802750587463. train batch time cost=0.10078954696655273s\n",
            "completed batch 107 of epoch 125. loss=0.03774731606245041. train batch time cost=0.10155940055847168s\n",
            "completed batch 108 of epoch 125. loss=0.01733803004026413. train batch time cost=0.10184836387634277s\n",
            "completed batch 109 of epoch 125. loss=0.008665695786476135. train batch time cost=0.10233426094055176s\n",
            "completed batch 110 of epoch 125. loss=0.032544925808906555. train batch time cost=0.10116147994995117s\n",
            "completed batch 111 of epoch 125. loss=0.02564397267997265. train batch time cost=0.10182857513427734s\n",
            "completed batch 112 of epoch 125. loss=0.006137786898761988. train batch time cost=0.10173177719116211s\n",
            "completed batch 113 of epoch 125. loss=0.008331884630024433. train batch time cost=0.10184955596923828s\n",
            "completed batch 114 of epoch 125. loss=0.011036469601094723. train batch time cost=0.10816764831542969s\n",
            "completed batch 115 of epoch 125. loss=0.012921089306473732. train batch time cost=0.10134196281433105s\n",
            "completed batch 116 of epoch 125. loss=0.03999697417020798. train batch time cost=0.10115981101989746s\n",
            "completed batch 117 of epoch 125. loss=0.016791928559541702. train batch time cost=0.10198616981506348s\n",
            "completed batch 118 of epoch 125. loss=0.03395770117640495. train batch time cost=0.1005258560180664s\n",
            "completed batch 119 of epoch 125. loss=0.02382487989962101. train batch time cost=0.10102629661560059s\n",
            "completed batch 120 of epoch 125. loss=0.0019202845869585872. train batch time cost=0.09960317611694336s\n",
            "completed batch 121 of epoch 125. loss=0.016918128356337547. train batch time cost=0.10191154479980469s\n",
            "completed batch 122 of epoch 125. loss=0.029437825083732605. train batch time cost=0.10138130187988281s\n",
            "completed batch 123 of epoch 125. loss=0.01475897803902626. train batch time cost=0.10162687301635742s\n",
            "completed batch 124 of epoch 125. loss=0.016451260074973106. train batch time cost=0.09858226776123047s\n",
            "completed batch 125 of epoch 125. loss=0.01522491592913866. train batch time cost=0.09892606735229492s\n",
            "completed batch 126 of epoch 125. loss=0.0029289249796420336. train batch time cost=0.029851913452148438s\n",
            "completed test of epoch 125. loss=0.0029289249796420336. accuracy=0.6984523215177234. train one epoch time cost=27.582526922225952s, test validation time cost=3.8549370765686035\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234]\n",
            "completed batch 1 of epoch 126. loss=0.009622314013540745. train batch time cost=0.10228848457336426s\n",
            "completed batch 2 of epoch 126. loss=0.017824865877628326. train batch time cost=0.10250115394592285s\n",
            "completed batch 3 of epoch 126. loss=0.011014732532203197. train batch time cost=0.10386228561401367s\n",
            "completed batch 4 of epoch 126. loss=0.006554842926561832. train batch time cost=0.10282635688781738s\n",
            "completed batch 5 of epoch 126. loss=0.025488462299108505. train batch time cost=0.10041666030883789s\n",
            "completed batch 6 of epoch 126. loss=0.00588733097538352. train batch time cost=0.10074400901794434s\n",
            "completed batch 7 of epoch 126. loss=0.012940977700054646. train batch time cost=0.10170555114746094s\n",
            "completed batch 8 of epoch 126. loss=0.030682450160384178. train batch time cost=0.10470080375671387s\n",
            "completed batch 9 of epoch 126. loss=0.005328302271664143. train batch time cost=0.1020514965057373s\n",
            "completed batch 10 of epoch 126. loss=0.004449793603271246. train batch time cost=0.1022794246673584s\n",
            "completed batch 11 of epoch 126. loss=0.0469445139169693. train batch time cost=0.10279130935668945s\n",
            "completed batch 12 of epoch 126. loss=0.12400219589471817. train batch time cost=0.1032254695892334s\n",
            "completed batch 13 of epoch 126. loss=0.042498424649238586. train batch time cost=0.10168838500976562s\n",
            "completed batch 14 of epoch 126. loss=0.017223842442035675. train batch time cost=0.10132098197937012s\n",
            "completed batch 15 of epoch 126. loss=0.00886631105095148. train batch time cost=0.10186147689819336s\n",
            "completed batch 16 of epoch 126. loss=0.009862959384918213. train batch time cost=0.1016702651977539s\n",
            "completed batch 17 of epoch 126. loss=0.01083670649677515. train batch time cost=0.10230803489685059s\n",
            "completed batch 18 of epoch 126. loss=0.008729659020900726. train batch time cost=0.10374784469604492s\n",
            "completed batch 19 of epoch 126. loss=0.015370693989098072. train batch time cost=0.10159945487976074s\n",
            "completed batch 20 of epoch 126. loss=0.008368073962628841. train batch time cost=0.10234427452087402s\n",
            "completed batch 21 of epoch 126. loss=0.024126529693603516. train batch time cost=0.1038508415222168s\n",
            "completed batch 22 of epoch 126. loss=0.003500551450997591. train batch time cost=0.10139250755310059s\n",
            "completed batch 23 of epoch 126. loss=0.03939467668533325. train batch time cost=0.10111331939697266s\n",
            "completed batch 24 of epoch 126. loss=0.0019406536594033241. train batch time cost=0.10173964500427246s\n",
            "completed batch 25 of epoch 126. loss=0.021233752369880676. train batch time cost=0.1040031909942627s\n",
            "completed batch 26 of epoch 126. loss=0.00489065470173955. train batch time cost=0.10202264785766602s\n",
            "completed batch 27 of epoch 126. loss=0.010092177428305149. train batch time cost=0.10111522674560547s\n",
            "completed batch 28 of epoch 126. loss=0.026134978979825974. train batch time cost=0.10125064849853516s\n",
            "completed batch 29 of epoch 126. loss=0.036106228828430176. train batch time cost=0.10249733924865723s\n",
            "completed batch 30 of epoch 126. loss=0.0019859299063682556. train batch time cost=0.10266423225402832s\n",
            "completed batch 31 of epoch 126. loss=0.01557355560362339. train batch time cost=0.1012716293334961s\n",
            "completed batch 32 of epoch 126. loss=0.023839661851525307. train batch time cost=0.10017013549804688s\n",
            "completed batch 33 of epoch 126. loss=0.024396691471338272. train batch time cost=0.10217714309692383s\n",
            "completed batch 34 of epoch 126. loss=0.02630670554935932. train batch time cost=0.1051490306854248s\n",
            "completed batch 35 of epoch 126. loss=0.041886962950229645. train batch time cost=0.10246491432189941s\n",
            "completed batch 36 of epoch 126. loss=0.009639497846364975. train batch time cost=0.11032843589782715s\n",
            "completed batch 37 of epoch 126. loss=0.026419447734951973. train batch time cost=0.10182833671569824s\n",
            "completed batch 38 of epoch 126. loss=0.002867612522095442. train batch time cost=0.10249066352844238s\n",
            "completed batch 39 of epoch 126. loss=0.033207543194293976. train batch time cost=0.10492515563964844s\n",
            "completed batch 40 of epoch 126. loss=0.003827311797067523. train batch time cost=0.10501241683959961s\n",
            "completed batch 41 of epoch 126. loss=0.01626170240342617. train batch time cost=0.10184597969055176s\n",
            "completed batch 42 of epoch 126. loss=0.009504401125013828. train batch time cost=0.10062813758850098s\n",
            "completed batch 43 of epoch 126. loss=0.016054637730121613. train batch time cost=0.10151338577270508s\n",
            "completed batch 44 of epoch 126. loss=0.004698331002146006. train batch time cost=0.1009969711303711s\n",
            "completed batch 45 of epoch 126. loss=0.016097284853458405. train batch time cost=0.10025405883789062s\n",
            "completed batch 46 of epoch 126. loss=0.006845920346677303. train batch time cost=0.10078120231628418s\n",
            "completed batch 47 of epoch 126. loss=0.005235985852777958. train batch time cost=0.10060858726501465s\n",
            "completed batch 48 of epoch 126. loss=0.0271441712975502. train batch time cost=0.10065746307373047s\n",
            "completed batch 49 of epoch 126. loss=0.018677780404686928. train batch time cost=0.10091638565063477s\n",
            "completed batch 50 of epoch 126. loss=0.05831963196396828. train batch time cost=0.10147285461425781s\n",
            "completed batch 51 of epoch 126. loss=0.0018140521133318543. train batch time cost=0.10031342506408691s\n",
            "completed batch 52 of epoch 126. loss=0.008235204964876175. train batch time cost=0.09676051139831543s\n",
            "completed batch 53 of epoch 126. loss=0.016103951260447502. train batch time cost=0.09596395492553711s\n",
            "completed batch 54 of epoch 126. loss=0.005857980344444513. train batch time cost=0.0946664810180664s\n",
            "completed batch 55 of epoch 126. loss=0.020075833424925804. train batch time cost=0.09463977813720703s\n",
            "completed batch 56 of epoch 126. loss=0.005479362793266773. train batch time cost=0.10149788856506348s\n",
            "completed batch 57 of epoch 126. loss=0.0438520610332489. train batch time cost=0.10233759880065918s\n",
            "completed batch 58 of epoch 126. loss=0.014372915029525757. train batch time cost=0.10195231437683105s\n",
            "completed batch 59 of epoch 126. loss=0.007901354692876339. train batch time cost=0.10203981399536133s\n",
            "completed batch 60 of epoch 126. loss=0.006739109754562378. train batch time cost=0.10238933563232422s\n",
            "completed batch 61 of epoch 126. loss=0.012253150343894958. train batch time cost=0.10369229316711426s\n",
            "completed batch 62 of epoch 126. loss=0.03217485174536705. train batch time cost=0.10224461555480957s\n",
            "completed batch 63 of epoch 126. loss=0.002366128610447049. train batch time cost=0.10250449180603027s\n",
            "completed batch 64 of epoch 126. loss=0.0035902007948607206. train batch time cost=0.10286116600036621s\n",
            "completed batch 65 of epoch 126. loss=0.0012544080382212996. train batch time cost=0.10187721252441406s\n",
            "completed batch 66 of epoch 126. loss=0.04512316733598709. train batch time cost=0.10120654106140137s\n",
            "completed batch 67 of epoch 126. loss=0.0045932079665362835. train batch time cost=0.10268044471740723s\n",
            "completed batch 68 of epoch 126. loss=0.009714660234749317. train batch time cost=0.10126852989196777s\n",
            "completed batch 69 of epoch 126. loss=0.012943198904395103. train batch time cost=0.1012873649597168s\n",
            "completed batch 70 of epoch 126. loss=0.008855810388922691. train batch time cost=0.1021883487701416s\n",
            "completed batch 71 of epoch 126. loss=0.039744917303323746. train batch time cost=0.10220599174499512s\n",
            "completed batch 72 of epoch 126. loss=0.01857573166489601. train batch time cost=0.1019432544708252s\n",
            "completed batch 73 of epoch 126. loss=0.004066183231770992. train batch time cost=0.10172343254089355s\n",
            "completed batch 74 of epoch 126. loss=0.0272977277636528. train batch time cost=0.1016387939453125s\n",
            "completed batch 75 of epoch 126. loss=0.027008529752492905. train batch time cost=0.10104918479919434s\n",
            "completed batch 76 of epoch 126. loss=0.009497275575995445. train batch time cost=0.1027841567993164s\n",
            "completed batch 77 of epoch 126. loss=0.018242893740534782. train batch time cost=0.1003103256225586s\n",
            "completed batch 78 of epoch 126. loss=0.008036713115870953. train batch time cost=0.10008931159973145s\n",
            "completed batch 79 of epoch 126. loss=0.0046409429050982. train batch time cost=0.10143733024597168s\n",
            "completed batch 80 of epoch 126. loss=0.027859950438141823. train batch time cost=0.10170173645019531s\n",
            "completed batch 81 of epoch 126. loss=0.0145390210673213. train batch time cost=0.10178685188293457s\n",
            "completed batch 82 of epoch 126. loss=0.004519685637205839. train batch time cost=0.0997762680053711s\n",
            "completed batch 83 of epoch 126. loss=0.08520980924367905. train batch time cost=0.10126924514770508s\n",
            "completed batch 84 of epoch 126. loss=0.01523996889591217. train batch time cost=0.10078811645507812s\n",
            "completed batch 85 of epoch 126. loss=0.00986454077064991. train batch time cost=0.10316014289855957s\n",
            "completed batch 86 of epoch 126. loss=0.08037834614515305. train batch time cost=0.10068249702453613s\n",
            "completed batch 87 of epoch 126. loss=0.028634008020162582. train batch time cost=0.10187625885009766s\n",
            "completed batch 88 of epoch 126. loss=0.006971475202590227. train batch time cost=0.101654052734375s\n",
            "completed batch 89 of epoch 126. loss=0.0697496235370636. train batch time cost=0.10331869125366211s\n",
            "completed batch 90 of epoch 126. loss=0.0033109551295638084. train batch time cost=0.10232830047607422s\n",
            "completed batch 91 of epoch 126. loss=0.007981432601809502. train batch time cost=0.10013985633850098s\n",
            "completed batch 92 of epoch 126. loss=0.013873393647372723. train batch time cost=0.10068821907043457s\n",
            "completed batch 93 of epoch 126. loss=0.019994433969259262. train batch time cost=0.10011124610900879s\n",
            "completed batch 94 of epoch 126. loss=0.0076962606981396675. train batch time cost=0.10075116157531738s\n",
            "completed batch 95 of epoch 126. loss=0.009718201123178005. train batch time cost=0.10199260711669922s\n",
            "completed batch 96 of epoch 126. loss=0.010129395872354507. train batch time cost=0.10191082954406738s\n",
            "completed batch 97 of epoch 126. loss=0.09086272120475769. train batch time cost=0.10318803787231445s\n",
            "completed batch 98 of epoch 126. loss=0.007674984633922577. train batch time cost=0.10360288619995117s\n",
            "completed batch 99 of epoch 126. loss=0.015394311398267746. train batch time cost=0.1047062873840332s\n",
            "completed batch 100 of epoch 126. loss=0.011464311741292477. train batch time cost=0.10204482078552246s\n",
            "completed batch 101 of epoch 126. loss=0.05857224762439728. train batch time cost=0.10693478584289551s\n",
            "completed batch 102 of epoch 126. loss=0.05817972496151924. train batch time cost=0.10322833061218262s\n",
            "completed batch 103 of epoch 126. loss=0.0315050408244133. train batch time cost=0.10154581069946289s\n",
            "completed batch 104 of epoch 126. loss=0.001901901443488896. train batch time cost=0.10112380981445312s\n",
            "completed batch 105 of epoch 126. loss=0.014294931665062904. train batch time cost=0.10021138191223145s\n",
            "completed batch 106 of epoch 126. loss=0.013696648180484772. train batch time cost=0.10043501853942871s\n",
            "completed batch 107 of epoch 126. loss=0.005383879877626896. train batch time cost=0.10214900970458984s\n",
            "completed batch 108 of epoch 126. loss=0.00445714732632041. train batch time cost=0.10130071640014648s\n",
            "completed batch 109 of epoch 126. loss=0.029768215492367744. train batch time cost=0.10057282447814941s\n",
            "completed batch 110 of epoch 126. loss=0.09941134601831436. train batch time cost=0.10131335258483887s\n",
            "completed batch 111 of epoch 126. loss=0.028561098501086235. train batch time cost=0.10228824615478516s\n",
            "completed batch 112 of epoch 126. loss=0.11146330833435059. train batch time cost=0.10230898857116699s\n",
            "completed batch 113 of epoch 126. loss=0.035924553871154785. train batch time cost=0.10100674629211426s\n",
            "completed batch 114 of epoch 126. loss=0.0074930512346327305. train batch time cost=0.10034036636352539s\n",
            "completed batch 115 of epoch 126. loss=0.004376348108053207. train batch time cost=0.10095095634460449s\n",
            "completed batch 116 of epoch 126. loss=0.0633576437830925. train batch time cost=0.10163474082946777s\n",
            "completed batch 117 of epoch 126. loss=0.003962263930588961. train batch time cost=0.1020040512084961s\n",
            "completed batch 118 of epoch 126. loss=0.00816397462040186. train batch time cost=0.10075235366821289s\n",
            "completed batch 119 of epoch 126. loss=0.10093098133802414. train batch time cost=0.10120034217834473s\n",
            "completed batch 120 of epoch 126. loss=0.011850674636662006. train batch time cost=0.10237836837768555s\n",
            "completed batch 121 of epoch 126. loss=0.10376881808042526. train batch time cost=0.10066580772399902s\n",
            "completed batch 122 of epoch 126. loss=0.009162814356386662. train batch time cost=0.10036730766296387s\n",
            "completed batch 123 of epoch 126. loss=0.0496363639831543. train batch time cost=0.1000814437866211s\n",
            "completed batch 124 of epoch 126. loss=0.01910681091248989. train batch time cost=0.10012483596801758s\n",
            "completed batch 125 of epoch 126. loss=0.10040757060050964. train batch time cost=0.10163354873657227s\n",
            "completed batch 126 of epoch 126. loss=0.006754807662218809. train batch time cost=0.031041383743286133s\n",
            "completed test of epoch 126. loss=0.006754807662218809. accuracy=0.654018971542686. train one epoch time cost=27.65737771987915s, test validation time cost=3.8531529903411865\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686]\n",
            "completed batch 1 of epoch 127. loss=0.0642879530787468. train batch time cost=0.0967257022857666s\n",
            "completed batch 2 of epoch 127. loss=0.25322145223617554. train batch time cost=0.09509110450744629s\n",
            "completed batch 3 of epoch 127. loss=0.007121465168893337. train batch time cost=0.09879660606384277s\n",
            "completed batch 4 of epoch 127. loss=0.011600586585700512. train batch time cost=0.09877157211303711s\n",
            "completed batch 5 of epoch 127. loss=0.046222537755966187. train batch time cost=0.10424447059631348s\n",
            "completed batch 6 of epoch 127. loss=0.02042049542069435. train batch time cost=0.09928297996520996s\n",
            "completed batch 7 of epoch 127. loss=0.07479561120271683. train batch time cost=0.10076642036437988s\n",
            "completed batch 8 of epoch 127. loss=0.18787947297096252. train batch time cost=0.09882283210754395s\n",
            "completed batch 9 of epoch 127. loss=0.021501995623111725. train batch time cost=0.10071778297424316s\n",
            "completed batch 10 of epoch 127. loss=0.014497875235974789. train batch time cost=0.10344600677490234s\n",
            "completed batch 11 of epoch 127. loss=0.020659388974308968. train batch time cost=0.10247540473937988s\n",
            "completed batch 12 of epoch 127. loss=0.06400953978300095. train batch time cost=0.10226917266845703s\n",
            "completed batch 13 of epoch 127. loss=0.189077690243721. train batch time cost=0.10208988189697266s\n",
            "completed batch 14 of epoch 127. loss=0.04032260924577713. train batch time cost=0.10309600830078125s\n",
            "completed batch 15 of epoch 127. loss=0.02421604096889496. train batch time cost=0.10172271728515625s\n",
            "completed batch 16 of epoch 127. loss=0.006259472109377384. train batch time cost=0.10102152824401855s\n",
            "completed batch 17 of epoch 127. loss=0.05104239657521248. train batch time cost=0.10037565231323242s\n",
            "completed batch 18 of epoch 127. loss=0.03154607489705086. train batch time cost=0.10149693489074707s\n",
            "completed batch 19 of epoch 127. loss=0.07555939257144928. train batch time cost=0.10317015647888184s\n",
            "completed batch 20 of epoch 127. loss=0.029246162623167038. train batch time cost=0.10201120376586914s\n",
            "completed batch 21 of epoch 127. loss=0.051801372319459915. train batch time cost=0.10147500038146973s\n",
            "completed batch 22 of epoch 127. loss=0.04045438766479492. train batch time cost=0.10070228576660156s\n",
            "completed batch 23 of epoch 127. loss=0.11275432258844376. train batch time cost=0.10081243515014648s\n",
            "completed batch 24 of epoch 127. loss=0.016543062403798103. train batch time cost=0.10224413871765137s\n",
            "completed batch 25 of epoch 127. loss=0.033430371433496475. train batch time cost=0.10172605514526367s\n",
            "completed batch 26 of epoch 127. loss=0.007242871914058924. train batch time cost=0.10178112983703613s\n",
            "completed batch 27 of epoch 127. loss=0.018666833639144897. train batch time cost=0.10137009620666504s\n",
            "completed batch 28 of epoch 127. loss=0.15183036029338837. train batch time cost=0.10251498222351074s\n",
            "completed batch 29 of epoch 127. loss=0.050063617527484894. train batch time cost=0.10258150100708008s\n",
            "completed batch 30 of epoch 127. loss=0.003922688774764538. train batch time cost=0.10103249549865723s\n",
            "completed batch 31 of epoch 127. loss=0.06012090668082237. train batch time cost=0.10102367401123047s\n",
            "completed batch 32 of epoch 127. loss=0.011934526264667511. train batch time cost=0.10209989547729492s\n",
            "completed batch 33 of epoch 127. loss=0.039597004652023315. train batch time cost=0.10254216194152832s\n",
            "completed batch 34 of epoch 127. loss=0.10284100472927094. train batch time cost=0.10225129127502441s\n",
            "completed batch 35 of epoch 127. loss=0.0557754747569561. train batch time cost=0.1010587215423584s\n",
            "completed batch 36 of epoch 127. loss=0.1029742956161499. train batch time cost=0.1013498306274414s\n",
            "completed batch 37 of epoch 127. loss=0.08515536040067673. train batch time cost=0.10219073295593262s\n",
            "completed batch 38 of epoch 127. loss=0.023908384144306183. train batch time cost=0.10233426094055176s\n",
            "completed batch 39 of epoch 127. loss=0.1889953315258026. train batch time cost=0.10168004035949707s\n",
            "completed batch 40 of epoch 127. loss=0.08041083812713623. train batch time cost=0.10147953033447266s\n",
            "completed batch 41 of epoch 127. loss=0.023984849452972412. train batch time cost=0.10177302360534668s\n",
            "completed batch 42 of epoch 127. loss=0.07756365090608597. train batch time cost=0.10129737854003906s\n",
            "completed batch 43 of epoch 127. loss=0.046040359884500504. train batch time cost=0.10112929344177246s\n",
            "completed batch 44 of epoch 127. loss=0.04277704283595085. train batch time cost=0.10073733329772949s\n",
            "completed batch 45 of epoch 127. loss=0.006390755530446768. train batch time cost=0.10042691230773926s\n",
            "completed batch 46 of epoch 127. loss=0.010994059965014458. train batch time cost=0.1031043529510498s\n",
            "completed batch 47 of epoch 127. loss=0.07288897782564163. train batch time cost=0.10201334953308105s\n",
            "completed batch 48 of epoch 127. loss=0.03227847069501877. train batch time cost=0.1024484634399414s\n",
            "completed batch 49 of epoch 127. loss=0.02528412640094757. train batch time cost=0.1011040210723877s\n",
            "completed batch 50 of epoch 127. loss=0.0750306248664856. train batch time cost=0.10448837280273438s\n",
            "completed batch 51 of epoch 127. loss=0.03151900693774223. train batch time cost=0.10254192352294922s\n",
            "completed batch 52 of epoch 127. loss=0.16117097437381744. train batch time cost=0.10231757164001465s\n",
            "completed batch 53 of epoch 127. loss=0.12491396069526672. train batch time cost=0.10126876831054688s\n",
            "completed batch 54 of epoch 127. loss=0.04422684386372566. train batch time cost=0.10085582733154297s\n",
            "completed batch 55 of epoch 127. loss=0.005091466940939426. train batch time cost=0.10204553604125977s\n",
            "completed batch 56 of epoch 127. loss=0.022340087220072746. train batch time cost=0.10246515274047852s\n",
            "completed batch 57 of epoch 127. loss=0.0691668838262558. train batch time cost=0.10336589813232422s\n",
            "completed batch 58 of epoch 127. loss=0.02176721580326557. train batch time cost=0.1008296012878418s\n",
            "completed batch 59 of epoch 127. loss=0.017453471198678017. train batch time cost=0.10390925407409668s\n",
            "completed batch 60 of epoch 127. loss=0.08142338693141937. train batch time cost=0.10667848587036133s\n",
            "completed batch 61 of epoch 127. loss=0.00678791431710124. train batch time cost=0.10160589218139648s\n",
            "completed batch 62 of epoch 127. loss=0.009242353029549122. train batch time cost=0.1020803451538086s\n",
            "completed batch 63 of epoch 127. loss=0.02482634410262108. train batch time cost=0.10265326499938965s\n",
            "completed batch 64 of epoch 127. loss=0.00985394511371851. train batch time cost=0.10006213188171387s\n",
            "completed batch 65 of epoch 127. loss=0.017222989350557327. train batch time cost=0.10207605361938477s\n",
            "completed batch 66 of epoch 127. loss=0.02763894386589527. train batch time cost=0.10177731513977051s\n",
            "completed batch 67 of epoch 127. loss=0.07510554045438766. train batch time cost=0.10168099403381348s\n",
            "completed batch 68 of epoch 127. loss=0.04521353542804718. train batch time cost=0.10230803489685059s\n",
            "completed batch 69 of epoch 127. loss=0.005520163103938103. train batch time cost=0.10413837432861328s\n",
            "completed batch 70 of epoch 127. loss=0.007218498270958662. train batch time cost=0.10193872451782227s\n",
            "completed batch 71 of epoch 127. loss=0.1041320338845253. train batch time cost=0.10152816772460938s\n",
            "completed batch 72 of epoch 127. loss=0.06808922439813614. train batch time cost=0.10086178779602051s\n",
            "completed batch 73 of epoch 127. loss=0.016344085335731506. train batch time cost=0.10018110275268555s\n",
            "completed batch 74 of epoch 127. loss=0.02479441463947296. train batch time cost=0.10088706016540527s\n",
            "completed batch 75 of epoch 127. loss=0.045955196022987366. train batch time cost=0.10053157806396484s\n",
            "completed batch 76 of epoch 127. loss=0.3353022336959839. train batch time cost=0.1016232967376709s\n",
            "completed batch 77 of epoch 127. loss=0.02449789084494114. train batch time cost=0.10066938400268555s\n",
            "completed batch 78 of epoch 127. loss=0.018559575080871582. train batch time cost=0.10291099548339844s\n",
            "completed batch 79 of epoch 127. loss=0.009008362889289856. train batch time cost=0.10288476943969727s\n",
            "completed batch 80 of epoch 127. loss=0.03813198581337929. train batch time cost=0.10174083709716797s\n",
            "completed batch 81 of epoch 127. loss=0.02050768956542015. train batch time cost=0.10139679908752441s\n",
            "completed batch 82 of epoch 127. loss=0.03354267403483391. train batch time cost=0.10093927383422852s\n",
            "completed batch 83 of epoch 127. loss=0.10998959839344025. train batch time cost=0.10221028327941895s\n",
            "completed batch 84 of epoch 127. loss=0.020798873156309128. train batch time cost=0.10218524932861328s\n",
            "completed batch 85 of epoch 127. loss=0.03169422596693039. train batch time cost=0.10200715065002441s\n",
            "completed batch 86 of epoch 127. loss=0.018809724599123. train batch time cost=0.10026192665100098s\n",
            "completed batch 87 of epoch 127. loss=0.023677591234445572. train batch time cost=0.09561991691589355s\n",
            "completed batch 88 of epoch 127. loss=0.0629919096827507. train batch time cost=0.09508347511291504s\n",
            "completed batch 89 of epoch 127. loss=0.015391029417514801. train batch time cost=0.09523415565490723s\n",
            "completed batch 90 of epoch 127. loss=0.031081320717930794. train batch time cost=0.09455275535583496s\n",
            "completed batch 91 of epoch 127. loss=0.09687763452529907. train batch time cost=0.0976262092590332s\n",
            "completed batch 92 of epoch 127. loss=0.009608726017177105. train batch time cost=0.09606671333312988s\n",
            "completed batch 93 of epoch 127. loss=0.017801031470298767. train batch time cost=0.0967867374420166s\n",
            "completed batch 94 of epoch 127. loss=0.014626897871494293. train batch time cost=0.09522414207458496s\n",
            "completed batch 95 of epoch 127. loss=0.03282415494322777. train batch time cost=0.0951375961303711s\n",
            "completed batch 96 of epoch 127. loss=0.014315400272607803. train batch time cost=0.09634065628051758s\n",
            "completed batch 97 of epoch 127. loss=0.01028309017419815. train batch time cost=0.0958397388458252s\n",
            "completed batch 98 of epoch 127. loss=0.021873148158192635. train batch time cost=0.10150146484375s\n",
            "completed batch 99 of epoch 127. loss=0.028366368263959885. train batch time cost=0.10119438171386719s\n",
            "completed batch 100 of epoch 127. loss=0.03105609491467476. train batch time cost=0.10218238830566406s\n",
            "completed batch 101 of epoch 127. loss=0.020377079024910927. train batch time cost=0.10181283950805664s\n",
            "completed batch 102 of epoch 127. loss=0.045811109244823456. train batch time cost=0.10169339179992676s\n",
            "completed batch 103 of epoch 127. loss=0.012650025077164173. train batch time cost=0.10071706771850586s\n",
            "completed batch 104 of epoch 127. loss=0.0034042191691696644. train batch time cost=0.10058093070983887s\n",
            "completed batch 105 of epoch 127. loss=0.018351534381508827. train batch time cost=0.10130953788757324s\n",
            "completed batch 106 of epoch 127. loss=0.011875731870532036. train batch time cost=0.10215616226196289s\n",
            "completed batch 107 of epoch 127. loss=0.008121619932353497. train batch time cost=0.10112953186035156s\n",
            "completed batch 108 of epoch 127. loss=0.03392665088176727. train batch time cost=0.10159587860107422s\n",
            "completed batch 109 of epoch 127. loss=0.05399005487561226. train batch time cost=0.10134625434875488s\n",
            "completed batch 110 of epoch 127. loss=0.0402609258890152. train batch time cost=0.10207247734069824s\n",
            "completed batch 111 of epoch 127. loss=0.07482576370239258. train batch time cost=0.10158753395080566s\n",
            "completed batch 112 of epoch 127. loss=0.054055068641901016. train batch time cost=0.10167145729064941s\n",
            "completed batch 113 of epoch 127. loss=0.010421058163046837. train batch time cost=0.1011192798614502s\n",
            "completed batch 114 of epoch 127. loss=0.011399212293326855. train batch time cost=0.10108447074890137s\n",
            "completed batch 115 of epoch 127. loss=0.08162309974431992. train batch time cost=0.10194015502929688s\n",
            "completed batch 116 of epoch 127. loss=0.08075511455535889. train batch time cost=0.10025882720947266s\n",
            "completed batch 117 of epoch 127. loss=0.036613695323467255. train batch time cost=0.10089421272277832s\n",
            "completed batch 118 of epoch 127. loss=0.014744940213859081. train batch time cost=0.10157990455627441s\n",
            "completed batch 119 of epoch 127. loss=0.005767439492046833. train batch time cost=0.10231208801269531s\n",
            "completed batch 120 of epoch 127. loss=0.00692407600581646. train batch time cost=0.10156846046447754s\n",
            "completed batch 121 of epoch 127. loss=0.06590849161148071. train batch time cost=0.1014869213104248s\n",
            "completed batch 122 of epoch 127. loss=0.05472520738840103. train batch time cost=0.10078811645507812s\n",
            "completed batch 123 of epoch 127. loss=0.011107053607702255. train batch time cost=0.10310053825378418s\n",
            "completed batch 124 of epoch 127. loss=0.04318155348300934. train batch time cost=0.10193705558776855s\n",
            "completed batch 125 of epoch 127. loss=0.023891430348157883. train batch time cost=0.10173678398132324s\n",
            "completed batch 126 of epoch 127. loss=5.480962499859743e-05. train batch time cost=0.030391931533813477s\n",
            "completed test of epoch 127. loss=5.480962499859743e-05. accuracy=0.6944583125312032. train one epoch time cost=27.57438373565674s, test validation time cost=3.9663453102111816\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032]\n",
            "completed batch 1 of epoch 128. loss=0.022299962118268013. train batch time cost=0.09507060050964355s\n",
            "completed batch 2 of epoch 128. loss=0.14907409250736237. train batch time cost=0.0950937271118164s\n",
            "completed batch 3 of epoch 128. loss=0.015459506772458553. train batch time cost=0.09685897827148438s\n",
            "completed batch 4 of epoch 128. loss=0.04266434162855148. train batch time cost=0.09560990333557129s\n",
            "completed batch 5 of epoch 128. loss=0.01571863703429699. train batch time cost=0.09551000595092773s\n",
            "completed batch 6 of epoch 128. loss=0.07345172762870789. train batch time cost=0.09499502182006836s\n",
            "completed batch 7 of epoch 128. loss=0.030465641990303993. train batch time cost=0.1031179428100586s\n",
            "completed batch 8 of epoch 128. loss=0.016525395214557648. train batch time cost=0.1035921573638916s\n",
            "completed batch 9 of epoch 128. loss=0.024521825835108757. train batch time cost=0.09634780883789062s\n",
            "completed batch 10 of epoch 128. loss=0.05644288286566734. train batch time cost=0.09558820724487305s\n",
            "completed batch 11 of epoch 128. loss=0.025039300322532654. train batch time cost=0.09475398063659668s\n",
            "completed batch 12 of epoch 128. loss=0.00876222550868988. train batch time cost=0.09539437294006348s\n",
            "completed batch 13 of epoch 128. loss=0.01573103666305542. train batch time cost=0.09489083290100098s\n",
            "completed batch 14 of epoch 128. loss=0.018557285889983177. train batch time cost=0.10183858871459961s\n",
            "completed batch 15 of epoch 128. loss=0.008083387278020382. train batch time cost=0.10083198547363281s\n",
            "completed batch 16 of epoch 128. loss=0.017688097432255745. train batch time cost=0.09541511535644531s\n",
            "completed batch 17 of epoch 128. loss=0.04238847270607948. train batch time cost=0.09567093849182129s\n",
            "completed batch 18 of epoch 128. loss=0.16327700018882751. train batch time cost=0.09545612335205078s\n",
            "completed batch 19 of epoch 128. loss=0.003512696363031864. train batch time cost=0.09529519081115723s\n",
            "completed batch 20 of epoch 128. loss=0.03524142503738403. train batch time cost=0.0968778133392334s\n",
            "completed batch 21 of epoch 128. loss=0.020971642807126045. train batch time cost=0.09548211097717285s\n",
            "completed batch 22 of epoch 128. loss=0.009799218736588955. train batch time cost=0.09519481658935547s\n",
            "completed batch 23 of epoch 128. loss=0.01056353934109211. train batch time cost=0.09518218040466309s\n",
            "completed batch 24 of epoch 128. loss=0.007545877248048782. train batch time cost=0.09587979316711426s\n",
            "completed batch 25 of epoch 128. loss=0.0505048967897892. train batch time cost=0.09458589553833008s\n",
            "completed batch 26 of epoch 128. loss=0.0139352111145854. train batch time cost=0.10131573677062988s\n",
            "completed batch 27 of epoch 128. loss=0.014303846284747124. train batch time cost=0.1018517017364502s\n",
            "completed batch 28 of epoch 128. loss=0.013341907411813736. train batch time cost=0.10223245620727539s\n",
            "completed batch 29 of epoch 128. loss=0.009544278495013714. train batch time cost=0.10430121421813965s\n",
            "completed batch 30 of epoch 128. loss=0.0025241337716579437. train batch time cost=0.10161828994750977s\n",
            "completed batch 31 of epoch 128. loss=0.007133414037525654. train batch time cost=0.10364174842834473s\n",
            "completed batch 32 of epoch 128. loss=0.14918334782123566. train batch time cost=0.10269951820373535s\n",
            "completed batch 33 of epoch 128. loss=0.02235279604792595. train batch time cost=0.1018521785736084s\n",
            "completed batch 34 of epoch 128. loss=0.17255067825317383. train batch time cost=0.10317158699035645s\n",
            "completed batch 35 of epoch 128. loss=0.10352429002523422. train batch time cost=0.10212063789367676s\n",
            "completed batch 36 of epoch 128. loss=0.06719552725553513. train batch time cost=0.10128569602966309s\n",
            "completed batch 37 of epoch 128. loss=0.06467965990304947. train batch time cost=0.10272550582885742s\n",
            "completed batch 38 of epoch 128. loss=0.014769899658858776. train batch time cost=0.09994816780090332s\n",
            "completed batch 39 of epoch 128. loss=0.026115063577890396. train batch time cost=0.10194587707519531s\n",
            "completed batch 40 of epoch 128. loss=0.01530421432107687. train batch time cost=0.10058212280273438s\n",
            "completed batch 41 of epoch 128. loss=0.030878407880663872. train batch time cost=0.10169172286987305s\n",
            "completed batch 42 of epoch 128. loss=0.011145077645778656. train batch time cost=0.10036563873291016s\n",
            "completed batch 43 of epoch 128. loss=0.021908342838287354. train batch time cost=0.10178804397583008s\n",
            "completed batch 44 of epoch 128. loss=0.05704553425312042. train batch time cost=0.1020815372467041s\n",
            "completed batch 45 of epoch 128. loss=0.038363710045814514. train batch time cost=0.10151815414428711s\n",
            "completed batch 46 of epoch 128. loss=0.005732034798711538. train batch time cost=0.10289621353149414s\n",
            "completed batch 47 of epoch 128. loss=0.026056213304400444. train batch time cost=0.10125207901000977s\n",
            "completed batch 48 of epoch 128. loss=0.017258966341614723. train batch time cost=0.1012876033782959s\n",
            "completed batch 49 of epoch 128. loss=0.04092975705862045. train batch time cost=0.10204339027404785s\n",
            "completed batch 50 of epoch 128. loss=0.14067158102989197. train batch time cost=0.10328364372253418s\n",
            "completed batch 51 of epoch 128. loss=0.02673562616109848. train batch time cost=0.10328269004821777s\n",
            "completed batch 52 of epoch 128. loss=0.17958620190620422. train batch time cost=0.10140800476074219s\n",
            "completed batch 53 of epoch 128. loss=0.05647439509630203. train batch time cost=0.10288405418395996s\n",
            "completed batch 54 of epoch 128. loss=0.11572723090648651. train batch time cost=0.10164546966552734s\n",
            "completed batch 55 of epoch 128. loss=0.052326202392578125. train batch time cost=0.1022942066192627s\n",
            "completed batch 56 of epoch 128. loss=0.08738399296998978. train batch time cost=0.1030876636505127s\n",
            "completed batch 57 of epoch 128. loss=0.06825398653745651. train batch time cost=0.10186624526977539s\n",
            "completed batch 58 of epoch 128. loss=0.0870470255613327. train batch time cost=0.10306930541992188s\n",
            "completed batch 59 of epoch 128. loss=0.0136739332228899. train batch time cost=0.1027371883392334s\n",
            "completed batch 60 of epoch 128. loss=0.03032596968114376. train batch time cost=0.10255932807922363s\n",
            "completed batch 61 of epoch 128. loss=0.0206444188952446. train batch time cost=0.10193800926208496s\n",
            "completed batch 62 of epoch 128. loss=0.11091240495443344. train batch time cost=0.10496377944946289s\n",
            "completed batch 63 of epoch 128. loss=0.02546468935906887. train batch time cost=0.10181784629821777s\n",
            "completed batch 64 of epoch 128. loss=0.01760144717991352. train batch time cost=0.10206389427185059s\n",
            "completed batch 65 of epoch 128. loss=0.03254985436797142. train batch time cost=0.10251855850219727s\n",
            "completed batch 66 of epoch 128. loss=0.09262914210557938. train batch time cost=0.10380244255065918s\n",
            "completed batch 67 of epoch 128. loss=0.08233422040939331. train batch time cost=0.10225272178649902s\n",
            "completed batch 68 of epoch 128. loss=0.008704347535967827. train batch time cost=0.10224437713623047s\n",
            "completed batch 69 of epoch 128. loss=0.018099013715982437. train batch time cost=0.10258746147155762s\n",
            "completed batch 70 of epoch 128. loss=0.056415341794490814. train batch time cost=0.10226917266845703s\n",
            "completed batch 71 of epoch 128. loss=0.1557670682668686. train batch time cost=0.10075235366821289s\n",
            "completed batch 72 of epoch 128. loss=0.04640284925699234. train batch time cost=0.10264873504638672s\n",
            "completed batch 73 of epoch 128. loss=0.015788521617650986. train batch time cost=0.10248208045959473s\n",
            "completed batch 74 of epoch 128. loss=0.03636278584599495. train batch time cost=0.10164093971252441s\n",
            "completed batch 75 of epoch 128. loss=0.06602267920970917. train batch time cost=0.10161018371582031s\n",
            "completed batch 76 of epoch 128. loss=0.0825849249958992. train batch time cost=0.10129904747009277s\n",
            "completed batch 77 of epoch 128. loss=0.07218091189861298. train batch time cost=0.10096287727355957s\n",
            "completed batch 78 of epoch 128. loss=0.002959907753393054. train batch time cost=0.10131454467773438s\n",
            "completed batch 79 of epoch 128. loss=0.055616140365600586. train batch time cost=0.10200977325439453s\n",
            "completed batch 80 of epoch 128. loss=0.08361729234457016. train batch time cost=0.10130524635314941s\n",
            "completed batch 81 of epoch 128. loss=0.14417846500873566. train batch time cost=0.10081338882446289s\n",
            "completed batch 82 of epoch 128. loss=0.0400259755551815. train batch time cost=0.10152196884155273s\n",
            "completed batch 83 of epoch 128. loss=0.05744548887014389. train batch time cost=0.10129952430725098s\n",
            "completed batch 84 of epoch 128. loss=0.13787712156772614. train batch time cost=0.10112643241882324s\n",
            "completed batch 85 of epoch 128. loss=0.06223757565021515. train batch time cost=0.10370659828186035s\n",
            "completed batch 86 of epoch 128. loss=0.029797714203596115. train batch time cost=0.09990167617797852s\n",
            "completed batch 87 of epoch 128. loss=0.06553681939840317. train batch time cost=0.10209465026855469s\n",
            "completed batch 88 of epoch 128. loss=0.06334633380174637. train batch time cost=0.10173630714416504s\n",
            "completed batch 89 of epoch 128. loss=0.1600888967514038. train batch time cost=0.10362529754638672s\n",
            "completed batch 90 of epoch 128. loss=0.08822456747293472. train batch time cost=0.10155653953552246s\n",
            "completed batch 91 of epoch 128. loss=0.04863054305315018. train batch time cost=0.10252499580383301s\n",
            "completed batch 92 of epoch 128. loss=0.10464032739400864. train batch time cost=0.10350251197814941s\n",
            "completed batch 93 of epoch 128. loss=0.11921369284391403. train batch time cost=0.1025094985961914s\n",
            "completed batch 94 of epoch 128. loss=0.053860854357481. train batch time cost=0.10165643692016602s\n",
            "completed batch 95 of epoch 128. loss=0.09455329179763794. train batch time cost=0.1030571460723877s\n",
            "completed batch 96 of epoch 128. loss=0.11974704265594482. train batch time cost=0.10287690162658691s\n",
            "completed batch 97 of epoch 128. loss=0.0958857387304306. train batch time cost=0.10258126258850098s\n",
            "completed batch 98 of epoch 128. loss=0.09046587347984314. train batch time cost=0.10403108596801758s\n",
            "completed batch 99 of epoch 128. loss=0.044071875512599945. train batch time cost=0.10184383392333984s\n",
            "completed batch 100 of epoch 128. loss=0.02132180519402027. train batch time cost=0.10202980041503906s\n",
            "completed batch 101 of epoch 128. loss=0.013655110262334347. train batch time cost=0.10126280784606934s\n",
            "completed batch 102 of epoch 128. loss=0.10647089779376984. train batch time cost=0.10185718536376953s\n",
            "completed batch 103 of epoch 128. loss=0.07824385166168213. train batch time cost=0.10242748260498047s\n",
            "completed batch 104 of epoch 128. loss=0.014666921459138393. train batch time cost=0.10170269012451172s\n",
            "completed batch 105 of epoch 128. loss=0.1193612590432167. train batch time cost=0.10204744338989258s\n",
            "completed batch 106 of epoch 128. loss=0.06258600950241089. train batch time cost=0.10201740264892578s\n",
            "completed batch 107 of epoch 128. loss=0.07787759602069855. train batch time cost=0.10193395614624023s\n",
            "completed batch 108 of epoch 128. loss=0.0901809111237526. train batch time cost=0.10196495056152344s\n",
            "completed batch 109 of epoch 128. loss=0.04081563279032707. train batch time cost=0.1007695198059082s\n",
            "completed batch 110 of epoch 128. loss=0.09179645776748657. train batch time cost=0.1012563705444336s\n",
            "completed batch 111 of epoch 128. loss=0.04621279984712601. train batch time cost=0.10160470008850098s\n",
            "completed batch 112 of epoch 128. loss=0.08189603686332703. train batch time cost=0.10139942169189453s\n",
            "completed batch 113 of epoch 128. loss=0.04494749382138252. train batch time cost=0.10244917869567871s\n",
            "completed batch 114 of epoch 128. loss=0.018188968300819397. train batch time cost=0.10078644752502441s\n",
            "completed batch 115 of epoch 128. loss=0.11703522503376007. train batch time cost=0.10117650032043457s\n",
            "completed batch 116 of epoch 128. loss=0.20315317809581757. train batch time cost=0.1023252010345459s\n",
            "completed batch 117 of epoch 128. loss=0.1721920520067215. train batch time cost=0.10108542442321777s\n",
            "completed batch 118 of epoch 128. loss=0.03922968730330467. train batch time cost=0.10229110717773438s\n",
            "completed batch 119 of epoch 128. loss=0.10737725347280502. train batch time cost=0.10183143615722656s\n",
            "completed batch 120 of epoch 128. loss=0.1957891285419464. train batch time cost=0.10177898406982422s\n",
            "completed batch 121 of epoch 128. loss=0.061235807836055756. train batch time cost=0.10177063941955566s\n",
            "completed batch 122 of epoch 128. loss=0.07364100962877274. train batch time cost=0.1024787425994873s\n",
            "completed batch 123 of epoch 128. loss=0.05164269357919693. train batch time cost=0.10118746757507324s\n",
            "completed batch 124 of epoch 128. loss=0.16787602007389069. train batch time cost=0.10189414024353027s\n",
            "completed batch 125 of epoch 128. loss=0.12055616825819016. train batch time cost=0.10091209411621094s\n",
            "completed batch 126 of epoch 128. loss=0.01080605760216713. train batch time cost=0.030152082443237305s\n",
            "completed test of epoch 128. loss=0.01080605760216713. accuracy=0.5876185721417874. train one epoch time cost=27.558046579360962s, test validation time cost=3.970863103866577\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874]\n",
            "completed batch 1 of epoch 129. loss=0.02573135681450367. train batch time cost=0.10047483444213867s\n",
            "completed batch 2 of epoch 129. loss=0.09076466411352158. train batch time cost=0.1012883186340332s\n",
            "completed batch 3 of epoch 129. loss=0.050527118146419525. train batch time cost=0.1023561954498291s\n",
            "completed batch 4 of epoch 129. loss=0.05982805788516998. train batch time cost=0.1044778823852539s\n",
            "completed batch 5 of epoch 129. loss=0.40178626775741577. train batch time cost=0.10340595245361328s\n",
            "completed batch 6 of epoch 129. loss=0.06818962097167969. train batch time cost=0.10467004776000977s\n",
            "completed batch 7 of epoch 129. loss=0.09099829196929932. train batch time cost=0.10236763954162598s\n",
            "completed batch 8 of epoch 129. loss=0.025925446301698685. train batch time cost=0.10236215591430664s\n",
            "completed batch 9 of epoch 129. loss=0.12777656316757202. train batch time cost=0.10214638710021973s\n",
            "completed batch 10 of epoch 129. loss=0.08855097740888596. train batch time cost=0.10188412666320801s\n",
            "completed batch 11 of epoch 129. loss=0.0434240847826004. train batch time cost=0.09995412826538086s\n",
            "completed batch 12 of epoch 129. loss=0.11604400724172592. train batch time cost=0.10014152526855469s\n",
            "completed batch 13 of epoch 129. loss=0.22062087059020996. train batch time cost=0.10244321823120117s\n",
            "completed batch 14 of epoch 129. loss=0.1395283192396164. train batch time cost=0.10165643692016602s\n",
            "completed batch 15 of epoch 129. loss=0.5537900924682617. train batch time cost=0.10140657424926758s\n",
            "completed batch 16 of epoch 129. loss=0.01760186441242695. train batch time cost=0.10163307189941406s\n",
            "completed batch 17 of epoch 129. loss=0.01907893642783165. train batch time cost=0.10289669036865234s\n",
            "completed batch 18 of epoch 129. loss=0.09498032927513123. train batch time cost=0.10336709022521973s\n",
            "completed batch 19 of epoch 129. loss=0.16393855214118958. train batch time cost=0.10389542579650879s\n",
            "completed batch 20 of epoch 129. loss=0.03398944437503815. train batch time cost=0.10224103927612305s\n",
            "completed batch 21 of epoch 129. loss=0.05723812058568001. train batch time cost=0.10247278213500977s\n",
            "completed batch 22 of epoch 129. loss=0.23782014846801758. train batch time cost=0.10405492782592773s\n",
            "completed batch 23 of epoch 129. loss=0.24718378484249115. train batch time cost=0.1027677059173584s\n",
            "completed batch 24 of epoch 129. loss=0.03917248174548149. train batch time cost=0.10138487815856934s\n",
            "completed batch 25 of epoch 129. loss=0.21577805280685425. train batch time cost=0.1007237434387207s\n",
            "completed batch 26 of epoch 129. loss=0.19655707478523254. train batch time cost=0.10169863700866699s\n",
            "completed batch 27 of epoch 129. loss=0.11522071808576584. train batch time cost=0.10342836380004883s\n",
            "completed batch 28 of epoch 129. loss=0.08529186993837357. train batch time cost=0.10253739356994629s\n",
            "completed batch 29 of epoch 129. loss=0.14889779686927795. train batch time cost=0.1019597053527832s\n",
            "completed batch 30 of epoch 129. loss=0.05175520479679108. train batch time cost=0.10345053672790527s\n",
            "completed batch 31 of epoch 129. loss=0.11344215273857117. train batch time cost=0.10130786895751953s\n",
            "completed batch 32 of epoch 129. loss=0.22964388132095337. train batch time cost=0.10219430923461914s\n",
            "completed batch 33 of epoch 129. loss=0.27156662940979004. train batch time cost=0.10187697410583496s\n",
            "completed batch 34 of epoch 129. loss=0.23608051240444183. train batch time cost=0.10109734535217285s\n",
            "completed batch 35 of epoch 129. loss=0.04986579716205597. train batch time cost=0.10208797454833984s\n",
            "completed batch 36 of epoch 129. loss=0.0328303687274456. train batch time cost=0.10112619400024414s\n",
            "completed batch 37 of epoch 129. loss=0.19079384207725525. train batch time cost=0.10169792175292969s\n",
            "completed batch 38 of epoch 129. loss=0.3771516978740692. train batch time cost=0.10127520561218262s\n",
            "completed batch 39 of epoch 129. loss=0.2727055251598358. train batch time cost=0.10228562355041504s\n",
            "completed batch 40 of epoch 129. loss=0.01191069558262825. train batch time cost=0.10219955444335938s\n",
            "completed batch 41 of epoch 129. loss=0.07307443022727966. train batch time cost=0.10241365432739258s\n",
            "completed batch 42 of epoch 129. loss=0.02915441244840622. train batch time cost=0.10212063789367676s\n",
            "completed batch 43 of epoch 129. loss=0.027512326836586. train batch time cost=0.10101318359375s\n",
            "completed batch 44 of epoch 129. loss=0.05838855355978012. train batch time cost=0.10095763206481934s\n",
            "completed batch 45 of epoch 129. loss=0.11864091455936432. train batch time cost=0.10299468040466309s\n",
            "completed batch 46 of epoch 129. loss=0.04599006846547127. train batch time cost=0.10221600532531738s\n",
            "completed batch 47 of epoch 129. loss=0.12724366784095764. train batch time cost=0.10257101058959961s\n",
            "completed batch 48 of epoch 129. loss=0.06638150662183762. train batch time cost=0.102081298828125s\n",
            "completed batch 49 of epoch 129. loss=0.17608517408370972. train batch time cost=0.10248136520385742s\n",
            "completed batch 50 of epoch 129. loss=0.04243731126189232. train batch time cost=0.10293078422546387s\n",
            "completed batch 51 of epoch 129. loss=0.009385651908814907. train batch time cost=0.10075783729553223s\n",
            "completed batch 52 of epoch 129. loss=0.08010276407003403. train batch time cost=0.10031270980834961s\n",
            "completed batch 53 of epoch 129. loss=0.11244866997003555. train batch time cost=0.10006022453308105s\n",
            "completed batch 54 of epoch 129. loss=0.11763227730989456. train batch time cost=0.10465764999389648s\n",
            "completed batch 55 of epoch 129. loss=0.11500014364719391. train batch time cost=0.09964919090270996s\n",
            "completed batch 56 of epoch 129. loss=0.11633091419935226. train batch time cost=0.10118961334228516s\n",
            "completed batch 57 of epoch 129. loss=0.1356261670589447. train batch time cost=0.10116457939147949s\n",
            "completed batch 58 of epoch 129. loss=0.032525684684515. train batch time cost=0.10268044471740723s\n",
            "completed batch 59 of epoch 129. loss=0.11549945175647736. train batch time cost=0.10320353507995605s\n",
            "completed batch 60 of epoch 129. loss=0.06570859253406525. train batch time cost=0.10138940811157227s\n",
            "completed batch 61 of epoch 129. loss=0.039624255150556564. train batch time cost=0.10209369659423828s\n",
            "completed batch 62 of epoch 129. loss=0.19401204586029053. train batch time cost=0.10038471221923828s\n",
            "completed batch 63 of epoch 129. loss=0.011997907422482967. train batch time cost=0.10387635231018066s\n",
            "completed batch 64 of epoch 129. loss=0.09017887711524963. train batch time cost=0.10428047180175781s\n",
            "completed batch 65 of epoch 129. loss=0.03653668239712715. train batch time cost=0.10091948509216309s\n",
            "completed batch 66 of epoch 129. loss=0.07017631828784943. train batch time cost=0.10016584396362305s\n",
            "completed batch 67 of epoch 129. loss=0.06146863475441933. train batch time cost=0.10457420349121094s\n",
            "completed batch 68 of epoch 129. loss=0.08361069113016129. train batch time cost=0.10495543479919434s\n",
            "completed batch 69 of epoch 129. loss=0.016359534114599228. train batch time cost=0.10164356231689453s\n",
            "completed batch 70 of epoch 129. loss=0.064716637134552. train batch time cost=0.10034632682800293s\n",
            "completed batch 71 of epoch 129. loss=0.17273268103599548. train batch time cost=0.10126280784606934s\n",
            "completed batch 72 of epoch 129. loss=0.011406353674829006. train batch time cost=0.10196995735168457s\n",
            "completed batch 73 of epoch 129. loss=0.014989949762821198. train batch time cost=0.10378026962280273s\n",
            "completed batch 74 of epoch 129. loss=0.055457230657339096. train batch time cost=0.10279726982116699s\n",
            "completed batch 75 of epoch 129. loss=0.1192675307393074. train batch time cost=0.10155463218688965s\n",
            "completed batch 76 of epoch 129. loss=0.0635504424571991. train batch time cost=0.10259675979614258s\n",
            "completed batch 77 of epoch 129. loss=0.041081905364990234. train batch time cost=0.1036980152130127s\n",
            "completed batch 78 of epoch 129. loss=0.03631703183054924. train batch time cost=0.10338521003723145s\n",
            "completed batch 79 of epoch 129. loss=0.08440011739730835. train batch time cost=0.10104608535766602s\n",
            "completed batch 80 of epoch 129. loss=0.040913812816143036. train batch time cost=0.10115694999694824s\n",
            "completed batch 81 of epoch 129. loss=0.1784033179283142. train batch time cost=0.10274600982666016s\n",
            "completed batch 82 of epoch 129. loss=0.07029299437999725. train batch time cost=0.10109663009643555s\n",
            "completed batch 83 of epoch 129. loss=0.08178682625293732. train batch time cost=0.10175085067749023s\n",
            "completed batch 84 of epoch 129. loss=0.020472684875130653. train batch time cost=0.10183358192443848s\n",
            "completed batch 85 of epoch 129. loss=0.031198987737298012. train batch time cost=0.10157608985900879s\n",
            "completed batch 86 of epoch 129. loss=0.02420901320874691. train batch time cost=0.10205292701721191s\n",
            "completed batch 87 of epoch 129. loss=0.08016080409288406. train batch time cost=0.10158777236938477s\n",
            "completed batch 88 of epoch 129. loss=0.032543107867240906. train batch time cost=0.10087919235229492s\n",
            "completed batch 89 of epoch 129. loss=0.04664321616292. train batch time cost=0.10070967674255371s\n",
            "completed batch 90 of epoch 129. loss=0.04288364201784134. train batch time cost=0.10271120071411133s\n",
            "completed batch 91 of epoch 129. loss=0.09921453893184662. train batch time cost=0.10180544853210449s\n",
            "completed batch 92 of epoch 129. loss=0.04652675613760948. train batch time cost=0.10070300102233887s\n",
            "completed batch 93 of epoch 129. loss=0.03401326388120651. train batch time cost=0.10018277168273926s\n",
            "completed batch 94 of epoch 129. loss=0.05989691987633705. train batch time cost=0.10073733329772949s\n",
            "completed batch 95 of epoch 129. loss=0.043024998158216476. train batch time cost=0.10100769996643066s\n",
            "completed batch 96 of epoch 129. loss=0.04002317786216736. train batch time cost=0.10136914253234863s\n",
            "completed batch 97 of epoch 129. loss=0.11244867742061615. train batch time cost=0.10193920135498047s\n",
            "completed batch 98 of epoch 129. loss=0.15093156695365906. train batch time cost=0.1005101203918457s\n",
            "completed batch 99 of epoch 129. loss=0.01820470578968525. train batch time cost=0.10116934776306152s\n",
            "completed batch 100 of epoch 129. loss=0.2396419197320938. train batch time cost=0.10208463668823242s\n",
            "completed batch 101 of epoch 129. loss=0.1083889901638031. train batch time cost=0.10075712203979492s\n",
            "completed batch 102 of epoch 129. loss=0.07595337927341461. train batch time cost=0.10058116912841797s\n",
            "completed batch 103 of epoch 129. loss=0.09833074361085892. train batch time cost=0.10015654563903809s\n",
            "completed batch 104 of epoch 129. loss=0.1062219962477684. train batch time cost=0.10556960105895996s\n",
            "completed batch 105 of epoch 129. loss=0.204926997423172. train batch time cost=0.10379648208618164s\n",
            "completed batch 106 of epoch 129. loss=0.027248375117778778. train batch time cost=0.10202217102050781s\n",
            "completed batch 107 of epoch 129. loss=0.08770446479320526. train batch time cost=0.10241866111755371s\n",
            "completed batch 108 of epoch 129. loss=0.01912318356335163. train batch time cost=0.1023566722869873s\n",
            "completed batch 109 of epoch 129. loss=0.0657813772559166. train batch time cost=0.10303282737731934s\n",
            "completed batch 110 of epoch 129. loss=0.0627545565366745. train batch time cost=0.10441184043884277s\n",
            "completed batch 111 of epoch 129. loss=0.04363762214779854. train batch time cost=0.10345888137817383s\n",
            "completed batch 112 of epoch 129. loss=0.0990777239203453. train batch time cost=0.10392236709594727s\n",
            "completed batch 113 of epoch 129. loss=0.027757201343774796. train batch time cost=0.10401225090026855s\n",
            "completed batch 114 of epoch 129. loss=0.028645403683185577. train batch time cost=0.1031806468963623s\n",
            "completed batch 115 of epoch 129. loss=0.022605260834097862. train batch time cost=0.10130596160888672s\n",
            "completed batch 116 of epoch 129. loss=0.04658813774585724. train batch time cost=0.10174822807312012s\n",
            "completed batch 117 of epoch 129. loss=0.14563435316085815. train batch time cost=0.10185432434082031s\n",
            "completed batch 118 of epoch 129. loss=0.00785425491631031. train batch time cost=0.10603976249694824s\n",
            "completed batch 119 of epoch 129. loss=0.025943627581000328. train batch time cost=0.10252118110656738s\n",
            "completed batch 120 of epoch 129. loss=0.011248789727687836. train batch time cost=0.10314202308654785s\n",
            "completed batch 121 of epoch 129. loss=0.054875750094652176. train batch time cost=0.10295987129211426s\n",
            "completed batch 122 of epoch 129. loss=0.037009187042713165. train batch time cost=0.10391569137573242s\n",
            "completed batch 123 of epoch 129. loss=0.043070219457149506. train batch time cost=0.10172128677368164s\n",
            "completed batch 124 of epoch 129. loss=0.029472770169377327. train batch time cost=0.10148787498474121s\n",
            "completed batch 125 of epoch 129. loss=0.034449100494384766. train batch time cost=0.10115313529968262s\n",
            "completed batch 126 of epoch 129. loss=0.0014504751889035106. train batch time cost=0.029802560806274414s\n",
            "completed test of epoch 129. loss=0.0014504751889035106. accuracy=0.7124313529705442. train one epoch time cost=27.70365595817566s, test validation time cost=3.91347336769104\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442]\n",
            "completed batch 1 of epoch 130. loss=0.14566202461719513. train batch time cost=0.09314799308776855s\n",
            "completed batch 2 of epoch 130. loss=0.01664792187511921. train batch time cost=0.0960242748260498s\n",
            "completed batch 3 of epoch 130. loss=0.06785201281309128. train batch time cost=0.09493231773376465s\n",
            "completed batch 4 of epoch 130. loss=0.03493070229887962. train batch time cost=0.09521341323852539s\n",
            "completed batch 5 of epoch 130. loss=0.008330709300935268. train batch time cost=0.09504461288452148s\n",
            "completed batch 6 of epoch 130. loss=0.04365302994847298. train batch time cost=0.09384942054748535s\n",
            "completed batch 7 of epoch 130. loss=0.07999905198812485. train batch time cost=0.09556770324707031s\n",
            "completed batch 8 of epoch 130. loss=0.01605677790939808. train batch time cost=0.09431958198547363s\n",
            "completed batch 9 of epoch 130. loss=0.0651216208934784. train batch time cost=0.09462547302246094s\n",
            "completed batch 10 of epoch 130. loss=0.037024881690740585. train batch time cost=0.09450006484985352s\n",
            "completed batch 11 of epoch 130. loss=0.037566885352134705. train batch time cost=0.0965571403503418s\n",
            "completed batch 12 of epoch 130. loss=0.023897964507341385. train batch time cost=0.09601306915283203s\n",
            "completed batch 13 of epoch 130. loss=0.03849579393863678. train batch time cost=0.09647440910339355s\n",
            "completed batch 14 of epoch 130. loss=0.05095083639025688. train batch time cost=0.09453082084655762s\n",
            "completed batch 15 of epoch 130. loss=0.029394857585430145. train batch time cost=0.0948643684387207s\n",
            "completed batch 16 of epoch 130. loss=0.006573620717972517. train batch time cost=0.09605717658996582s\n",
            "completed batch 17 of epoch 130. loss=0.019327502697706223. train batch time cost=0.09592652320861816s\n",
            "completed batch 18 of epoch 130. loss=0.008342458866536617. train batch time cost=0.10151410102844238s\n",
            "completed batch 19 of epoch 130. loss=0.02721157856285572. train batch time cost=0.10102605819702148s\n",
            "completed batch 20 of epoch 130. loss=0.04479769617319107. train batch time cost=0.1006474494934082s\n",
            "completed batch 21 of epoch 130. loss=0.03714718669652939. train batch time cost=0.10289573669433594s\n",
            "completed batch 22 of epoch 130. loss=0.029601719230413437. train batch time cost=0.10056328773498535s\n",
            "completed batch 23 of epoch 130. loss=0.008123635314404964. train batch time cost=0.10060763359069824s\n",
            "completed batch 24 of epoch 130. loss=0.01862347684800625. train batch time cost=0.09658598899841309s\n",
            "completed batch 25 of epoch 130. loss=0.011898834258317947. train batch time cost=0.10032248497009277s\n",
            "completed batch 26 of epoch 130. loss=0.02793031930923462. train batch time cost=0.10225820541381836s\n",
            "completed batch 27 of epoch 130. loss=0.06162646412849426. train batch time cost=0.10213470458984375s\n",
            "completed batch 28 of epoch 130. loss=0.02067730575799942. train batch time cost=0.10343003273010254s\n",
            "completed batch 29 of epoch 130. loss=0.08131831139326096. train batch time cost=0.10296821594238281s\n",
            "completed batch 30 of epoch 130. loss=0.011741331778466702. train batch time cost=0.1021583080291748s\n",
            "completed batch 31 of epoch 130. loss=0.006019520107656717. train batch time cost=0.10231518745422363s\n",
            "completed batch 32 of epoch 130. loss=0.08890332281589508. train batch time cost=0.10350179672241211s\n",
            "completed batch 33 of epoch 130. loss=0.09484496712684631. train batch time cost=0.10030817985534668s\n",
            "completed batch 34 of epoch 130. loss=0.04308066517114639. train batch time cost=0.10144257545471191s\n",
            "completed batch 35 of epoch 130. loss=0.011321000754833221. train batch time cost=0.10168886184692383s\n",
            "completed batch 36 of epoch 130. loss=0.07048898935317993. train batch time cost=0.10210680961608887s\n",
            "completed batch 37 of epoch 130. loss=0.04159994795918465. train batch time cost=0.10170292854309082s\n",
            "completed batch 38 of epoch 130. loss=0.05403350293636322. train batch time cost=0.10111498832702637s\n",
            "completed batch 39 of epoch 130. loss=0.020558267831802368. train batch time cost=0.1031043529510498s\n",
            "completed batch 40 of epoch 130. loss=0.04191946983337402. train batch time cost=0.10164618492126465s\n",
            "completed batch 41 of epoch 130. loss=0.06394670903682709. train batch time cost=0.10056185722351074s\n",
            "completed batch 42 of epoch 130. loss=0.10762868076562881. train batch time cost=0.10044288635253906s\n",
            "completed batch 43 of epoch 130. loss=0.014510913752019405. train batch time cost=0.1011343002319336s\n",
            "completed batch 44 of epoch 130. loss=0.012782629579305649. train batch time cost=0.10157108306884766s\n",
            "completed batch 45 of epoch 130. loss=0.09700009226799011. train batch time cost=0.1009368896484375s\n",
            "completed batch 46 of epoch 130. loss=0.07875751703977585. train batch time cost=0.10135245323181152s\n",
            "completed batch 47 of epoch 130. loss=0.08240199834108353. train batch time cost=0.1006307601928711s\n",
            "completed batch 48 of epoch 130. loss=0.040354080498218536. train batch time cost=0.10319638252258301s\n",
            "completed batch 49 of epoch 130. loss=0.12557847797870636. train batch time cost=0.10211849212646484s\n",
            "completed batch 50 of epoch 130. loss=0.09934055805206299. train batch time cost=0.10125923156738281s\n",
            "completed batch 51 of epoch 130. loss=0.008342958986759186. train batch time cost=0.10097622871398926s\n",
            "completed batch 52 of epoch 130. loss=0.011632951907813549. train batch time cost=0.10056638717651367s\n",
            "completed batch 53 of epoch 130. loss=0.012257321737706661. train batch time cost=0.10135912895202637s\n",
            "completed batch 54 of epoch 130. loss=0.15509085357189178. train batch time cost=0.10138964653015137s\n",
            "completed batch 55 of epoch 130. loss=0.01766257919371128. train batch time cost=0.1023409366607666s\n",
            "completed batch 56 of epoch 130. loss=0.02308514155447483. train batch time cost=0.10258293151855469s\n",
            "completed batch 57 of epoch 130. loss=0.015616058371961117. train batch time cost=0.10228562355041504s\n",
            "completed batch 58 of epoch 130. loss=0.008449439890682697. train batch time cost=0.10274648666381836s\n",
            "completed batch 59 of epoch 130. loss=0.02514459379017353. train batch time cost=0.10165905952453613s\n",
            "completed batch 60 of epoch 130. loss=0.08756674826145172. train batch time cost=0.10122203826904297s\n",
            "completed batch 61 of epoch 130. loss=0.08587142080068588. train batch time cost=0.10104799270629883s\n",
            "completed batch 62 of epoch 130. loss=0.018886806443333626. train batch time cost=0.10162830352783203s\n",
            "completed batch 63 of epoch 130. loss=0.040014926344156265. train batch time cost=0.1023874282836914s\n",
            "completed batch 64 of epoch 130. loss=0.025718025863170624. train batch time cost=0.10113024711608887s\n",
            "completed batch 65 of epoch 130. loss=0.002968354383483529. train batch time cost=0.10237431526184082s\n",
            "completed batch 66 of epoch 130. loss=0.04004330933094025. train batch time cost=0.10254526138305664s\n",
            "completed batch 67 of epoch 130. loss=0.059644319117069244. train batch time cost=0.10271906852722168s\n",
            "completed batch 68 of epoch 130. loss=0.045807648450136185. train batch time cost=0.10320639610290527s\n",
            "completed batch 69 of epoch 130. loss=0.14642995595932007. train batch time cost=0.10132980346679688s\n",
            "completed batch 70 of epoch 130. loss=0.031304240226745605. train batch time cost=0.1016688346862793s\n",
            "completed batch 71 of epoch 130. loss=0.051845379173755646. train batch time cost=0.10279202461242676s\n",
            "completed batch 72 of epoch 130. loss=0.004314427729696035. train batch time cost=0.10119795799255371s\n",
            "completed batch 73 of epoch 130. loss=0.18453696370124817. train batch time cost=0.10227036476135254s\n",
            "completed batch 74 of epoch 130. loss=0.0427301824092865. train batch time cost=0.10244894027709961s\n",
            "completed batch 75 of epoch 130. loss=0.08525356650352478. train batch time cost=0.10176229476928711s\n",
            "completed batch 76 of epoch 130. loss=0.04807792603969574. train batch time cost=0.1019432544708252s\n",
            "completed batch 77 of epoch 130. loss=0.01031738892197609. train batch time cost=0.1010584831237793s\n",
            "completed batch 78 of epoch 130. loss=0.005950989667326212. train batch time cost=0.09442305564880371s\n",
            "completed batch 79 of epoch 130. loss=0.01528480090200901. train batch time cost=0.09405231475830078s\n",
            "completed batch 80 of epoch 130. loss=0.007329576648771763. train batch time cost=0.10081648826599121s\n",
            "completed batch 81 of epoch 130. loss=0.06952708959579468. train batch time cost=0.10028266906738281s\n",
            "completed batch 82 of epoch 130. loss=0.21137851476669312. train batch time cost=0.10090136528015137s\n",
            "completed batch 83 of epoch 130. loss=0.04914487898349762. train batch time cost=0.09953999519348145s\n",
            "completed batch 84 of epoch 130. loss=0.1900937408208847. train batch time cost=0.1004939079284668s\n",
            "completed batch 85 of epoch 130. loss=0.1696881800889969. train batch time cost=0.10208916664123535s\n",
            "completed batch 86 of epoch 130. loss=0.0315796360373497. train batch time cost=0.10481834411621094s\n",
            "completed batch 87 of epoch 130. loss=0.11957921087741852. train batch time cost=0.10412955284118652s\n",
            "completed batch 88 of epoch 130. loss=0.03890013322234154. train batch time cost=0.10476803779602051s\n",
            "completed batch 89 of epoch 130. loss=0.08129660040140152. train batch time cost=0.10398483276367188s\n",
            "completed batch 90 of epoch 130. loss=0.04096035286784172. train batch time cost=0.10233187675476074s\n",
            "completed batch 91 of epoch 130. loss=0.011350546963512897. train batch time cost=0.09561562538146973s\n",
            "completed batch 92 of epoch 130. loss=0.0563780777156353. train batch time cost=0.09518575668334961s\n",
            "completed batch 93 of epoch 130. loss=0.0682598277926445. train batch time cost=0.09682345390319824s\n",
            "completed batch 94 of epoch 130. loss=0.1454945206642151. train batch time cost=0.0960853099822998s\n",
            "completed batch 95 of epoch 130. loss=0.1938501000404358. train batch time cost=0.09616613388061523s\n",
            "completed batch 96 of epoch 130. loss=0.18633365631103516. train batch time cost=0.10097956657409668s\n",
            "completed batch 97 of epoch 130. loss=0.12071305513381958. train batch time cost=0.09493613243103027s\n",
            "completed batch 98 of epoch 130. loss=0.08322904258966446. train batch time cost=0.09672069549560547s\n",
            "completed batch 99 of epoch 130. loss=0.04018797352910042. train batch time cost=0.10215210914611816s\n",
            "completed batch 100 of epoch 130. loss=0.02671145088970661. train batch time cost=0.1024773120880127s\n",
            "completed batch 101 of epoch 130. loss=0.025876305997371674. train batch time cost=0.10295844078063965s\n",
            "completed batch 102 of epoch 130. loss=0.1958446353673935. train batch time cost=0.10155677795410156s\n",
            "completed batch 103 of epoch 130. loss=0.08543635159730911. train batch time cost=0.10146188735961914s\n",
            "completed batch 104 of epoch 130. loss=0.4106890857219696. train batch time cost=0.10212922096252441s\n",
            "completed batch 105 of epoch 130. loss=0.07306759059429169. train batch time cost=0.10238122940063477s\n",
            "completed batch 106 of epoch 130. loss=0.09053125977516174. train batch time cost=0.10077810287475586s\n",
            "completed batch 107 of epoch 130. loss=0.026888759806752205. train batch time cost=0.10118556022644043s\n",
            "completed batch 108 of epoch 130. loss=0.26477929949760437. train batch time cost=0.10081982612609863s\n",
            "completed batch 109 of epoch 130. loss=0.0998387411236763. train batch time cost=0.10176491737365723s\n",
            "completed batch 110 of epoch 130. loss=0.01573396660387516. train batch time cost=0.10222601890563965s\n",
            "completed batch 111 of epoch 130. loss=0.10773629695177078. train batch time cost=0.10111045837402344s\n",
            "completed batch 112 of epoch 130. loss=0.2301894724369049. train batch time cost=0.10038924217224121s\n",
            "completed batch 113 of epoch 130. loss=0.058548420667648315. train batch time cost=0.09661531448364258s\n",
            "completed batch 114 of epoch 130. loss=0.022708145901560783. train batch time cost=0.09428834915161133s\n",
            "completed batch 115 of epoch 130. loss=0.04833095148205757. train batch time cost=0.09412813186645508s\n",
            "completed batch 116 of epoch 130. loss=0.15592707693576813. train batch time cost=0.09624290466308594s\n",
            "completed batch 117 of epoch 130. loss=0.04290381819009781. train batch time cost=0.09565162658691406s\n",
            "completed batch 118 of epoch 130. loss=0.1458970308303833. train batch time cost=0.0968780517578125s\n",
            "completed batch 119 of epoch 130. loss=0.07374390214681625. train batch time cost=0.0964348316192627s\n",
            "completed batch 120 of epoch 130. loss=0.010695485398173332. train batch time cost=0.09645748138427734s\n",
            "completed batch 121 of epoch 130. loss=0.03741252049803734. train batch time cost=0.09571123123168945s\n",
            "completed batch 122 of epoch 130. loss=0.0773877501487732. train batch time cost=0.09596371650695801s\n",
            "completed batch 123 of epoch 130. loss=0.04491102322936058. train batch time cost=0.09650421142578125s\n",
            "completed batch 124 of epoch 130. loss=0.19469265639781952. train batch time cost=0.10176801681518555s\n",
            "completed batch 125 of epoch 130. loss=0.025987714529037476. train batch time cost=0.10174202919006348s\n",
            "completed batch 126 of epoch 130. loss=0.004276739899069071. train batch time cost=0.0289609432220459s\n",
            "completed test of epoch 130. loss=0.004276739899069071. accuracy=0.7224163754368448. train one epoch time cost=27.424033641815186s, test validation time cost=3.9269869327545166\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448]\n",
            "completed batch 1 of epoch 131. loss=0.012177230790257454. train batch time cost=0.10267806053161621s\n",
            "completed batch 2 of epoch 131. loss=0.03686382621526718. train batch time cost=0.1035003662109375s\n",
            "completed batch 3 of epoch 131. loss=0.06044408679008484. train batch time cost=0.09965848922729492s\n",
            "completed batch 4 of epoch 131. loss=0.041447337716817856. train batch time cost=0.09946298599243164s\n",
            "completed batch 5 of epoch 131. loss=0.05253668501973152. train batch time cost=0.09482097625732422s\n",
            "completed batch 6 of epoch 131. loss=0.09596574306488037. train batch time cost=0.09417247772216797s\n",
            "completed batch 7 of epoch 131. loss=0.0064739263616502285. train batch time cost=0.09613180160522461s\n",
            "completed batch 8 of epoch 131. loss=0.041085027158260345. train batch time cost=0.0952153205871582s\n",
            "completed batch 9 of epoch 131. loss=0.043488744646310806. train batch time cost=0.09981369972229004s\n",
            "completed batch 10 of epoch 131. loss=0.0408821627497673. train batch time cost=0.09409046173095703s\n",
            "completed batch 11 of epoch 131. loss=0.035331953316926956. train batch time cost=0.09407615661621094s\n",
            "completed batch 12 of epoch 131. loss=0.23466643691062927. train batch time cost=0.09483003616333008s\n",
            "completed batch 13 of epoch 131. loss=0.039711207151412964. train batch time cost=0.09444761276245117s\n",
            "completed batch 14 of epoch 131. loss=0.013301454484462738. train batch time cost=0.09527826309204102s\n",
            "completed batch 15 of epoch 131. loss=0.01488328818231821. train batch time cost=0.09605932235717773s\n",
            "completed batch 16 of epoch 131. loss=0.013567748479545116. train batch time cost=0.09595751762390137s\n",
            "completed batch 17 of epoch 131. loss=0.016319861635565758. train batch time cost=0.09508013725280762s\n",
            "completed batch 18 of epoch 131. loss=0.07965782284736633. train batch time cost=0.0953361988067627s\n",
            "completed batch 19 of epoch 131. loss=0.1362912505865097. train batch time cost=0.09560704231262207s\n",
            "completed batch 20 of epoch 131. loss=0.0074355583637952805. train batch time cost=0.1017603874206543s\n",
            "completed batch 21 of epoch 131. loss=0.04524371027946472. train batch time cost=0.10264325141906738s\n",
            "completed batch 22 of epoch 131. loss=0.040086083114147186. train batch time cost=0.10285782814025879s\n",
            "completed batch 23 of epoch 131. loss=0.11207276582717896. train batch time cost=0.10508108139038086s\n",
            "completed batch 24 of epoch 131. loss=0.020988257601857185. train batch time cost=0.1038675308227539s\n",
            "completed batch 25 of epoch 131. loss=0.022464990615844727. train batch time cost=0.10017776489257812s\n",
            "completed batch 26 of epoch 131. loss=0.017667939886450768. train batch time cost=0.09494400024414062s\n",
            "completed batch 27 of epoch 131. loss=0.08324266970157623. train batch time cost=0.09431958198547363s\n",
            "completed batch 28 of epoch 131. loss=0.024741599336266518. train batch time cost=0.09653353691101074s\n",
            "completed batch 29 of epoch 131. loss=0.011555925942957401. train batch time cost=0.09452247619628906s\n",
            "completed batch 30 of epoch 131. loss=0.005039629992097616. train batch time cost=0.09435582160949707s\n",
            "completed batch 31 of epoch 131. loss=0.025375472381711006. train batch time cost=0.0948648452758789s\n",
            "completed batch 32 of epoch 131. loss=0.023382404819130898. train batch time cost=0.09456825256347656s\n",
            "completed batch 33 of epoch 131. loss=0.04753224551677704. train batch time cost=0.09475874900817871s\n",
            "completed batch 34 of epoch 131. loss=0.02277848683297634. train batch time cost=0.09472799301147461s\n",
            "completed batch 35 of epoch 131. loss=0.09068386256694794. train batch time cost=0.09459781646728516s\n",
            "completed batch 36 of epoch 131. loss=0.024830710142850876. train batch time cost=0.09651756286621094s\n",
            "completed batch 37 of epoch 131. loss=0.021285803988575935. train batch time cost=0.09614682197570801s\n",
            "completed batch 38 of epoch 131. loss=0.015314647927880287. train batch time cost=0.09483575820922852s\n",
            "completed batch 39 of epoch 131. loss=0.03393922373652458. train batch time cost=0.09569859504699707s\n",
            "completed batch 40 of epoch 131. loss=0.0539478100836277. train batch time cost=0.09524941444396973s\n",
            "completed batch 41 of epoch 131. loss=0.03449133411049843. train batch time cost=0.09608340263366699s\n",
            "completed batch 42 of epoch 131. loss=0.06901140511035919. train batch time cost=0.09516739845275879s\n",
            "completed batch 43 of epoch 131. loss=0.018666839227080345. train batch time cost=0.09436488151550293s\n",
            "completed batch 44 of epoch 131. loss=0.012962501496076584. train batch time cost=0.09595227241516113s\n",
            "completed batch 45 of epoch 131. loss=0.025751076638698578. train batch time cost=0.09531092643737793s\n",
            "completed batch 46 of epoch 131. loss=0.0830211192369461. train batch time cost=0.09558272361755371s\n",
            "completed batch 47 of epoch 131. loss=0.005439502652734518. train batch time cost=0.10067343711853027s\n",
            "completed batch 48 of epoch 131. loss=0.029864327982068062. train batch time cost=0.0942542552947998s\n",
            "completed batch 49 of epoch 131. loss=0.10676240921020508. train batch time cost=0.09598088264465332s\n",
            "completed batch 50 of epoch 131. loss=0.02199455536901951. train batch time cost=0.0955495834350586s\n",
            "completed batch 51 of epoch 131. loss=0.028393007814884186. train batch time cost=0.10059380531311035s\n",
            "completed batch 52 of epoch 131. loss=0.008317123167216778. train batch time cost=0.09742975234985352s\n",
            "completed batch 53 of epoch 131. loss=0.011194596067070961. train batch time cost=0.09910702705383301s\n",
            "completed batch 54 of epoch 131. loss=0.026918701827526093. train batch time cost=0.10113191604614258s\n",
            "completed batch 55 of epoch 131. loss=0.030438313260674477. train batch time cost=0.10139727592468262s\n",
            "completed batch 56 of epoch 131. loss=0.02076057530939579. train batch time cost=0.10267496109008789s\n",
            "completed batch 57 of epoch 131. loss=0.007604156155139208. train batch time cost=0.10096454620361328s\n",
            "completed batch 58 of epoch 131. loss=0.014180157333612442. train batch time cost=0.10325813293457031s\n",
            "completed batch 59 of epoch 131. loss=0.012476199306547642. train batch time cost=0.10337424278259277s\n",
            "completed batch 60 of epoch 131. loss=0.028017766773700714. train batch time cost=0.1041097640991211s\n",
            "completed batch 61 of epoch 131. loss=0.04922526329755783. train batch time cost=0.10075521469116211s\n",
            "completed batch 62 of epoch 131. loss=0.020414745435118675. train batch time cost=0.10513687133789062s\n",
            "completed batch 63 of epoch 131. loss=0.028036227449774742. train batch time cost=0.10345458984375s\n",
            "completed batch 64 of epoch 131. loss=0.01693345606327057. train batch time cost=0.10294723510742188s\n",
            "completed batch 65 of epoch 131. loss=0.002057393081486225. train batch time cost=0.1039879322052002s\n",
            "completed batch 66 of epoch 131. loss=0.04894272983074188. train batch time cost=0.10224676132202148s\n",
            "completed batch 67 of epoch 131. loss=0.01842530444264412. train batch time cost=0.1029195785522461s\n",
            "completed batch 68 of epoch 131. loss=0.05772453546524048. train batch time cost=0.1027834415435791s\n",
            "completed batch 69 of epoch 131. loss=0.010245596058666706. train batch time cost=0.10248899459838867s\n",
            "completed batch 70 of epoch 131. loss=0.008317702449858189. train batch time cost=0.10144948959350586s\n",
            "completed batch 71 of epoch 131. loss=0.038692593574523926. train batch time cost=0.10196709632873535s\n",
            "completed batch 72 of epoch 131. loss=0.008989490568637848. train batch time cost=0.10146164894104004s\n",
            "completed batch 73 of epoch 131. loss=0.009009269997477531. train batch time cost=0.10198426246643066s\n",
            "completed batch 74 of epoch 131. loss=0.06808843463659286. train batch time cost=0.10156655311584473s\n",
            "completed batch 75 of epoch 131. loss=0.04556246101856232. train batch time cost=0.10190534591674805s\n",
            "completed batch 76 of epoch 131. loss=0.010233649052679539. train batch time cost=0.10215592384338379s\n",
            "completed batch 77 of epoch 131. loss=0.011407669633626938. train batch time cost=0.10261154174804688s\n",
            "completed batch 78 of epoch 131. loss=0.06674044579267502. train batch time cost=0.10320019721984863s\n",
            "completed batch 79 of epoch 131. loss=0.030791355296969414. train batch time cost=0.10185599327087402s\n",
            "completed batch 80 of epoch 131. loss=0.01512698084115982. train batch time cost=0.1010286808013916s\n",
            "completed batch 81 of epoch 131. loss=0.11375319212675095. train batch time cost=0.1022040843963623s\n",
            "completed batch 82 of epoch 131. loss=0.03282203897833824. train batch time cost=0.10259294509887695s\n",
            "completed batch 83 of epoch 131. loss=0.031234297901391983. train batch time cost=0.10137581825256348s\n",
            "completed batch 84 of epoch 131. loss=0.015194245614111423. train batch time cost=0.10177946090698242s\n",
            "completed batch 85 of epoch 131. loss=0.025504879653453827. train batch time cost=0.10196089744567871s\n",
            "completed batch 86 of epoch 131. loss=0.04371266067028046. train batch time cost=0.10365414619445801s\n",
            "completed batch 87 of epoch 131. loss=0.024785863235592842. train batch time cost=0.10439372062683105s\n",
            "completed batch 88 of epoch 131. loss=0.029423637315630913. train batch time cost=0.10292458534240723s\n",
            "completed batch 89 of epoch 131. loss=0.008282646536827087. train batch time cost=0.10375165939331055s\n",
            "completed batch 90 of epoch 131. loss=0.025992242619395256. train batch time cost=0.10187387466430664s\n",
            "completed batch 91 of epoch 131. loss=0.06114941090345383. train batch time cost=0.09990477561950684s\n",
            "completed batch 92 of epoch 131. loss=0.008238722570240498. train batch time cost=0.09671664237976074s\n",
            "completed batch 93 of epoch 131. loss=0.02807484194636345. train batch time cost=0.10210347175598145s\n",
            "completed batch 94 of epoch 131. loss=0.02384226769208908. train batch time cost=0.10246920585632324s\n",
            "completed batch 95 of epoch 131. loss=0.06111874058842659. train batch time cost=0.10136771202087402s\n",
            "completed batch 96 of epoch 131. loss=0.012951554730534554. train batch time cost=0.1031956672668457s\n",
            "completed batch 97 of epoch 131. loss=0.06200161948800087. train batch time cost=0.10268664360046387s\n",
            "completed batch 98 of epoch 131. loss=0.005647767335176468. train batch time cost=0.1022334098815918s\n",
            "completed batch 99 of epoch 131. loss=0.008159742690622807. train batch time cost=0.10131525993347168s\n",
            "completed batch 100 of epoch 131. loss=0.04108549654483795. train batch time cost=0.10156989097595215s\n",
            "completed batch 101 of epoch 131. loss=0.004230146761983633. train batch time cost=0.10471940040588379s\n",
            "completed batch 102 of epoch 131. loss=0.027760125696659088. train batch time cost=0.10135483741760254s\n",
            "completed batch 103 of epoch 131. loss=0.10799067467451096. train batch time cost=0.1021122932434082s\n",
            "completed batch 104 of epoch 131. loss=0.0034090124536305666. train batch time cost=0.10056161880493164s\n",
            "completed batch 105 of epoch 131. loss=0.0031158956699073315. train batch time cost=0.10350203514099121s\n",
            "completed batch 106 of epoch 131. loss=0.004761865362524986. train batch time cost=0.10159754753112793s\n",
            "completed batch 107 of epoch 131. loss=0.044696077704429626. train batch time cost=0.10357141494750977s\n",
            "completed batch 108 of epoch 131. loss=0.020966170355677605. train batch time cost=0.10361385345458984s\n",
            "completed batch 109 of epoch 131. loss=0.04781385511159897. train batch time cost=0.10118460655212402s\n",
            "completed batch 110 of epoch 131. loss=0.017799340188503265. train batch time cost=0.10054874420166016s\n",
            "completed batch 111 of epoch 131. loss=0.01875624991953373. train batch time cost=0.09877848625183105s\n",
            "completed batch 112 of epoch 131. loss=0.012960901483893394. train batch time cost=0.10246086120605469s\n",
            "completed batch 113 of epoch 131. loss=0.021531060338020325. train batch time cost=0.10083627700805664s\n",
            "completed batch 114 of epoch 131. loss=0.016975559294223785. train batch time cost=0.10390496253967285s\n",
            "completed batch 115 of epoch 131. loss=0.011729614809155464. train batch time cost=0.10282707214355469s\n",
            "completed batch 116 of epoch 131. loss=0.008485008031129837. train batch time cost=0.10109901428222656s\n",
            "completed batch 117 of epoch 131. loss=0.05801239237189293. train batch time cost=0.10303854942321777s\n",
            "completed batch 118 of epoch 131. loss=0.009160587564110756. train batch time cost=0.10307097434997559s\n",
            "completed batch 119 of epoch 131. loss=0.04112226143479347. train batch time cost=0.10276341438293457s\n",
            "completed batch 120 of epoch 131. loss=0.008539938367903233. train batch time cost=0.10177397727966309s\n",
            "completed batch 121 of epoch 131. loss=0.006468337494879961. train batch time cost=0.1012113094329834s\n",
            "completed batch 122 of epoch 131. loss=0.046673908829689026. train batch time cost=0.101654052734375s\n",
            "completed batch 123 of epoch 131. loss=0.010655084624886513. train batch time cost=0.1021573543548584s\n",
            "completed batch 124 of epoch 131. loss=0.017430569976568222. train batch time cost=0.10155868530273438s\n",
            "completed batch 125 of epoch 131. loss=0.012352757155895233. train batch time cost=0.10241889953613281s\n",
            "completed batch 126 of epoch 131. loss=0.00012469668581616133. train batch time cost=0.03003859519958496s\n",
            "completed test of epoch 131. loss=0.00012469668581616133. accuracy=0.6285571642536195. train one epoch time cost=27.41602921485901s, test validation time cost=3.8660430908203125\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195]\n",
            "completed batch 1 of epoch 132. loss=0.026400206610560417. train batch time cost=0.09641695022583008s\n",
            "completed batch 2 of epoch 132. loss=0.032949384301900864. train batch time cost=0.09646964073181152s\n",
            "completed batch 3 of epoch 132. loss=0.1363084763288498. train batch time cost=0.09618544578552246s\n",
            "completed batch 4 of epoch 132. loss=0.03127863258123398. train batch time cost=0.09562849998474121s\n",
            "completed batch 5 of epoch 132. loss=0.03359550982713699. train batch time cost=0.09580326080322266s\n",
            "completed batch 6 of epoch 132. loss=0.014222227036952972. train batch time cost=0.09421396255493164s\n",
            "completed batch 7 of epoch 132. loss=0.00288232509046793. train batch time cost=0.09466671943664551s\n",
            "completed batch 8 of epoch 132. loss=0.06783637404441833. train batch time cost=0.09791016578674316s\n",
            "completed batch 9 of epoch 132. loss=0.03749354928731918. train batch time cost=0.09637141227722168s\n",
            "completed batch 10 of epoch 132. loss=0.0025575796607881784. train batch time cost=0.09699368476867676s\n",
            "completed batch 11 of epoch 132. loss=0.020581692457199097. train batch time cost=0.09533286094665527s\n",
            "completed batch 12 of epoch 132. loss=0.010595722123980522. train batch time cost=0.09759330749511719s\n",
            "completed batch 13 of epoch 132. loss=0.013706283643841743. train batch time cost=0.0974888801574707s\n",
            "completed batch 14 of epoch 132. loss=0.019683029502630234. train batch time cost=0.09446549415588379s\n",
            "completed batch 15 of epoch 132. loss=0.01753305457532406. train batch time cost=0.09454703330993652s\n",
            "completed batch 16 of epoch 132. loss=0.005266787949949503. train batch time cost=0.09535050392150879s\n",
            "completed batch 17 of epoch 132. loss=0.049143433570861816. train batch time cost=0.09457182884216309s\n",
            "completed batch 18 of epoch 132. loss=0.002038204576820135. train batch time cost=0.09564924240112305s\n",
            "completed batch 19 of epoch 132. loss=0.0708078145980835. train batch time cost=0.09560847282409668s\n",
            "completed batch 20 of epoch 132. loss=0.008018851280212402. train batch time cost=0.0958259105682373s\n",
            "completed batch 21 of epoch 132. loss=0.0258026085793972. train batch time cost=0.10440969467163086s\n",
            "completed batch 22 of epoch 132. loss=0.032263919711112976. train batch time cost=0.10291314125061035s\n",
            "completed batch 23 of epoch 132. loss=0.10598136484622955. train batch time cost=0.1016535758972168s\n",
            "completed batch 24 of epoch 132. loss=0.005247020162642002. train batch time cost=0.10036969184875488s\n",
            "completed batch 25 of epoch 132. loss=0.0057711354456841946. train batch time cost=0.10121464729309082s\n",
            "completed batch 26 of epoch 132. loss=0.00885491631925106. train batch time cost=0.10331392288208008s\n",
            "completed batch 27 of epoch 132. loss=0.003870188957080245. train batch time cost=0.10282444953918457s\n",
            "completed batch 28 of epoch 132. loss=0.00946408323943615. train batch time cost=0.10019469261169434s\n",
            "completed batch 29 of epoch 132. loss=0.0030002545099705458. train batch time cost=0.09952807426452637s\n",
            "completed batch 30 of epoch 132. loss=0.0012321457033976912. train batch time cost=0.10332155227661133s\n",
            "completed batch 31 of epoch 132. loss=0.004057147540152073. train batch time cost=0.10299181938171387s\n",
            "completed batch 32 of epoch 132. loss=0.11168096959590912. train batch time cost=0.09836196899414062s\n",
            "completed batch 33 of epoch 132. loss=0.024103384464979172. train batch time cost=0.09476637840270996s\n",
            "completed batch 34 of epoch 132. loss=0.02305176854133606. train batch time cost=0.09928250312805176s\n",
            "completed batch 35 of epoch 132. loss=0.013025053776800632. train batch time cost=0.09850692749023438s\n",
            "completed batch 36 of epoch 132. loss=0.00782934483140707. train batch time cost=0.09633612632751465s\n",
            "completed batch 37 of epoch 132. loss=0.0073469593189656734. train batch time cost=0.09636974334716797s\n",
            "completed batch 38 of epoch 132. loss=0.002510795136913657. train batch time cost=0.09600448608398438s\n",
            "completed batch 39 of epoch 132. loss=0.011369777843356133. train batch time cost=0.09686779975891113s\n",
            "completed batch 40 of epoch 132. loss=0.0030666489619761705. train batch time cost=0.09583353996276855s\n",
            "completed batch 41 of epoch 132. loss=0.02665429562330246. train batch time cost=0.09707880020141602s\n",
            "completed batch 42 of epoch 132. loss=0.007195415440946817. train batch time cost=0.09569621086120605s\n",
            "completed batch 43 of epoch 132. loss=0.007757177110761404. train batch time cost=0.09458708763122559s\n",
            "completed batch 44 of epoch 132. loss=0.0033282912336289883. train batch time cost=0.09399151802062988s\n",
            "completed batch 45 of epoch 132. loss=0.009169488213956356. train batch time cost=0.09448981285095215s\n",
            "completed batch 46 of epoch 132. loss=0.017571430653333664. train batch time cost=0.09487509727478027s\n",
            "completed batch 47 of epoch 132. loss=0.0159841850399971. train batch time cost=0.09397006034851074s\n",
            "completed batch 48 of epoch 132. loss=0.023585626855492592. train batch time cost=0.09473276138305664s\n",
            "completed batch 49 of epoch 132. loss=0.005625379737466574. train batch time cost=0.09566950798034668s\n",
            "completed batch 50 of epoch 132. loss=0.011601060628890991. train batch time cost=0.09626173973083496s\n",
            "completed batch 51 of epoch 132. loss=0.02364428900182247. train batch time cost=0.09578084945678711s\n",
            "completed batch 52 of epoch 132. loss=0.010104591958224773. train batch time cost=0.09492325782775879s\n",
            "completed batch 53 of epoch 132. loss=0.019022928550839424. train batch time cost=0.09795165061950684s\n",
            "completed batch 54 of epoch 132. loss=0.01661122590303421. train batch time cost=0.09612727165222168s\n",
            "completed batch 55 of epoch 132. loss=0.014673657715320587. train batch time cost=0.09634256362915039s\n",
            "completed batch 56 of epoch 132. loss=0.011853237636387348. train batch time cost=0.09450459480285645s\n",
            "completed batch 57 of epoch 132. loss=0.0147336944937706. train batch time cost=0.09404730796813965s\n",
            "completed batch 58 of epoch 132. loss=0.019987307488918304. train batch time cost=0.09505748748779297s\n",
            "completed batch 59 of epoch 132. loss=0.06810128688812256. train batch time cost=0.09501242637634277s\n",
            "completed batch 60 of epoch 132. loss=0.021030882373452187. train batch time cost=0.09868144989013672s\n",
            "completed batch 61 of epoch 132. loss=0.01761784590780735. train batch time cost=0.09452629089355469s\n",
            "completed batch 62 of epoch 132. loss=0.008408011868596077. train batch time cost=0.09380602836608887s\n",
            "completed batch 63 of epoch 132. loss=0.014784909784793854. train batch time cost=0.10100936889648438s\n",
            "completed batch 64 of epoch 132. loss=0.010951261967420578. train batch time cost=0.10157299041748047s\n",
            "completed batch 65 of epoch 132. loss=0.011127926409244537. train batch time cost=0.1015632152557373s\n",
            "completed batch 66 of epoch 132. loss=0.005036657210439444. train batch time cost=0.10036897659301758s\n",
            "completed batch 67 of epoch 132. loss=0.01834839954972267. train batch time cost=0.1029670238494873s\n",
            "completed batch 68 of epoch 132. loss=0.004247347358614206. train batch time cost=0.10619664192199707s\n",
            "completed batch 69 of epoch 132. loss=0.0009731986792758107. train batch time cost=0.10291314125061035s\n",
            "completed batch 70 of epoch 132. loss=0.01883496530354023. train batch time cost=0.10045337677001953s\n",
            "completed batch 71 of epoch 132. loss=0.022145522758364677. train batch time cost=0.09995126724243164s\n",
            "completed batch 72 of epoch 132. loss=0.010830849409103394. train batch time cost=0.10044670104980469s\n",
            "completed batch 73 of epoch 132. loss=0.04502711817622185. train batch time cost=0.1022942066192627s\n",
            "completed batch 74 of epoch 132. loss=0.035823166370391846. train batch time cost=0.10188937187194824s\n",
            "completed batch 75 of epoch 132. loss=0.015145872719585896. train batch time cost=0.10042500495910645s\n",
            "completed batch 76 of epoch 132. loss=0.000835457700304687. train batch time cost=0.10211825370788574s\n",
            "completed batch 77 of epoch 132. loss=0.0035750463139265776. train batch time cost=0.1028740406036377s\n",
            "completed batch 78 of epoch 132. loss=0.012761641293764114. train batch time cost=0.10390090942382812s\n",
            "completed batch 79 of epoch 132. loss=0.012256518937647343. train batch time cost=0.10199832916259766s\n",
            "completed batch 80 of epoch 132. loss=0.012394090183079243. train batch time cost=0.10077929496765137s\n",
            "completed batch 81 of epoch 132. loss=0.03368080034852028. train batch time cost=0.10174155235290527s\n",
            "completed batch 82 of epoch 132. loss=0.004665795248001814. train batch time cost=0.10245513916015625s\n",
            "completed batch 83 of epoch 132. loss=0.007320854812860489. train batch time cost=0.10194516181945801s\n",
            "completed batch 84 of epoch 132. loss=0.08068535476922989. train batch time cost=0.10075831413269043s\n",
            "completed batch 85 of epoch 132. loss=0.04543488100171089. train batch time cost=0.10211896896362305s\n",
            "completed batch 86 of epoch 132. loss=0.002015813020989299. train batch time cost=0.10472702980041504s\n",
            "completed batch 87 of epoch 132. loss=0.007420497480779886. train batch time cost=0.10046982765197754s\n",
            "completed batch 88 of epoch 132. loss=0.021058306097984314. train batch time cost=0.10060477256774902s\n",
            "completed batch 89 of epoch 132. loss=0.060260333120822906. train batch time cost=0.10277819633483887s\n",
            "completed batch 90 of epoch 132. loss=0.013591245748102665. train batch time cost=0.10199809074401855s\n",
            "completed batch 91 of epoch 132. loss=0.0020701605826616287. train batch time cost=0.10180187225341797s\n",
            "completed batch 92 of epoch 132. loss=0.03860347718000412. train batch time cost=0.10231471061706543s\n",
            "completed batch 93 of epoch 132. loss=0.01675287075340748. train batch time cost=0.10135746002197266s\n",
            "completed batch 94 of epoch 132. loss=0.024720577523112297. train batch time cost=0.10098505020141602s\n",
            "completed batch 95 of epoch 132. loss=0.0322563573718071. train batch time cost=0.10176897048950195s\n",
            "completed batch 96 of epoch 132. loss=0.06397057324647903. train batch time cost=0.10355234146118164s\n",
            "completed batch 97 of epoch 132. loss=0.01835104636847973. train batch time cost=0.10126471519470215s\n",
            "completed batch 98 of epoch 132. loss=0.0567169189453125. train batch time cost=0.0998845100402832s\n",
            "completed batch 99 of epoch 132. loss=0.010198479518294334. train batch time cost=0.10075759887695312s\n",
            "completed batch 100 of epoch 132. loss=0.005061456933617592. train batch time cost=0.10085511207580566s\n",
            "completed batch 101 of epoch 132. loss=0.04697810113430023. train batch time cost=0.10141563415527344s\n",
            "completed batch 102 of epoch 132. loss=0.012542767450213432. train batch time cost=0.10059809684753418s\n",
            "completed batch 103 of epoch 132. loss=0.015020422637462616. train batch time cost=0.1011662483215332s\n",
            "completed batch 104 of epoch 132. loss=0.08325690031051636. train batch time cost=0.10191679000854492s\n",
            "completed batch 105 of epoch 132. loss=0.1652880162000656. train batch time cost=0.10227370262145996s\n",
            "completed batch 106 of epoch 132. loss=0.0037563268560916185. train batch time cost=0.10164856910705566s\n",
            "completed batch 107 of epoch 132. loss=0.033194128423929214. train batch time cost=0.1011514663696289s\n",
            "completed batch 108 of epoch 132. loss=0.0405283086001873. train batch time cost=0.10112428665161133s\n",
            "completed batch 109 of epoch 132. loss=0.024800198152661324. train batch time cost=0.10096263885498047s\n",
            "completed batch 110 of epoch 132. loss=0.5103304386138916. train batch time cost=0.10993218421936035s\n",
            "completed batch 111 of epoch 132. loss=0.008168688975274563. train batch time cost=0.10084795951843262s\n",
            "completed batch 112 of epoch 132. loss=0.05470624566078186. train batch time cost=0.10031771659851074s\n",
            "completed batch 113 of epoch 132. loss=0.02067854255437851. train batch time cost=0.10129499435424805s\n",
            "completed batch 114 of epoch 132. loss=0.02706766314804554. train batch time cost=0.10365700721740723s\n",
            "completed batch 115 of epoch 132. loss=0.005061975680291653. train batch time cost=0.10187458992004395s\n",
            "completed batch 116 of epoch 132. loss=0.07300510257482529. train batch time cost=0.10051131248474121s\n",
            "completed batch 117 of epoch 132. loss=0.03219035640358925. train batch time cost=0.10108327865600586s\n",
            "completed batch 118 of epoch 132. loss=0.002647318411618471. train batch time cost=0.10231709480285645s\n",
            "completed batch 119 of epoch 132. loss=0.016365034505724907. train batch time cost=0.10214042663574219s\n",
            "completed batch 120 of epoch 132. loss=0.0169562716037035. train batch time cost=0.10179710388183594s\n",
            "completed batch 121 of epoch 132. loss=0.03836248070001602. train batch time cost=0.10165143013000488s\n",
            "completed batch 122 of epoch 132. loss=0.01773391105234623. train batch time cost=0.10073494911193848s\n",
            "completed batch 123 of epoch 132. loss=0.024362048134207726. train batch time cost=0.10346269607543945s\n",
            "completed batch 124 of epoch 132. loss=0.032450467348098755. train batch time cost=0.10164332389831543s\n",
            "completed batch 125 of epoch 132. loss=0.0084388991817832. train batch time cost=0.1013941764831543s\n",
            "completed batch 126 of epoch 132. loss=0.00014113941870164126. train batch time cost=0.029860973358154297s\n",
            "completed test of epoch 132. loss=0.00014113941870164126. accuracy=0.7164253619570644. train one epoch time cost=27.374852180480957s, test validation time cost=3.9334685802459717\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644]\n",
            "completed batch 1 of epoch 133. loss=0.014362892135977745. train batch time cost=0.10300111770629883s\n",
            "completed batch 2 of epoch 133. loss=0.019967464730143547. train batch time cost=0.1032261848449707s\n",
            "completed batch 3 of epoch 133. loss=0.020126575604081154. train batch time cost=0.10134410858154297s\n",
            "completed batch 4 of epoch 133. loss=0.02207643911242485. train batch time cost=0.10247039794921875s\n",
            "completed batch 5 of epoch 133. loss=0.024242645129561424. train batch time cost=0.10079026222229004s\n",
            "completed batch 6 of epoch 133. loss=0.09749212116003036. train batch time cost=0.10066580772399902s\n",
            "completed batch 7 of epoch 133. loss=0.03424046188592911. train batch time cost=0.10453534126281738s\n",
            "completed batch 8 of epoch 133. loss=0.009337355382740498. train batch time cost=0.10034656524658203s\n",
            "completed batch 9 of epoch 133. loss=0.09854432940483093. train batch time cost=0.0946207046508789s\n",
            "completed batch 10 of epoch 133. loss=0.09243740141391754. train batch time cost=0.0951228141784668s\n",
            "completed batch 11 of epoch 133. loss=0.11285170912742615. train batch time cost=0.09505677223205566s\n",
            "completed batch 12 of epoch 133. loss=0.011638419702649117. train batch time cost=0.09532761573791504s\n",
            "completed batch 13 of epoch 133. loss=0.02748592011630535. train batch time cost=0.09597635269165039s\n",
            "completed batch 14 of epoch 133. loss=0.07045122981071472. train batch time cost=0.09458231925964355s\n",
            "completed batch 15 of epoch 133. loss=0.022575512528419495. train batch time cost=0.10142374038696289s\n",
            "completed batch 16 of epoch 133. loss=0.009152150712907314. train batch time cost=0.10066771507263184s\n",
            "completed batch 17 of epoch 133. loss=0.006575001869350672. train batch time cost=0.10088491439819336s\n",
            "completed batch 18 of epoch 133. loss=0.007915690541267395. train batch time cost=0.09778118133544922s\n",
            "completed batch 19 of epoch 133. loss=0.005497653502970934. train batch time cost=0.09443521499633789s\n",
            "completed batch 20 of epoch 133. loss=0.007743396330624819. train batch time cost=0.0940101146697998s\n",
            "completed batch 21 of epoch 133. loss=0.005096654407680035. train batch time cost=0.09506392478942871s\n",
            "completed batch 22 of epoch 133. loss=0.004791077692061663. train batch time cost=0.09598660469055176s\n",
            "completed batch 23 of epoch 133. loss=0.00811819639056921. train batch time cost=0.09554457664489746s\n",
            "completed batch 24 of epoch 133. loss=0.018696224316954613. train batch time cost=0.09520220756530762s\n",
            "completed batch 25 of epoch 133. loss=0.007783580105751753. train batch time cost=0.09626603126525879s\n",
            "completed batch 26 of epoch 133. loss=0.008634130470454693. train batch time cost=0.09722232818603516s\n",
            "completed batch 27 of epoch 133. loss=0.0025980856735259295. train batch time cost=0.09678316116333008s\n",
            "completed batch 28 of epoch 133. loss=0.0495317243039608. train batch time cost=0.09499359130859375s\n",
            "completed batch 29 of epoch 133. loss=0.010147073306143284. train batch time cost=0.09436845779418945s\n",
            "completed batch 30 of epoch 133. loss=0.06403123587369919. train batch time cost=0.09491944313049316s\n",
            "completed batch 31 of epoch 133. loss=0.002269268501549959. train batch time cost=0.09515690803527832s\n",
            "completed batch 32 of epoch 133. loss=0.07413344830274582. train batch time cost=0.09537768363952637s\n",
            "completed batch 33 of epoch 133. loss=0.014047007076442242. train batch time cost=0.0957634449005127s\n",
            "completed batch 34 of epoch 133. loss=0.02314930595457554. train batch time cost=0.09479427337646484s\n",
            "completed batch 35 of epoch 133. loss=0.004412953741848469. train batch time cost=0.09564018249511719s\n",
            "completed batch 36 of epoch 133. loss=0.004464639350771904. train batch time cost=0.09607362747192383s\n",
            "completed batch 37 of epoch 133. loss=0.0061852228827774525. train batch time cost=0.0948021411895752s\n",
            "completed batch 38 of epoch 133. loss=0.012172814458608627. train batch time cost=0.09586024284362793s\n",
            "completed batch 39 of epoch 133. loss=0.01783607341349125. train batch time cost=0.09526848793029785s\n",
            "completed batch 40 of epoch 133. loss=0.015232466161251068. train batch time cost=0.0957784652709961s\n",
            "completed batch 41 of epoch 133. loss=0.0894564613699913. train batch time cost=0.0976707935333252s\n",
            "completed batch 42 of epoch 133. loss=0.008746919222176075. train batch time cost=0.09614348411560059s\n",
            "completed batch 43 of epoch 133. loss=0.038588471710681915. train batch time cost=0.09464454650878906s\n",
            "completed batch 44 of epoch 133. loss=0.014293648302555084. train batch time cost=0.09664273262023926s\n",
            "completed batch 45 of epoch 133. loss=0.001932168030180037. train batch time cost=0.09609651565551758s\n",
            "completed batch 46 of epoch 133. loss=0.0023252004757523537. train batch time cost=0.09524822235107422s\n",
            "completed batch 47 of epoch 133. loss=0.019828131422400475. train batch time cost=0.09400320053100586s\n",
            "completed batch 48 of epoch 133. loss=0.016575254499912262. train batch time cost=0.09456205368041992s\n",
            "completed batch 49 of epoch 133. loss=0.010225938633084297. train batch time cost=0.10478687286376953s\n",
            "completed batch 50 of epoch 133. loss=0.054283980280160904. train batch time cost=0.10195016860961914s\n",
            "completed batch 51 of epoch 133. loss=0.000917722936719656. train batch time cost=0.1020348072052002s\n",
            "completed batch 52 of epoch 133. loss=0.006754579022526741. train batch time cost=0.10153913497924805s\n",
            "completed batch 53 of epoch 133. loss=0.0035399349872022867. train batch time cost=0.1035315990447998s\n",
            "completed batch 54 of epoch 133. loss=0.015907548367977142. train batch time cost=0.10456514358520508s\n",
            "completed batch 55 of epoch 133. loss=0.005651878193020821. train batch time cost=0.10125589370727539s\n",
            "completed batch 56 of epoch 133. loss=0.008146511390805244. train batch time cost=0.10176873207092285s\n",
            "completed batch 57 of epoch 133. loss=0.002460826188325882. train batch time cost=0.10314559936523438s\n",
            "completed batch 58 of epoch 133. loss=0.019941125065088272. train batch time cost=0.10568761825561523s\n",
            "completed batch 59 of epoch 133. loss=0.0034142236690968275. train batch time cost=0.10249805450439453s\n",
            "completed batch 60 of epoch 133. loss=0.027921995148062706. train batch time cost=0.10089683532714844s\n",
            "completed batch 61 of epoch 133. loss=0.003376471810042858. train batch time cost=0.10206913948059082s\n",
            "completed batch 62 of epoch 133. loss=0.014133726246654987. train batch time cost=0.10182309150695801s\n",
            "completed batch 63 of epoch 133. loss=0.008209280669689178. train batch time cost=0.1014108657836914s\n",
            "completed batch 64 of epoch 133. loss=0.012148391455411911. train batch time cost=0.10321235656738281s\n",
            "completed batch 65 of epoch 133. loss=0.004154751542955637. train batch time cost=0.1027827262878418s\n",
            "completed batch 66 of epoch 133. loss=0.009034307673573494. train batch time cost=0.10186958312988281s\n",
            "completed batch 67 of epoch 133. loss=0.005544536281377077. train batch time cost=0.10223507881164551s\n",
            "completed batch 68 of epoch 133. loss=0.004815025720745325. train batch time cost=0.10309624671936035s\n",
            "completed batch 69 of epoch 133. loss=0.0014682896435260773. train batch time cost=0.10222697257995605s\n",
            "completed batch 70 of epoch 133. loss=0.01019122265279293. train batch time cost=0.10363292694091797s\n",
            "completed batch 71 of epoch 133. loss=0.013439906761050224. train batch time cost=0.10123467445373535s\n",
            "completed batch 72 of epoch 133. loss=0.0024513835087418556. train batch time cost=0.10239529609680176s\n",
            "completed batch 73 of epoch 133. loss=0.006885236129164696. train batch time cost=0.10245108604431152s\n",
            "completed batch 74 of epoch 133. loss=0.0069283852353692055. train batch time cost=0.1017155647277832s\n",
            "completed batch 75 of epoch 133. loss=0.06830190122127533. train batch time cost=0.09555506706237793s\n",
            "completed batch 76 of epoch 133. loss=0.036888834089040756. train batch time cost=0.09695124626159668s\n",
            "completed batch 77 of epoch 133. loss=0.004793982487171888. train batch time cost=0.0956718921661377s\n",
            "completed batch 78 of epoch 133. loss=0.08638735860586166. train batch time cost=0.09505438804626465s\n",
            "completed batch 79 of epoch 133. loss=0.0032249747309833765. train batch time cost=0.09674334526062012s\n",
            "completed batch 80 of epoch 133. loss=0.00554251903668046. train batch time cost=0.09532499313354492s\n",
            "completed batch 81 of epoch 133. loss=0.02996288612484932. train batch time cost=0.09519577026367188s\n",
            "completed batch 82 of epoch 133. loss=0.00953415036201477. train batch time cost=0.09654402732849121s\n",
            "completed batch 83 of epoch 133. loss=0.010490423068404198. train batch time cost=0.09757375717163086s\n",
            "completed batch 84 of epoch 133. loss=0.0024907973129302263. train batch time cost=0.09706258773803711s\n",
            "completed batch 85 of epoch 133. loss=0.016060585156083107. train batch time cost=0.09447360038757324s\n",
            "completed batch 86 of epoch 133. loss=0.007572293747216463. train batch time cost=0.09511828422546387s\n",
            "completed batch 87 of epoch 133. loss=0.003968411590903997. train batch time cost=0.09712529182434082s\n",
            "completed batch 88 of epoch 133. loss=0.006037763319909573. train batch time cost=0.09534621238708496s\n",
            "completed batch 89 of epoch 133. loss=0.003653241554275155. train batch time cost=0.09568095207214355s\n",
            "completed batch 90 of epoch 133. loss=0.009433592669665813. train batch time cost=0.0940852165222168s\n",
            "completed batch 91 of epoch 133. loss=0.011639893054962158. train batch time cost=0.09546279907226562s\n",
            "completed batch 92 of epoch 133. loss=0.03672284632921219. train batch time cost=0.09602880477905273s\n",
            "completed batch 93 of epoch 133. loss=0.010358878411352634. train batch time cost=0.09527182579040527s\n",
            "completed batch 94 of epoch 133. loss=0.03196549043059349. train batch time cost=0.0943148136138916s\n",
            "completed batch 95 of epoch 133. loss=0.007547114975750446. train batch time cost=0.09459114074707031s\n",
            "completed batch 96 of epoch 133. loss=0.01851808652281761. train batch time cost=0.0955343246459961s\n",
            "completed batch 97 of epoch 133. loss=0.004932801239192486. train batch time cost=0.0953073501586914s\n",
            "completed batch 98 of epoch 133. loss=0.008037504740059376. train batch time cost=0.0952596664428711s\n",
            "completed batch 99 of epoch 133. loss=0.0015892382944002748. train batch time cost=0.09623384475708008s\n",
            "completed batch 100 of epoch 133. loss=0.0022711707279086113. train batch time cost=0.10130715370178223s\n",
            "completed batch 101 of epoch 133. loss=0.03523419424891472. train batch time cost=0.10086822509765625s\n",
            "completed batch 102 of epoch 133. loss=0.0652753934264183. train batch time cost=0.10150718688964844s\n",
            "completed batch 103 of epoch 133. loss=0.005122218746691942. train batch time cost=0.11402106285095215s\n",
            "completed batch 104 of epoch 133. loss=0.002014514524489641. train batch time cost=0.10173249244689941s\n",
            "completed batch 105 of epoch 133. loss=0.017161106690764427. train batch time cost=0.09608125686645508s\n",
            "completed batch 106 of epoch 133. loss=0.030341744422912598. train batch time cost=0.1027989387512207s\n",
            "completed batch 107 of epoch 133. loss=0.006035985890775919. train batch time cost=0.10288715362548828s\n",
            "completed batch 108 of epoch 133. loss=0.01112760417163372. train batch time cost=0.10180306434631348s\n",
            "completed batch 109 of epoch 133. loss=0.009874608367681503. train batch time cost=0.09867620468139648s\n",
            "completed batch 110 of epoch 133. loss=0.01347125880420208. train batch time cost=0.1017141342163086s\n",
            "completed batch 111 of epoch 133. loss=0.056163959205150604. train batch time cost=0.10166144371032715s\n",
            "completed batch 112 of epoch 133. loss=0.03250182047486305. train batch time cost=0.10260200500488281s\n",
            "completed batch 113 of epoch 133. loss=0.013442041352391243. train batch time cost=0.10167336463928223s\n",
            "completed batch 114 of epoch 133. loss=0.028571205213665962. train batch time cost=0.10167908668518066s\n",
            "completed batch 115 of epoch 133. loss=0.014622671529650688. train batch time cost=0.10351037979125977s\n",
            "completed batch 116 of epoch 133. loss=0.007147530093789101. train batch time cost=0.10429263114929199s\n",
            "completed batch 117 of epoch 133. loss=0.003600792493671179. train batch time cost=0.10298848152160645s\n",
            "completed batch 118 of epoch 133. loss=0.1031496450304985. train batch time cost=0.10206937789916992s\n",
            "completed batch 119 of epoch 133. loss=0.017765309661626816. train batch time cost=0.10218000411987305s\n",
            "completed batch 120 of epoch 133. loss=0.05071636661887169. train batch time cost=0.1018681526184082s\n",
            "completed batch 121 of epoch 133. loss=0.04614570736885071. train batch time cost=0.10108256340026855s\n",
            "completed batch 122 of epoch 133. loss=0.0026046421844512224. train batch time cost=0.10069799423217773s\n",
            "completed batch 123 of epoch 133. loss=0.008604493923485279. train batch time cost=0.10199117660522461s\n",
            "completed batch 124 of epoch 133. loss=0.008108628913760185. train batch time cost=0.10137104988098145s\n",
            "completed batch 125 of epoch 133. loss=0.002635285025462508. train batch time cost=0.10220742225646973s\n",
            "completed batch 126 of epoch 133. loss=0.001781066064722836. train batch time cost=0.029987335205078125s\n",
            "completed test of epoch 133. loss=0.001781066064722836. accuracy=0.7338991512730904. train one epoch time cost=27.30356788635254s, test validation time cost=3.7967727184295654\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904]\n",
            "completed batch 1 of epoch 134. loss=0.040842823684215546. train batch time cost=0.09651827812194824s\n",
            "completed batch 2 of epoch 134. loss=0.04067083075642586. train batch time cost=0.09876561164855957s\n",
            "completed batch 3 of epoch 134. loss=0.04256414622068405. train batch time cost=0.09486103057861328s\n",
            "completed batch 4 of epoch 134. loss=0.02649296633899212. train batch time cost=0.09882283210754395s\n",
            "completed batch 5 of epoch 134. loss=0.018857654184103012. train batch time cost=0.09573936462402344s\n",
            "completed batch 6 of epoch 134. loss=0.15346172451972961. train batch time cost=0.0948939323425293s\n",
            "completed batch 7 of epoch 134. loss=0.02430810034275055. train batch time cost=0.0958406925201416s\n",
            "completed batch 8 of epoch 134. loss=0.006824851036071777. train batch time cost=0.0937187671661377s\n",
            "completed batch 9 of epoch 134. loss=0.10172279924154282. train batch time cost=0.09477066993713379s\n",
            "completed batch 10 of epoch 134. loss=0.0034264593850821257. train batch time cost=0.09407854080200195s\n",
            "completed batch 11 of epoch 134. loss=0.015601788647472858. train batch time cost=0.09470152854919434s\n",
            "completed batch 12 of epoch 134. loss=0.012349371798336506. train batch time cost=0.09432864189147949s\n",
            "completed batch 13 of epoch 134. loss=0.0220294538885355. train batch time cost=0.09451556205749512s\n",
            "completed batch 14 of epoch 134. loss=0.137916699051857. train batch time cost=0.09446835517883301s\n",
            "completed batch 15 of epoch 134. loss=0.14189231395721436. train batch time cost=0.09472179412841797s\n",
            "completed batch 16 of epoch 134. loss=0.03194006532430649. train batch time cost=0.09381818771362305s\n",
            "completed batch 17 of epoch 134. loss=0.014001980423927307. train batch time cost=0.09353017807006836s\n",
            "completed batch 18 of epoch 134. loss=0.00910161342471838. train batch time cost=0.09388589859008789s\n",
            "completed batch 19 of epoch 134. loss=0.02740820124745369. train batch time cost=0.09531211853027344s\n",
            "completed batch 20 of epoch 134. loss=0.004474744666367769. train batch time cost=0.09981060028076172s\n",
            "completed batch 21 of epoch 134. loss=0.01136347558349371. train batch time cost=0.10624146461486816s\n",
            "completed batch 22 of epoch 134. loss=0.02925921604037285. train batch time cost=0.10338330268859863s\n",
            "completed batch 23 of epoch 134. loss=0.014023205265402794. train batch time cost=0.10162568092346191s\n",
            "completed batch 24 of epoch 134. loss=0.1044764369726181. train batch time cost=0.1014108657836914s\n",
            "completed batch 25 of epoch 134. loss=0.028124280273914337. train batch time cost=0.09968924522399902s\n",
            "completed batch 26 of epoch 134. loss=0.007959880866110325. train batch time cost=0.10114097595214844s\n",
            "completed batch 27 of epoch 134. loss=0.016169482842087746. train batch time cost=0.10494160652160645s\n",
            "completed batch 28 of epoch 134. loss=0.011565613560378551. train batch time cost=0.10187935829162598s\n",
            "completed batch 29 of epoch 134. loss=0.052763212472200394. train batch time cost=0.10292840003967285s\n",
            "completed batch 30 of epoch 134. loss=0.0026169009506702423. train batch time cost=0.10895800590515137s\n",
            "completed batch 31 of epoch 134. loss=0.04635658860206604. train batch time cost=0.10124850273132324s\n",
            "completed batch 32 of epoch 134. loss=0.009990145452320576. train batch time cost=0.10207104682922363s\n",
            "completed batch 33 of epoch 134. loss=0.02388382889330387. train batch time cost=0.10176706314086914s\n",
            "completed batch 34 of epoch 134. loss=0.03230736032128334. train batch time cost=0.10143589973449707s\n",
            "completed batch 35 of epoch 134. loss=0.01624717377126217. train batch time cost=0.1008148193359375s\n",
            "completed batch 36 of epoch 134. loss=0.008626188151538372. train batch time cost=0.10079002380371094s\n",
            "completed batch 37 of epoch 134. loss=0.009911462664604187. train batch time cost=0.1013641357421875s\n",
            "completed batch 38 of epoch 134. loss=0.011217600665986538. train batch time cost=0.10253691673278809s\n",
            "completed batch 39 of epoch 134. loss=0.02730313315987587. train batch time cost=0.10233211517333984s\n",
            "completed batch 40 of epoch 134. loss=0.010475107468664646. train batch time cost=0.1004188060760498s\n",
            "completed batch 41 of epoch 134. loss=0.011610591784119606. train batch time cost=0.10101127624511719s\n",
            "completed batch 42 of epoch 134. loss=0.03403584659099579. train batch time cost=0.10114336013793945s\n",
            "completed batch 43 of epoch 134. loss=0.008922153152525425. train batch time cost=0.10260796546936035s\n",
            "completed batch 44 of epoch 134. loss=0.009850375354290009. train batch time cost=0.1019597053527832s\n",
            "completed batch 45 of epoch 134. loss=0.001508535584434867. train batch time cost=0.10144853591918945s\n",
            "completed batch 46 of epoch 134. loss=0.09835971891880035. train batch time cost=0.10219669342041016s\n",
            "completed batch 47 of epoch 134. loss=0.027971897274255753. train batch time cost=0.09692025184631348s\n",
            "completed batch 48 of epoch 134. loss=0.013616413809359074. train batch time cost=0.09618735313415527s\n",
            "completed batch 49 of epoch 134. loss=0.04125628247857094. train batch time cost=0.1026160717010498s\n",
            "completed batch 50 of epoch 134. loss=0.027667701244354248. train batch time cost=0.09553241729736328s\n",
            "completed batch 51 of epoch 134. loss=0.009273315779864788. train batch time cost=0.09600663185119629s\n",
            "completed batch 52 of epoch 134. loss=0.0032167895697057247. train batch time cost=0.09593844413757324s\n",
            "completed batch 53 of epoch 134. loss=0.04153887927532196. train batch time cost=0.09503769874572754s\n",
            "completed batch 54 of epoch 134. loss=0.01858261041343212. train batch time cost=0.09617090225219727s\n",
            "completed batch 55 of epoch 134. loss=0.0352323055267334. train batch time cost=0.09565019607543945s\n",
            "completed batch 56 of epoch 134. loss=0.017330273985862732. train batch time cost=0.09560823440551758s\n",
            "completed batch 57 of epoch 134. loss=0.1035524383187294. train batch time cost=0.09542036056518555s\n",
            "completed batch 58 of epoch 134. loss=0.04190707579255104. train batch time cost=0.09497976303100586s\n",
            "completed batch 59 of epoch 134. loss=0.0022874793503433466. train batch time cost=0.09516620635986328s\n",
            "completed batch 60 of epoch 134. loss=0.012928269803524017. train batch time cost=0.09443902969360352s\n",
            "completed batch 61 of epoch 134. loss=0.01570320315659046. train batch time cost=0.09612870216369629s\n",
            "completed batch 62 of epoch 134. loss=0.024666547775268555. train batch time cost=0.09578633308410645s\n",
            "completed batch 63 of epoch 134. loss=0.005244105122983456. train batch time cost=0.09458398818969727s\n",
            "completed batch 64 of epoch 134. loss=0.008081575855612755. train batch time cost=0.09501791000366211s\n",
            "completed batch 65 of epoch 134. loss=0.00979946181178093. train batch time cost=0.09437322616577148s\n",
            "completed batch 66 of epoch 134. loss=0.03801713138818741. train batch time cost=0.0965261459350586s\n",
            "completed batch 67 of epoch 134. loss=0.1358567476272583. train batch time cost=0.0952448844909668s\n",
            "completed batch 68 of epoch 134. loss=0.00486011803150177. train batch time cost=0.09513235092163086s\n",
            "completed batch 69 of epoch 134. loss=0.022610262036323547. train batch time cost=0.09572982788085938s\n",
            "completed batch 70 of epoch 134. loss=0.005400174763053656. train batch time cost=0.09666562080383301s\n",
            "completed batch 71 of epoch 134. loss=0.016851864755153656. train batch time cost=0.09502148628234863s\n",
            "completed batch 72 of epoch 134. loss=0.003838661126792431. train batch time cost=0.09578847885131836s\n",
            "completed batch 73 of epoch 134. loss=0.01416196022182703. train batch time cost=0.09677577018737793s\n",
            "completed batch 74 of epoch 134. loss=0.012674189172685146. train batch time cost=0.09422588348388672s\n",
            "completed batch 75 of epoch 134. loss=0.04079603776335716. train batch time cost=0.09595775604248047s\n",
            "completed batch 76 of epoch 134. loss=0.011659974232316017. train batch time cost=0.09752488136291504s\n",
            "completed batch 77 of epoch 134. loss=0.005786179099231958. train batch time cost=0.10046005249023438s\n",
            "completed batch 78 of epoch 134. loss=0.02360333874821663. train batch time cost=0.09984469413757324s\n",
            "completed batch 79 of epoch 134. loss=0.0070415581576526165. train batch time cost=0.10348320007324219s\n",
            "completed batch 80 of epoch 134. loss=0.05680784210562706. train batch time cost=0.1027677059173584s\n",
            "completed batch 81 of epoch 134. loss=0.009183447808027267. train batch time cost=0.10509800910949707s\n",
            "completed batch 82 of epoch 134. loss=0.022153666242957115. train batch time cost=0.10042548179626465s\n",
            "completed batch 83 of epoch 134. loss=0.006715294439345598. train batch time cost=0.10027766227722168s\n",
            "completed batch 84 of epoch 134. loss=0.0209291260689497. train batch time cost=0.10230302810668945s\n",
            "completed batch 85 of epoch 134. loss=0.0033437591046094894. train batch time cost=0.10335421562194824s\n",
            "completed batch 86 of epoch 134. loss=0.014271802268922329. train batch time cost=0.10139656066894531s\n",
            "completed batch 87 of epoch 134. loss=0.03229200094938278. train batch time cost=0.10152745246887207s\n",
            "completed batch 88 of epoch 134. loss=0.005666804499924183. train batch time cost=0.10252189636230469s\n",
            "completed batch 89 of epoch 134. loss=0.05253790691494942. train batch time cost=0.1018526554107666s\n",
            "completed batch 90 of epoch 134. loss=0.017445405945181847. train batch time cost=0.10062384605407715s\n",
            "completed batch 91 of epoch 134. loss=0.037360336631536484. train batch time cost=0.10109996795654297s\n",
            "completed batch 92 of epoch 134. loss=0.011336746625602245. train batch time cost=0.1018836498260498s\n",
            "completed batch 93 of epoch 134. loss=0.027900589630007744. train batch time cost=0.10284018516540527s\n",
            "completed batch 94 of epoch 134. loss=0.011670890264213085. train batch time cost=0.10121393203735352s\n",
            "completed batch 95 of epoch 134. loss=0.00500708119943738. train batch time cost=0.10031247138977051s\n",
            "completed batch 96 of epoch 134. loss=0.02745812200009823. train batch time cost=0.10997509956359863s\n",
            "completed batch 97 of epoch 134. loss=0.018507761880755424. train batch time cost=0.10242652893066406s\n",
            "completed batch 98 of epoch 134. loss=0.019275156781077385. train batch time cost=0.10158753395080566s\n",
            "completed batch 99 of epoch 134. loss=0.013725749216973782. train batch time cost=0.10170412063598633s\n",
            "completed batch 100 of epoch 134. loss=0.012810023501515388. train batch time cost=0.1009376049041748s\n",
            "completed batch 101 of epoch 134. loss=0.008179486729204655. train batch time cost=0.10130572319030762s\n",
            "completed batch 102 of epoch 134. loss=0.012243472971022129. train batch time cost=0.10062122344970703s\n",
            "completed batch 103 of epoch 134. loss=0.002425879007205367. train batch time cost=0.10254120826721191s\n",
            "completed batch 104 of epoch 134. loss=0.056418757885694504. train batch time cost=0.10081720352172852s\n",
            "completed batch 105 of epoch 134. loss=0.017649846151471138. train batch time cost=0.09982419013977051s\n",
            "completed batch 106 of epoch 134. loss=0.01453655306249857. train batch time cost=0.10053038597106934s\n",
            "completed batch 107 of epoch 134. loss=0.031167851760983467. train batch time cost=0.10118389129638672s\n",
            "completed batch 108 of epoch 134. loss=0.004392034374177456. train batch time cost=0.10065531730651855s\n",
            "completed batch 109 of epoch 134. loss=0.00718900840729475. train batch time cost=0.10010194778442383s\n",
            "completed batch 110 of epoch 134. loss=0.0038275092374533415. train batch time cost=0.1001427173614502s\n",
            "completed batch 111 of epoch 134. loss=0.005996724124997854. train batch time cost=0.10250639915466309s\n",
            "completed batch 112 of epoch 134. loss=0.05234627425670624. train batch time cost=0.10291123390197754s\n",
            "completed batch 113 of epoch 134. loss=0.0012585034128278494. train batch time cost=0.10230517387390137s\n",
            "completed batch 114 of epoch 134. loss=0.006816585548222065. train batch time cost=0.10133767127990723s\n",
            "completed batch 115 of epoch 134. loss=0.009846650063991547. train batch time cost=0.10159158706665039s\n",
            "completed batch 116 of epoch 134. loss=0.05588820204138756. train batch time cost=0.10154414176940918s\n",
            "completed batch 117 of epoch 134. loss=0.022494236007332802. train batch time cost=0.10125112533569336s\n",
            "completed batch 118 of epoch 134. loss=0.09681321680545807. train batch time cost=0.10158801078796387s\n",
            "completed batch 119 of epoch 134. loss=0.018220245838165283. train batch time cost=0.10163593292236328s\n",
            "completed batch 120 of epoch 134. loss=0.0252490546554327. train batch time cost=0.10202288627624512s\n",
            "completed batch 121 of epoch 134. loss=0.019664134830236435. train batch time cost=0.10256409645080566s\n",
            "completed batch 122 of epoch 134. loss=0.0011764445807784796. train batch time cost=0.10244441032409668s\n",
            "completed batch 123 of epoch 134. loss=0.07027085870504379. train batch time cost=0.10172057151794434s\n",
            "completed batch 124 of epoch 134. loss=0.004460877738893032. train batch time cost=0.10123586654663086s\n",
            "completed batch 125 of epoch 134. loss=0.007909242995083332. train batch time cost=0.10030984878540039s\n",
            "completed batch 126 of epoch 134. loss=0.0006593777798116207. train batch time cost=0.02908635139465332s\n",
            "completed test of epoch 134. loss=0.0006593777798116207. accuracy=0.7149276085871193. train one epoch time cost=27.346912622451782s, test validation time cost=3.885852336883545\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193]\n",
            "completed batch 1 of epoch 135. loss=0.029057059437036514. train batch time cost=0.10299181938171387s\n",
            "completed batch 2 of epoch 135. loss=0.08017498254776001. train batch time cost=0.10195755958557129s\n",
            "completed batch 3 of epoch 135. loss=0.032535120844841. train batch time cost=0.1013338565826416s\n",
            "completed batch 4 of epoch 135. loss=0.0099235400557518. train batch time cost=0.10231709480285645s\n",
            "completed batch 5 of epoch 135. loss=0.019754981622099876. train batch time cost=0.09599947929382324s\n",
            "completed batch 6 of epoch 135. loss=0.023927070200443268. train batch time cost=0.1013801097869873s\n",
            "completed batch 7 of epoch 135. loss=0.046805478632450104. train batch time cost=0.09979605674743652s\n",
            "completed batch 8 of epoch 135. loss=0.025205984711647034. train batch time cost=0.09962582588195801s\n",
            "completed batch 9 of epoch 135. loss=0.017796888947486877. train batch time cost=0.10087895393371582s\n",
            "completed batch 10 of epoch 135. loss=0.0016553119057789445. train batch time cost=0.10007619857788086s\n",
            "completed batch 11 of epoch 135. loss=0.048417288810014725. train batch time cost=0.10100626945495605s\n",
            "completed batch 12 of epoch 135. loss=0.0052716718055307865. train batch time cost=0.10088086128234863s\n",
            "completed batch 13 of epoch 135. loss=0.0069279479794204235. train batch time cost=0.09997797012329102s\n",
            "completed batch 14 of epoch 135. loss=0.00897569116204977. train batch time cost=0.10050797462463379s\n",
            "completed batch 15 of epoch 135. loss=0.06531474739313126. train batch time cost=0.10176372528076172s\n",
            "completed batch 16 of epoch 135. loss=0.005986614618450403. train batch time cost=0.10053133964538574s\n",
            "completed batch 17 of epoch 135. loss=0.03807024285197258. train batch time cost=0.09958767890930176s\n",
            "completed batch 18 of epoch 135. loss=0.016472717747092247. train batch time cost=0.10152840614318848s\n",
            "completed batch 19 of epoch 135. loss=0.011311173439025879. train batch time cost=0.10286355018615723s\n",
            "completed batch 20 of epoch 135. loss=0.0031127259135246277. train batch time cost=0.10396671295166016s\n",
            "completed batch 21 of epoch 135. loss=0.0024216831661760807. train batch time cost=0.1016073226928711s\n",
            "completed batch 22 of epoch 135. loss=0.0357871912419796. train batch time cost=0.10194683074951172s\n",
            "completed batch 23 of epoch 135. loss=0.007806737441569567. train batch time cost=0.10298681259155273s\n",
            "completed batch 24 of epoch 135. loss=0.04488424211740494. train batch time cost=0.10354876518249512s\n",
            "completed batch 25 of epoch 135. loss=0.006628368981182575. train batch time cost=0.10238242149353027s\n",
            "completed batch 26 of epoch 135. loss=0.013226817362010479. train batch time cost=0.10243964195251465s\n",
            "completed batch 27 of epoch 135. loss=0.019214382395148277. train batch time cost=0.10191869735717773s\n",
            "completed batch 28 of epoch 135. loss=0.10451377928256989. train batch time cost=0.10207056999206543s\n",
            "completed batch 29 of epoch 135. loss=0.06953824311494827. train batch time cost=0.10208725929260254s\n",
            "completed batch 30 of epoch 135. loss=0.0038284664042294025. train batch time cost=0.10232281684875488s\n",
            "completed batch 31 of epoch 135. loss=0.06478022783994675. train batch time cost=0.10144615173339844s\n",
            "completed batch 32 of epoch 135. loss=0.07268381863832474. train batch time cost=0.10255670547485352s\n",
            "completed batch 33 of epoch 135. loss=0.07931448519229889. train batch time cost=0.1027987003326416s\n",
            "completed batch 34 of epoch 135. loss=0.09990578144788742. train batch time cost=0.10130858421325684s\n",
            "completed batch 35 of epoch 135. loss=0.011767474003136158. train batch time cost=0.10143852233886719s\n",
            "completed batch 36 of epoch 135. loss=0.01663912832736969. train batch time cost=0.10033655166625977s\n",
            "completed batch 37 of epoch 135. loss=0.01986517757177353. train batch time cost=0.10734105110168457s\n",
            "completed batch 38 of epoch 135. loss=0.04421152547001839. train batch time cost=0.10185432434082031s\n",
            "completed batch 39 of epoch 135. loss=0.024107810109853745. train batch time cost=0.1010134220123291s\n",
            "completed batch 40 of epoch 135. loss=0.02107883431017399. train batch time cost=0.1026926040649414s\n",
            "completed batch 41 of epoch 135. loss=0.012741045095026493. train batch time cost=0.1020653247833252s\n",
            "completed batch 42 of epoch 135. loss=0.00305568496696651. train batch time cost=0.1020040512084961s\n",
            "completed batch 43 of epoch 135. loss=0.0024164854548871517. train batch time cost=0.10414385795593262s\n",
            "completed batch 44 of epoch 135. loss=0.004227049183100462. train batch time cost=0.10064387321472168s\n",
            "completed batch 45 of epoch 135. loss=0.07562834024429321. train batch time cost=0.10158324241638184s\n",
            "completed batch 46 of epoch 135. loss=0.0037327937316149473. train batch time cost=0.1027677059173584s\n",
            "completed batch 47 of epoch 135. loss=0.05545633286237717. train batch time cost=0.10338234901428223s\n",
            "completed batch 48 of epoch 135. loss=0.20589110255241394. train batch time cost=0.10361385345458984s\n",
            "completed batch 49 of epoch 135. loss=0.3464202582836151. train batch time cost=0.1020956039428711s\n",
            "completed batch 50 of epoch 135. loss=0.06188351660966873. train batch time cost=0.10276246070861816s\n",
            "completed batch 51 of epoch 135. loss=0.020309044048190117. train batch time cost=0.10394048690795898s\n",
            "completed batch 52 of epoch 135. loss=0.09032347053289413. train batch time cost=0.10179352760314941s\n",
            "completed batch 53 of epoch 135. loss=0.06689231097698212. train batch time cost=0.10047006607055664s\n",
            "completed batch 54 of epoch 135. loss=0.2039831578731537. train batch time cost=0.10350966453552246s\n",
            "completed batch 55 of epoch 135. loss=0.05227609723806381. train batch time cost=0.10453462600708008s\n",
            "completed batch 56 of epoch 135. loss=0.16435730457305908. train batch time cost=0.10228180885314941s\n",
            "completed batch 57 of epoch 135. loss=0.05392646789550781. train batch time cost=0.10241436958312988s\n",
            "completed batch 58 of epoch 135. loss=0.12910586595535278. train batch time cost=0.10156702995300293s\n",
            "completed batch 59 of epoch 135. loss=0.08000077307224274. train batch time cost=0.10301542282104492s\n",
            "completed batch 60 of epoch 135. loss=0.055875472724437714. train batch time cost=0.10360026359558105s\n",
            "completed batch 61 of epoch 135. loss=0.06524910032749176. train batch time cost=0.10209488868713379s\n",
            "completed batch 62 of epoch 135. loss=0.11027971655130386. train batch time cost=0.1015172004699707s\n",
            "completed batch 63 of epoch 135. loss=0.2671179175376892. train batch time cost=0.10150861740112305s\n",
            "completed batch 64 of epoch 135. loss=0.12499929964542389. train batch time cost=0.10270500183105469s\n",
            "completed batch 65 of epoch 135. loss=0.062469758093357086. train batch time cost=0.10631275177001953s\n",
            "completed batch 66 of epoch 135. loss=0.1412471979856491. train batch time cost=0.1011495590209961s\n",
            "completed batch 67 of epoch 135. loss=0.04620245099067688. train batch time cost=0.10191512107849121s\n",
            "completed batch 68 of epoch 135. loss=0.1227293610572815. train batch time cost=0.10375547409057617s\n",
            "completed batch 69 of epoch 135. loss=0.024000471457839012. train batch time cost=0.10653495788574219s\n",
            "completed batch 70 of epoch 135. loss=0.049019716680049896. train batch time cost=0.10214638710021973s\n",
            "completed batch 71 of epoch 135. loss=0.07215414196252823. train batch time cost=0.10161733627319336s\n",
            "completed batch 72 of epoch 135. loss=0.018492186442017555. train batch time cost=0.1019136905670166s\n",
            "completed batch 73 of epoch 135. loss=0.07905227690935135. train batch time cost=0.1025853157043457s\n",
            "completed batch 74 of epoch 135. loss=0.14896081387996674. train batch time cost=0.10180068016052246s\n",
            "completed batch 75 of epoch 135. loss=0.11058946698904037. train batch time cost=0.10152530670166016s\n",
            "completed batch 76 of epoch 135. loss=0.15918676555156708. train batch time cost=0.10184049606323242s\n",
            "completed batch 77 of epoch 135. loss=0.142544224858284. train batch time cost=0.09521150588989258s\n",
            "completed batch 78 of epoch 135. loss=0.05730327591300011. train batch time cost=0.09537386894226074s\n",
            "completed batch 79 of epoch 135. loss=0.23081150650978088. train batch time cost=0.10046124458312988s\n",
            "completed batch 80 of epoch 135. loss=0.1601305603981018. train batch time cost=0.10182785987854004s\n",
            "completed batch 81 of epoch 135. loss=0.22612062096595764. train batch time cost=0.10057210922241211s\n",
            "completed batch 82 of epoch 135. loss=0.0937088131904602. train batch time cost=0.10314798355102539s\n",
            "completed batch 83 of epoch 135. loss=0.02678760141134262. train batch time cost=0.11145186424255371s\n",
            "completed batch 84 of epoch 135. loss=0.03272451087832451. train batch time cost=0.10100340843200684s\n",
            "completed batch 85 of epoch 135. loss=0.08376312255859375. train batch time cost=0.10052680969238281s\n",
            "completed batch 86 of epoch 135. loss=0.03079237975180149. train batch time cost=0.10128283500671387s\n",
            "completed batch 87 of epoch 135. loss=0.029076213017106056. train batch time cost=0.10122108459472656s\n",
            "completed batch 88 of epoch 135. loss=0.024745432659983635. train batch time cost=0.1030416488647461s\n",
            "completed batch 89 of epoch 135. loss=0.0496317520737648. train batch time cost=0.10183954238891602s\n",
            "completed batch 90 of epoch 135. loss=0.09266914427280426. train batch time cost=0.10073971748352051s\n",
            "completed batch 91 of epoch 135. loss=0.08718261867761612. train batch time cost=0.10127091407775879s\n",
            "completed batch 92 of epoch 135. loss=0.04933585971593857. train batch time cost=0.1019439697265625s\n",
            "completed batch 93 of epoch 135. loss=0.04350215196609497. train batch time cost=0.10228633880615234s\n",
            "completed batch 94 of epoch 135. loss=0.08861766755580902. train batch time cost=0.10116767883300781s\n",
            "completed batch 95 of epoch 135. loss=0.04603442922234535. train batch time cost=0.10140275955200195s\n",
            "completed batch 96 of epoch 135. loss=0.07880240678787231. train batch time cost=0.10118889808654785s\n",
            "completed batch 97 of epoch 135. loss=0.020805155858397484. train batch time cost=0.10080289840698242s\n",
            "completed batch 98 of epoch 135. loss=0.07534456253051758. train batch time cost=0.10127520561218262s\n",
            "completed batch 99 of epoch 135. loss=0.02034805528819561. train batch time cost=0.1004941463470459s\n",
            "completed batch 100 of epoch 135. loss=0.07338785380125046. train batch time cost=0.10140204429626465s\n",
            "completed batch 101 of epoch 135. loss=0.09377201646566391. train batch time cost=0.10258293151855469s\n",
            "completed batch 102 of epoch 135. loss=0.05304378271102905. train batch time cost=0.10201549530029297s\n",
            "completed batch 103 of epoch 135. loss=0.06354496628046036. train batch time cost=0.10305976867675781s\n",
            "completed batch 104 of epoch 135. loss=0.07912083715200424. train batch time cost=0.10464715957641602s\n",
            "completed batch 105 of epoch 135. loss=0.0085444962605834. train batch time cost=0.1027226448059082s\n",
            "completed batch 106 of epoch 135. loss=0.028660574927926064. train batch time cost=0.10198783874511719s\n",
            "completed batch 107 of epoch 135. loss=0.0569634810090065. train batch time cost=0.1038048267364502s\n",
            "completed batch 108 of epoch 135. loss=0.04118216037750244. train batch time cost=0.10201525688171387s\n",
            "completed batch 109 of epoch 135. loss=0.019659219309687614. train batch time cost=0.10317015647888184s\n",
            "completed batch 110 of epoch 135. loss=0.03546491265296936. train batch time cost=0.10229349136352539s\n",
            "completed batch 111 of epoch 135. loss=0.056619204580783844. train batch time cost=0.10303664207458496s\n",
            "completed batch 112 of epoch 135. loss=0.023252947255969048. train batch time cost=0.10240483283996582s\n",
            "completed batch 113 of epoch 135. loss=0.023706793785095215. train batch time cost=0.09783816337585449s\n",
            "completed batch 114 of epoch 135. loss=0.19138552248477936. train batch time cost=0.09630465507507324s\n",
            "completed batch 115 of epoch 135. loss=0.09429756551980972. train batch time cost=0.09565281867980957s\n",
            "completed batch 116 of epoch 135. loss=0.04530445858836174. train batch time cost=0.09690022468566895s\n",
            "completed batch 117 of epoch 135. loss=0.03310093283653259. train batch time cost=0.0959925651550293s\n",
            "completed batch 118 of epoch 135. loss=0.03695732727646828. train batch time cost=0.0957794189453125s\n",
            "completed batch 119 of epoch 135. loss=0.053060468286275864. train batch time cost=0.09598565101623535s\n",
            "completed batch 120 of epoch 135. loss=0.03899604082107544. train batch time cost=0.09613966941833496s\n",
            "completed batch 121 of epoch 135. loss=0.017953425645828247. train batch time cost=0.09562253952026367s\n",
            "completed batch 122 of epoch 135. loss=0.020870784297585487. train batch time cost=0.09511375427246094s\n",
            "completed batch 123 of epoch 135. loss=0.10246976464986801. train batch time cost=0.09492945671081543s\n",
            "completed batch 124 of epoch 135. loss=0.07925385236740112. train batch time cost=0.09465765953063965s\n",
            "completed batch 125 of epoch 135. loss=0.016502462327480316. train batch time cost=0.09483027458190918s\n",
            "completed batch 126 of epoch 135. loss=0.00011851932504214346. train batch time cost=0.028453826904296875s\n",
            "completed test of epoch 135. loss=0.00011851932504214346. accuracy=0.6789815277084373. train one epoch time cost=27.59551215171814s, test validation time cost=3.775411367416382\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373]\n",
            "completed batch 1 of epoch 136. loss=0.016549788415431976. train batch time cost=0.0948019027709961s\n",
            "completed batch 2 of epoch 136. loss=0.03254309669137001. train batch time cost=0.09454679489135742s\n",
            "completed batch 3 of epoch 136. loss=0.02520264871418476. train batch time cost=0.09529900550842285s\n",
            "completed batch 4 of epoch 136. loss=0.1793963462114334. train batch time cost=0.09573006629943848s\n",
            "completed batch 5 of epoch 136. loss=0.12181196361780167. train batch time cost=0.0954444408416748s\n",
            "completed batch 6 of epoch 136. loss=0.01285577192902565. train batch time cost=0.09648966789245605s\n",
            "completed batch 7 of epoch 136. loss=0.04422309249639511. train batch time cost=0.09553956985473633s\n",
            "completed batch 8 of epoch 136. loss=0.08574476838111877. train batch time cost=0.09496045112609863s\n",
            "completed batch 9 of epoch 136. loss=0.0340232327580452. train batch time cost=0.09725427627563477s\n",
            "completed batch 10 of epoch 136. loss=0.008868381381034851. train batch time cost=0.09566712379455566s\n",
            "completed batch 11 of epoch 136. loss=0.02834405191242695. train batch time cost=0.09574317932128906s\n",
            "completed batch 12 of epoch 136. loss=0.022197246551513672. train batch time cost=0.0951530933380127s\n",
            "completed batch 13 of epoch 136. loss=0.062237031757831573. train batch time cost=0.09487485885620117s\n",
            "completed batch 14 of epoch 136. loss=0.04761582240462303. train batch time cost=0.09517765045166016s\n",
            "completed batch 15 of epoch 136. loss=0.025442153215408325. train batch time cost=0.10286068916320801s\n",
            "completed batch 16 of epoch 136. loss=0.04266800358891487. train batch time cost=0.10141611099243164s\n",
            "completed batch 17 of epoch 136. loss=0.1368803232908249. train batch time cost=0.10067439079284668s\n",
            "completed batch 18 of epoch 136. loss=0.031164389103651047. train batch time cost=0.10296106338500977s\n",
            "completed batch 19 of epoch 136. loss=0.007680918090045452. train batch time cost=0.10375475883483887s\n",
            "completed batch 20 of epoch 136. loss=0.06387782096862793. train batch time cost=0.10159468650817871s\n",
            "completed batch 21 of epoch 136. loss=0.013712087646126747. train batch time cost=0.10137414932250977s\n",
            "completed batch 22 of epoch 136. loss=0.009421199560165405. train batch time cost=0.10122203826904297s\n",
            "completed batch 23 of epoch 136. loss=0.08847794681787491. train batch time cost=0.1010289192199707s\n",
            "completed batch 24 of epoch 136. loss=0.18009109795093536. train batch time cost=0.10053133964538574s\n",
            "completed batch 25 of epoch 136. loss=0.010193168185651302. train batch time cost=0.10129737854003906s\n",
            "completed batch 26 of epoch 136. loss=0.013553301803767681. train batch time cost=0.10251712799072266s\n",
            "completed batch 27 of epoch 136. loss=0.010587038472294807. train batch time cost=0.1038978099822998s\n",
            "completed batch 28 of epoch 136. loss=0.024487685412168503. train batch time cost=0.09913969039916992s\n",
            "completed batch 29 of epoch 136. loss=0.14121371507644653. train batch time cost=0.0948326587677002s\n",
            "completed batch 30 of epoch 136. loss=0.007517195772379637. train batch time cost=0.0949406623840332s\n",
            "completed batch 31 of epoch 136. loss=0.03826942294836044. train batch time cost=0.09419727325439453s\n",
            "completed batch 32 of epoch 136. loss=0.03939813748002052. train batch time cost=0.0978994369506836s\n",
            "completed batch 33 of epoch 136. loss=0.08151762187480927. train batch time cost=0.09579157829284668s\n",
            "completed batch 34 of epoch 136. loss=0.019252095371484756. train batch time cost=0.09534573554992676s\n",
            "completed batch 35 of epoch 136. loss=0.07575488090515137. train batch time cost=0.0948038101196289s\n",
            "completed batch 36 of epoch 136. loss=0.03221045434474945. train batch time cost=0.09510421752929688s\n",
            "completed batch 37 of epoch 136. loss=0.23104223608970642. train batch time cost=0.09629940986633301s\n",
            "completed batch 38 of epoch 136. loss=0.061436448246240616. train batch time cost=0.1022334098815918s\n",
            "completed batch 39 of epoch 136. loss=0.020311301574110985. train batch time cost=0.09654927253723145s\n",
            "completed batch 40 of epoch 136. loss=0.034206535667181015. train batch time cost=0.09473705291748047s\n",
            "completed batch 41 of epoch 136. loss=0.010702192783355713. train batch time cost=0.09428644180297852s\n",
            "completed batch 42 of epoch 136. loss=0.03652030974626541. train batch time cost=0.09537410736083984s\n",
            "completed batch 43 of epoch 136. loss=0.0876854956150055. train batch time cost=0.09560012817382812s\n",
            "completed batch 44 of epoch 136. loss=0.03566906973719597. train batch time cost=0.0978858470916748s\n",
            "completed batch 45 of epoch 136. loss=0.03554700314998627. train batch time cost=0.09395003318786621s\n",
            "completed batch 46 of epoch 136. loss=0.0061669428832829. train batch time cost=0.09538578987121582s\n",
            "completed batch 47 of epoch 136. loss=0.028867267072200775. train batch time cost=0.09666299819946289s\n",
            "completed batch 48 of epoch 136. loss=0.022010959684848785. train batch time cost=0.0945594310760498s\n",
            "completed batch 49 of epoch 136. loss=0.06215551868081093. train batch time cost=0.10213565826416016s\n",
            "completed batch 50 of epoch 136. loss=0.03438308835029602. train batch time cost=0.1019599437713623s\n",
            "completed batch 51 of epoch 136. loss=0.00701122684404254. train batch time cost=0.10131096839904785s\n",
            "completed batch 52 of epoch 136. loss=0.05508746579289436. train batch time cost=0.10283827781677246s\n",
            "completed batch 53 of epoch 136. loss=0.1342484951019287. train batch time cost=0.1022176742553711s\n",
            "completed batch 54 of epoch 136. loss=0.11006364226341248. train batch time cost=0.10192370414733887s\n",
            "completed batch 55 of epoch 136. loss=0.153869166970253. train batch time cost=0.10175037384033203s\n",
            "completed batch 56 of epoch 136. loss=0.014156730845570564. train batch time cost=0.10166645050048828s\n",
            "completed batch 57 of epoch 136. loss=0.031414374709129333. train batch time cost=0.1020669937133789s\n",
            "completed batch 58 of epoch 136. loss=0.21274098753929138. train batch time cost=0.10321879386901855s\n",
            "completed batch 59 of epoch 136. loss=0.020649855956435204. train batch time cost=0.1015167236328125s\n",
            "completed batch 60 of epoch 136. loss=0.05641579627990723. train batch time cost=0.10090446472167969s\n",
            "completed batch 61 of epoch 136. loss=0.019831810146570206. train batch time cost=0.10120511054992676s\n",
            "completed batch 62 of epoch 136. loss=0.04398750886321068. train batch time cost=0.10063552856445312s\n",
            "completed batch 63 of epoch 136. loss=0.021098647266626358. train batch time cost=0.10212564468383789s\n",
            "completed batch 64 of epoch 136. loss=0.006612375378608704. train batch time cost=0.1008443832397461s\n",
            "completed batch 65 of epoch 136. loss=0.0037101232446730137. train batch time cost=0.10167813301086426s\n",
            "completed batch 66 of epoch 136. loss=0.11914068460464478. train batch time cost=0.10241055488586426s\n",
            "completed batch 67 of epoch 136. loss=0.018168993294239044. train batch time cost=0.10086464881896973s\n",
            "completed batch 68 of epoch 136. loss=0.003812293289229274. train batch time cost=0.10317063331604004s\n",
            "completed batch 69 of epoch 136. loss=0.05263565853238106. train batch time cost=0.10140633583068848s\n",
            "completed batch 70 of epoch 136. loss=0.06484459340572357. train batch time cost=0.1013636589050293s\n",
            "completed batch 71 of epoch 136. loss=0.025050759315490723. train batch time cost=0.10264706611633301s\n",
            "completed batch 72 of epoch 136. loss=0.058401692658662796. train batch time cost=0.10226202011108398s\n",
            "completed batch 73 of epoch 136. loss=0.027737772092223167. train batch time cost=0.10172367095947266s\n",
            "completed batch 74 of epoch 136. loss=0.08495723456144333. train batch time cost=0.10341429710388184s\n",
            "completed batch 75 of epoch 136. loss=0.02643919736146927. train batch time cost=0.1026761531829834s\n",
            "completed batch 76 of epoch 136. loss=0.13343527913093567. train batch time cost=0.10164594650268555s\n",
            "completed batch 77 of epoch 136. loss=0.07606983929872513. train batch time cost=0.10195326805114746s\n",
            "completed batch 78 of epoch 136. loss=0.03984897583723068. train batch time cost=0.10177373886108398s\n",
            "completed batch 79 of epoch 136. loss=0.008224510587751865. train batch time cost=0.10139155387878418s\n",
            "completed batch 80 of epoch 136. loss=0.011377309449017048. train batch time cost=0.10427021980285645s\n",
            "completed batch 81 of epoch 136. loss=0.006217387039214373. train batch time cost=0.10011577606201172s\n",
            "completed batch 82 of epoch 136. loss=0.03673340380191803. train batch time cost=0.09841108322143555s\n",
            "completed batch 83 of epoch 136. loss=0.0603875033557415. train batch time cost=0.1042623519897461s\n",
            "completed batch 84 of epoch 136. loss=0.03606081008911133. train batch time cost=0.10218191146850586s\n",
            "completed batch 85 of epoch 136. loss=0.02133067324757576. train batch time cost=0.10117721557617188s\n",
            "completed batch 86 of epoch 136. loss=0.27562227845191956. train batch time cost=0.10020327568054199s\n",
            "completed batch 87 of epoch 136. loss=0.01638125441968441. train batch time cost=0.10076904296875s\n",
            "completed batch 88 of epoch 136. loss=0.03531716391444206. train batch time cost=0.10056352615356445s\n",
            "completed batch 89 of epoch 136. loss=0.009391392581164837. train batch time cost=0.10084366798400879s\n",
            "completed batch 90 of epoch 136. loss=0.07325795292854309. train batch time cost=0.09518623352050781s\n",
            "completed batch 91 of epoch 136. loss=0.019968923181295395. train batch time cost=0.0956881046295166s\n",
            "completed batch 92 of epoch 136. loss=0.03232228755950928. train batch time cost=0.09698081016540527s\n",
            "completed batch 93 of epoch 136. loss=0.08897951990365982. train batch time cost=0.09542250633239746s\n",
            "completed batch 94 of epoch 136. loss=0.080983966588974. train batch time cost=0.09597945213317871s\n",
            "completed batch 95 of epoch 136. loss=0.005881760735064745. train batch time cost=0.09489846229553223s\n",
            "completed batch 96 of epoch 136. loss=0.04207471385598183. train batch time cost=0.09512877464294434s\n",
            "completed batch 97 of epoch 136. loss=0.07478979974985123. train batch time cost=0.09614753723144531s\n",
            "completed batch 98 of epoch 136. loss=0.027355311438441277. train batch time cost=0.09578800201416016s\n",
            "completed batch 99 of epoch 136. loss=0.021023740991950035. train batch time cost=0.09602713584899902s\n",
            "completed batch 100 of epoch 136. loss=0.03683780878782272. train batch time cost=0.09650015830993652s\n",
            "completed batch 101 of epoch 136. loss=0.17331448197364807. train batch time cost=0.09465146064758301s\n",
            "completed batch 102 of epoch 136. loss=0.07296518236398697. train batch time cost=0.09701228141784668s\n",
            "completed batch 103 of epoch 136. loss=0.04705538973212242. train batch time cost=0.09647512435913086s\n",
            "completed batch 104 of epoch 136. loss=0.0879296287894249. train batch time cost=0.09578514099121094s\n",
            "completed batch 105 of epoch 136. loss=0.017290718853473663. train batch time cost=0.09489774703979492s\n",
            "completed batch 106 of epoch 136. loss=0.027983345091342926. train batch time cost=0.09512805938720703s\n",
            "completed batch 107 of epoch 136. loss=0.0151884276419878. train batch time cost=0.09672999382019043s\n",
            "completed batch 108 of epoch 136. loss=0.06388557702302933. train batch time cost=0.09525299072265625s\n",
            "completed batch 109 of epoch 136. loss=0.01856502704322338. train batch time cost=0.09454011917114258s\n",
            "completed batch 110 of epoch 136. loss=0.09888139367103577. train batch time cost=0.09495162963867188s\n",
            "completed batch 111 of epoch 136. loss=0.036408066749572754. train batch time cost=0.09518265724182129s\n",
            "completed batch 112 of epoch 136. loss=0.05264893174171448. train batch time cost=0.09662437438964844s\n",
            "completed batch 113 of epoch 136. loss=0.010396911762654781. train batch time cost=0.09536361694335938s\n",
            "completed batch 114 of epoch 136. loss=0.009321901015937328. train batch time cost=0.10200786590576172s\n",
            "completed batch 115 of epoch 136. loss=0.047692690044641495. train batch time cost=0.10130071640014648s\n",
            "completed batch 116 of epoch 136. loss=0.01672978885471821. train batch time cost=0.10115170478820801s\n",
            "completed batch 117 of epoch 136. loss=0.07398227602243423. train batch time cost=0.10196995735168457s\n",
            "completed batch 118 of epoch 136. loss=0.02146613970398903. train batch time cost=0.10184359550476074s\n",
            "completed batch 119 of epoch 136. loss=0.023033618927001953. train batch time cost=0.10758018493652344s\n",
            "completed batch 120 of epoch 136. loss=0.04470096156001091. train batch time cost=0.10136008262634277s\n",
            "completed batch 121 of epoch 136. loss=0.054361943155527115. train batch time cost=0.10185432434082031s\n",
            "completed batch 122 of epoch 136. loss=0.06233133375644684. train batch time cost=0.10230255126953125s\n",
            "completed batch 123 of epoch 136. loss=0.012417701072990894. train batch time cost=0.1010129451751709s\n",
            "completed batch 124 of epoch 136. loss=0.006365660112351179. train batch time cost=0.10088849067687988s\n",
            "completed batch 125 of epoch 136. loss=0.013469800353050232. train batch time cost=0.10129547119140625s\n",
            "completed batch 126 of epoch 136. loss=0.001624826923944056. train batch time cost=0.029958486557006836s\n",
            "completed test of epoch 136. loss=0.001624826923944056. accuracy=0.6290564153769346. train one epoch time cost=27.323720932006836s, test validation time cost=3.9816784858703613\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346]\n",
            "completed batch 1 of epoch 137. loss=0.01824585162103176. train batch time cost=0.10304450988769531s\n",
            "completed batch 2 of epoch 137. loss=0.04104229807853699. train batch time cost=0.1026010513305664s\n",
            "completed batch 3 of epoch 137. loss=0.0680718943476677. train batch time cost=0.10239624977111816s\n",
            "completed batch 4 of epoch 137. loss=0.018860608339309692. train batch time cost=0.1019129753112793s\n",
            "completed batch 5 of epoch 137. loss=0.10450507700443268. train batch time cost=0.10270047187805176s\n",
            "completed batch 6 of epoch 137. loss=0.02415461838245392. train batch time cost=0.10385727882385254s\n",
            "completed batch 7 of epoch 137. loss=0.025082821026444435. train batch time cost=0.10249662399291992s\n",
            "completed batch 8 of epoch 137. loss=0.08257421106100082. train batch time cost=0.1013638973236084s\n",
            "completed batch 9 of epoch 137. loss=0.024072060361504555. train batch time cost=0.09550809860229492s\n",
            "completed batch 10 of epoch 137. loss=0.04110266640782356. train batch time cost=0.10366296768188477s\n",
            "completed batch 11 of epoch 137. loss=0.059440985321998596. train batch time cost=0.0959007740020752s\n",
            "completed batch 12 of epoch 137. loss=0.02811264991760254. train batch time cost=0.09626936912536621s\n",
            "completed batch 13 of epoch 137. loss=0.02843574807047844. train batch time cost=0.09655332565307617s\n",
            "completed batch 14 of epoch 137. loss=0.02232513576745987. train batch time cost=0.10206913948059082s\n",
            "completed batch 15 of epoch 137. loss=0.09895160049200058. train batch time cost=0.10344195365905762s\n",
            "completed batch 16 of epoch 137. loss=0.006287851836532354. train batch time cost=0.10315728187561035s\n",
            "completed batch 17 of epoch 137. loss=0.005865825340151787. train batch time cost=0.10254120826721191s\n",
            "completed batch 18 of epoch 137. loss=0.11465010046958923. train batch time cost=0.10161375999450684s\n",
            "completed batch 19 of epoch 137. loss=0.23017176985740662. train batch time cost=0.10205554962158203s\n",
            "completed batch 20 of epoch 137. loss=0.025621455162763596. train batch time cost=0.10292601585388184s\n",
            "completed batch 21 of epoch 137. loss=0.010134074836969376. train batch time cost=0.10259556770324707s\n",
            "completed batch 22 of epoch 137. loss=0.0023896326310932636. train batch time cost=0.10520601272583008s\n",
            "completed batch 23 of epoch 137. loss=0.00745291355997324. train batch time cost=0.10022139549255371s\n",
            "completed batch 24 of epoch 137. loss=0.04480435699224472. train batch time cost=0.10109257698059082s\n",
            "completed batch 25 of epoch 137. loss=0.01957838609814644. train batch time cost=0.10224795341491699s\n",
            "completed batch 26 of epoch 137. loss=0.01247381791472435. train batch time cost=0.10024738311767578s\n",
            "completed batch 27 of epoch 137. loss=0.11697890609502792. train batch time cost=0.10059571266174316s\n",
            "completed batch 28 of epoch 137. loss=0.024737648665905. train batch time cost=0.10153913497924805s\n",
            "completed batch 29 of epoch 137. loss=0.12525992095470428. train batch time cost=0.10045456886291504s\n",
            "completed batch 30 of epoch 137. loss=0.013565829023718834. train batch time cost=0.1023564338684082s\n",
            "completed batch 31 of epoch 137. loss=0.0939985066652298. train batch time cost=0.10110330581665039s\n",
            "completed batch 32 of epoch 137. loss=0.018321568146348. train batch time cost=0.10189032554626465s\n",
            "completed batch 33 of epoch 137. loss=0.011069581843912601. train batch time cost=0.10271072387695312s\n",
            "completed batch 34 of epoch 137. loss=0.07041129469871521. train batch time cost=0.10238027572631836s\n",
            "completed batch 35 of epoch 137. loss=0.06642482429742813. train batch time cost=0.1010892391204834s\n",
            "completed batch 36 of epoch 137. loss=0.04027577489614487. train batch time cost=0.10593891143798828s\n",
            "completed batch 37 of epoch 137. loss=0.03722231462597847. train batch time cost=0.10171675682067871s\n",
            "completed batch 38 of epoch 137. loss=0.01503071840852499. train batch time cost=0.10145926475524902s\n",
            "completed batch 39 of epoch 137. loss=0.05800699442625046. train batch time cost=0.10233855247497559s\n",
            "completed batch 40 of epoch 137. loss=0.008798656985163689. train batch time cost=0.1020352840423584s\n",
            "completed batch 41 of epoch 137. loss=0.07193265110254288. train batch time cost=0.10299324989318848s\n",
            "completed batch 42 of epoch 137. loss=0.06331227719783783. train batch time cost=0.10363340377807617s\n",
            "completed batch 43 of epoch 137. loss=0.007448108866810799. train batch time cost=0.1018986701965332s\n",
            "completed batch 44 of epoch 137. loss=0.02536340244114399. train batch time cost=0.10249018669128418s\n",
            "completed batch 45 of epoch 137. loss=0.1147146001458168. train batch time cost=0.10084295272827148s\n",
            "completed batch 46 of epoch 137. loss=0.042749952524900436. train batch time cost=0.10188770294189453s\n",
            "completed batch 47 of epoch 137. loss=0.027762185782194138. train batch time cost=0.1024775505065918s\n",
            "completed batch 48 of epoch 137. loss=0.037961386144161224. train batch time cost=0.1011502742767334s\n",
            "completed batch 49 of epoch 137. loss=0.013553769327700138. train batch time cost=0.10209465026855469s\n",
            "completed batch 50 of epoch 137. loss=0.015474344603717327. train batch time cost=0.10394287109375s\n",
            "completed batch 51 of epoch 137. loss=0.010062470100820065. train batch time cost=0.10309886932373047s\n",
            "completed batch 52 of epoch 137. loss=0.038288481533527374. train batch time cost=0.10187721252441406s\n",
            "completed batch 53 of epoch 137. loss=0.028096305206418037. train batch time cost=0.10208368301391602s\n",
            "completed batch 54 of epoch 137. loss=0.03320357948541641. train batch time cost=0.10239028930664062s\n",
            "completed batch 55 of epoch 137. loss=0.028802014887332916. train batch time cost=0.10185027122497559s\n",
            "completed batch 56 of epoch 137. loss=0.08828788995742798. train batch time cost=0.10518860816955566s\n",
            "completed batch 57 of epoch 137. loss=0.026725495234131813. train batch time cost=0.10087156295776367s\n",
            "completed batch 58 of epoch 137. loss=0.022526422515511513. train batch time cost=0.10089421272277832s\n",
            "completed batch 59 of epoch 137. loss=0.01457125786691904. train batch time cost=0.10155749320983887s\n",
            "completed batch 60 of epoch 137. loss=0.04244282469153404. train batch time cost=0.09556841850280762s\n",
            "completed batch 61 of epoch 137. loss=0.01795518957078457. train batch time cost=0.09788727760314941s\n",
            "completed batch 62 of epoch 137. loss=0.04167741537094116. train batch time cost=0.09592366218566895s\n",
            "completed batch 63 of epoch 137. loss=0.03725894168019295. train batch time cost=0.09657549858093262s\n",
            "completed batch 64 of epoch 137. loss=0.02784872055053711. train batch time cost=0.09470820426940918s\n",
            "completed batch 65 of epoch 137. loss=0.01860952377319336. train batch time cost=0.10228800773620605s\n",
            "completed batch 66 of epoch 137. loss=0.0076939864084124565. train batch time cost=0.10125088691711426s\n",
            "completed batch 67 of epoch 137. loss=0.0200594961643219. train batch time cost=0.10110926628112793s\n",
            "completed batch 68 of epoch 137. loss=0.017765404656529427. train batch time cost=0.10013151168823242s\n",
            "completed batch 69 of epoch 137. loss=0.003375553060323. train batch time cost=0.10128951072692871s\n",
            "completed batch 70 of epoch 137. loss=0.023087279871106148. train batch time cost=0.10457229614257812s\n",
            "completed batch 71 of epoch 137. loss=0.056487053632736206. train batch time cost=0.09725666046142578s\n",
            "completed batch 72 of epoch 137. loss=0.0009853161172941327. train batch time cost=0.09474325180053711s\n",
            "completed batch 73 of epoch 137. loss=0.008600733242928982. train batch time cost=0.09512066841125488s\n",
            "completed batch 74 of epoch 137. loss=0.01113989856094122. train batch time cost=0.10109376907348633s\n",
            "completed batch 75 of epoch 137. loss=0.007743543013930321. train batch time cost=0.10121583938598633s\n",
            "completed batch 76 of epoch 137. loss=0.01805085875093937. train batch time cost=0.10037350654602051s\n",
            "completed batch 77 of epoch 137. loss=0.008856335654854774. train batch time cost=0.10031008720397949s\n",
            "completed batch 78 of epoch 137. loss=0.01143518928438425. train batch time cost=0.10084986686706543s\n",
            "completed batch 79 of epoch 137. loss=0.04661444574594498. train batch time cost=0.10143804550170898s\n",
            "completed batch 80 of epoch 137. loss=0.02390323206782341. train batch time cost=0.10284042358398438s\n",
            "completed batch 81 of epoch 137. loss=0.0843362882733345. train batch time cost=0.10018610954284668s\n",
            "completed batch 82 of epoch 137. loss=0.06737085431814194. train batch time cost=0.10062003135681152s\n",
            "completed batch 83 of epoch 137. loss=0.04896865785121918. train batch time cost=0.10045456886291504s\n",
            "completed batch 84 of epoch 137. loss=0.026878001168370247. train batch time cost=0.10235929489135742s\n",
            "completed batch 85 of epoch 137. loss=0.02147725410759449. train batch time cost=0.10304713249206543s\n",
            "completed batch 86 of epoch 137. loss=0.01689164899289608. train batch time cost=0.10120034217834473s\n",
            "completed batch 87 of epoch 137. loss=0.017363334074616432. train batch time cost=0.10206007957458496s\n",
            "completed batch 88 of epoch 137. loss=0.009304997511208057. train batch time cost=0.10267829895019531s\n",
            "completed batch 89 of epoch 137. loss=0.021918166428804398. train batch time cost=0.10231661796569824s\n",
            "completed batch 90 of epoch 137. loss=0.01246099267154932. train batch time cost=0.10280752182006836s\n",
            "completed batch 91 of epoch 137. loss=0.010981742292642593. train batch time cost=0.10155057907104492s\n",
            "completed batch 92 of epoch 137. loss=0.0030927767511457205. train batch time cost=0.1011650562286377s\n",
            "completed batch 93 of epoch 137. loss=0.015854187309741974. train batch time cost=0.10165596008300781s\n",
            "completed batch 94 of epoch 137. loss=0.046080559492111206. train batch time cost=0.10154032707214355s\n",
            "completed batch 95 of epoch 137. loss=0.0029350807890295982. train batch time cost=0.10098528861999512s\n",
            "completed batch 96 of epoch 137. loss=0.05909405276179314. train batch time cost=0.10184836387634277s\n",
            "completed batch 97 of epoch 137. loss=0.014597252011299133. train batch time cost=0.1021726131439209s\n",
            "completed batch 98 of epoch 137. loss=0.0676349624991417. train batch time cost=0.10171842575073242s\n",
            "completed batch 99 of epoch 137. loss=0.031719934195280075. train batch time cost=0.10069394111633301s\n",
            "completed batch 100 of epoch 137. loss=0.008461636491119862. train batch time cost=0.10404515266418457s\n",
            "completed batch 101 of epoch 137. loss=0.1490197777748108. train batch time cost=0.10227704048156738s\n",
            "completed batch 102 of epoch 137. loss=0.003556005423888564. train batch time cost=0.10235714912414551s\n",
            "completed batch 103 of epoch 137. loss=0.006914169527590275. train batch time cost=0.10152220726013184s\n",
            "completed batch 104 of epoch 137. loss=0.0026385553646832705. train batch time cost=0.10227179527282715s\n",
            "completed batch 105 of epoch 137. loss=0.0029648845084011555. train batch time cost=0.1025247573852539s\n",
            "completed batch 106 of epoch 137. loss=0.015537495724856853. train batch time cost=0.10378885269165039s\n",
            "completed batch 107 of epoch 137. loss=0.05036765709519386. train batch time cost=0.10413503646850586s\n",
            "completed batch 108 of epoch 137. loss=0.032817356288433075. train batch time cost=0.10397624969482422s\n",
            "completed batch 109 of epoch 137. loss=0.023949207738041878. train batch time cost=0.10485553741455078s\n",
            "completed batch 110 of epoch 137. loss=0.01826416701078415. train batch time cost=0.10258603096008301s\n",
            "completed batch 111 of epoch 137. loss=0.010104633867740631. train batch time cost=0.10259699821472168s\n",
            "completed batch 112 of epoch 137. loss=0.04587520658969879. train batch time cost=0.1021113395690918s\n",
            "completed batch 113 of epoch 137. loss=0.004892576020210981. train batch time cost=0.10196805000305176s\n",
            "completed batch 114 of epoch 137. loss=0.018399758264422417. train batch time cost=0.1016397476196289s\n",
            "completed batch 115 of epoch 137. loss=0.015405012294650078. train batch time cost=0.10345625877380371s\n",
            "completed batch 116 of epoch 137. loss=0.01631687581539154. train batch time cost=0.10226225852966309s\n",
            "completed batch 117 of epoch 137. loss=0.02709091827273369. train batch time cost=0.10199642181396484s\n",
            "completed batch 118 of epoch 137. loss=0.01855606585741043. train batch time cost=0.10163354873657227s\n",
            "completed batch 119 of epoch 137. loss=0.05267423018813133. train batch time cost=0.1040337085723877s\n",
            "completed batch 120 of epoch 137. loss=0.022050069645047188. train batch time cost=0.10418438911437988s\n",
            "completed batch 121 of epoch 137. loss=0.008514191955327988. train batch time cost=0.10202479362487793s\n",
            "completed batch 122 of epoch 137. loss=0.04575911536812782. train batch time cost=0.10201311111450195s\n",
            "completed batch 123 of epoch 137. loss=0.007752440869808197. train batch time cost=0.10184192657470703s\n",
            "completed batch 124 of epoch 137. loss=0.05500604584813118. train batch time cost=0.10229039192199707s\n",
            "completed batch 125 of epoch 137. loss=0.025491494685411453. train batch time cost=0.102020263671875s\n",
            "completed batch 126 of epoch 137. loss=0.00011964637815253809. train batch time cost=0.029439210891723633s\n",
            "completed test of epoch 137. loss=0.00011964637815253809. accuracy=0.7054418372441338. train one epoch time cost=27.63265895843506s, test validation time cost=3.859684944152832\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338]\n",
            "completed batch 1 of epoch 138. loss=0.11094997078180313. train batch time cost=0.09516096115112305s\n",
            "completed batch 2 of epoch 138. loss=0.09778216481208801. train batch time cost=0.09357523918151855s\n",
            "completed batch 3 of epoch 138. loss=0.08743897080421448. train batch time cost=0.09338831901550293s\n",
            "completed batch 4 of epoch 138. loss=0.03307487815618515. train batch time cost=0.09518074989318848s\n",
            "completed batch 5 of epoch 138. loss=0.039304863661527634. train batch time cost=0.09457850456237793s\n",
            "completed batch 6 of epoch 138. loss=0.01572651043534279. train batch time cost=0.10283684730529785s\n",
            "completed batch 7 of epoch 138. loss=0.028207138180732727. train batch time cost=0.10218930244445801s\n",
            "completed batch 8 of epoch 138. loss=0.00949485320597887. train batch time cost=0.10198450088500977s\n",
            "completed batch 9 of epoch 138. loss=0.03589753434062004. train batch time cost=0.10318732261657715s\n",
            "completed batch 10 of epoch 138. loss=0.0030682003125548363. train batch time cost=0.09591197967529297s\n",
            "completed batch 11 of epoch 138. loss=0.015665657818317413. train batch time cost=0.09677958488464355s\n",
            "completed batch 12 of epoch 138. loss=0.03956686332821846. train batch time cost=0.09492659568786621s\n",
            "completed batch 13 of epoch 138. loss=0.054967623203992844. train batch time cost=0.09499669075012207s\n",
            "completed batch 14 of epoch 138. loss=0.04777514562010765. train batch time cost=0.104461669921875s\n",
            "completed batch 15 of epoch 138. loss=0.009160568937659264. train batch time cost=0.10093259811401367s\n",
            "completed batch 16 of epoch 138. loss=0.03705574944615364. train batch time cost=0.10233163833618164s\n",
            "completed batch 17 of epoch 138. loss=0.024067668244242668. train batch time cost=0.10234808921813965s\n",
            "completed batch 18 of epoch 138. loss=0.05748815834522247. train batch time cost=0.1008296012878418s\n",
            "completed batch 19 of epoch 138. loss=0.010569439269602299. train batch time cost=0.1008157730102539s\n",
            "completed batch 20 of epoch 138. loss=0.04319135099649429. train batch time cost=0.10552763938903809s\n",
            "completed batch 21 of epoch 138. loss=0.024769147858023643. train batch time cost=0.10221266746520996s\n",
            "completed batch 22 of epoch 138. loss=0.021020282059907913. train batch time cost=0.10203886032104492s\n",
            "completed batch 23 of epoch 138. loss=0.02915601246058941. train batch time cost=0.10356283187866211s\n",
            "completed batch 24 of epoch 138. loss=0.0022057099267840385. train batch time cost=0.1076962947845459s\n",
            "completed batch 25 of epoch 138. loss=0.042839597910642624. train batch time cost=0.10151004791259766s\n",
            "completed batch 26 of epoch 138. loss=0.0036435078363865614. train batch time cost=0.10160207748413086s\n",
            "completed batch 27 of epoch 138. loss=0.012021159753203392. train batch time cost=0.10206294059753418s\n",
            "completed batch 28 of epoch 138. loss=0.014201832003891468. train batch time cost=0.10378050804138184s\n",
            "completed batch 29 of epoch 138. loss=0.003902106313034892. train batch time cost=0.10031270980834961s\n",
            "completed batch 30 of epoch 138. loss=0.008553854189813137. train batch time cost=0.1003730297088623s\n",
            "completed batch 31 of epoch 138. loss=0.04487393796443939. train batch time cost=0.10297369956970215s\n",
            "completed batch 32 of epoch 138. loss=0.02514987625181675. train batch time cost=0.10191154479980469s\n",
            "completed batch 33 of epoch 138. loss=0.09372837841510773. train batch time cost=0.10424947738647461s\n",
            "completed batch 34 of epoch 138. loss=0.00686222268268466. train batch time cost=0.10356688499450684s\n",
            "completed batch 35 of epoch 138. loss=0.05546821281313896. train batch time cost=0.10277843475341797s\n",
            "completed batch 36 of epoch 138. loss=0.006995668169111013. train batch time cost=0.10119128227233887s\n",
            "completed batch 37 of epoch 138. loss=0.011658938601613045. train batch time cost=0.10115432739257812s\n",
            "completed batch 38 of epoch 138. loss=0.008325859904289246. train batch time cost=0.10152506828308105s\n",
            "completed batch 39 of epoch 138. loss=0.017984770238399506. train batch time cost=0.1011354923248291s\n",
            "completed batch 40 of epoch 138. loss=0.006126987747848034. train batch time cost=0.10004711151123047s\n",
            "completed batch 41 of epoch 138. loss=0.0147754717618227. train batch time cost=0.10217118263244629s\n",
            "completed batch 42 of epoch 138. loss=0.05398550257086754. train batch time cost=0.10454607009887695s\n",
            "completed batch 43 of epoch 138. loss=0.037941526621580124. train batch time cost=0.10390186309814453s\n",
            "completed batch 44 of epoch 138. loss=0.03445069491863251. train batch time cost=0.1028599739074707s\n",
            "completed batch 45 of epoch 138. loss=0.04615677520632744. train batch time cost=0.10183072090148926s\n",
            "completed batch 46 of epoch 138. loss=0.0015821617562323809. train batch time cost=0.1006920337677002s\n",
            "completed batch 47 of epoch 138. loss=0.008809327147901058. train batch time cost=0.10146570205688477s\n",
            "completed batch 48 of epoch 138. loss=0.03931231051683426. train batch time cost=0.09992194175720215s\n",
            "completed batch 49 of epoch 138. loss=0.04336564987897873. train batch time cost=0.10097241401672363s\n",
            "completed batch 50 of epoch 138. loss=0.015604319982230663. train batch time cost=0.10051274299621582s\n",
            "completed batch 51 of epoch 138. loss=0.01044758502393961. train batch time cost=0.10329556465148926s\n",
            "completed batch 52 of epoch 138. loss=0.17617136240005493. train batch time cost=0.10193014144897461s\n",
            "completed batch 53 of epoch 138. loss=0.008264008909463882. train batch time cost=0.1012868881225586s\n",
            "completed batch 54 of epoch 138. loss=0.042267557233572006. train batch time cost=0.10223579406738281s\n",
            "completed batch 55 of epoch 138. loss=0.04199747368693352. train batch time cost=0.10108733177185059s\n",
            "completed batch 56 of epoch 138. loss=0.03835594654083252. train batch time cost=0.10525178909301758s\n",
            "completed batch 57 of epoch 138. loss=0.04770417883992195. train batch time cost=0.09476876258850098s\n",
            "completed batch 58 of epoch 138. loss=0.022839989513158798. train batch time cost=0.09433317184448242s\n",
            "completed batch 59 of epoch 138. loss=0.023010501638054848. train batch time cost=0.09893393516540527s\n",
            "completed batch 60 of epoch 138. loss=0.01698577031493187. train batch time cost=0.0964193344116211s\n",
            "completed batch 61 of epoch 138. loss=0.010739387013018131. train batch time cost=0.09697175025939941s\n",
            "completed batch 62 of epoch 138. loss=0.04590635374188423. train batch time cost=0.09971070289611816s\n",
            "completed batch 63 of epoch 138. loss=0.020283767953515053. train batch time cost=0.09483790397644043s\n",
            "completed batch 64 of epoch 138. loss=0.06806079298257828. train batch time cost=0.09546422958374023s\n",
            "completed batch 65 of epoch 138. loss=0.032559458166360855. train batch time cost=0.09423041343688965s\n",
            "completed batch 66 of epoch 138. loss=0.032313983887434006. train batch time cost=0.0945591926574707s\n",
            "completed batch 67 of epoch 138. loss=0.11798711866140366. train batch time cost=0.09495282173156738s\n",
            "completed batch 68 of epoch 138. loss=0.03075472079217434. train batch time cost=0.09506368637084961s\n",
            "completed batch 69 of epoch 138. loss=0.017143070697784424. train batch time cost=0.09510087966918945s\n",
            "completed batch 70 of epoch 138. loss=0.024653993546962738. train batch time cost=0.09862422943115234s\n",
            "completed batch 71 of epoch 138. loss=0.01807842217385769. train batch time cost=0.10047078132629395s\n",
            "completed batch 72 of epoch 138. loss=0.03985116630792618. train batch time cost=0.10076570510864258s\n",
            "completed batch 73 of epoch 138. loss=0.06612040102481842. train batch time cost=0.10119247436523438s\n",
            "completed batch 74 of epoch 138. loss=0.05910484492778778. train batch time cost=0.10149002075195312s\n",
            "completed batch 75 of epoch 138. loss=0.0469386987388134. train batch time cost=0.10088300704956055s\n",
            "completed batch 76 of epoch 138. loss=0.040719788521528244. train batch time cost=0.0994424819946289s\n",
            "completed batch 77 of epoch 138. loss=0.008294233120977879. train batch time cost=0.10316061973571777s\n",
            "completed batch 78 of epoch 138. loss=0.010273821651935577. train batch time cost=0.10133695602416992s\n",
            "completed batch 79 of epoch 138. loss=0.0026220434810966253. train batch time cost=0.10197997093200684s\n",
            "completed batch 80 of epoch 138. loss=0.018490048125386238. train batch time cost=0.10219836235046387s\n",
            "completed batch 81 of epoch 138. loss=0.019470587372779846. train batch time cost=0.1006159782409668s\n",
            "completed batch 82 of epoch 138. loss=0.0388774536550045. train batch time cost=0.1030735969543457s\n",
            "completed batch 83 of epoch 138. loss=0.007054243702441454. train batch time cost=0.1016082763671875s\n",
            "completed batch 84 of epoch 138. loss=0.10559367388486862. train batch time cost=0.10082292556762695s\n",
            "completed batch 85 of epoch 138. loss=0.022489141672849655. train batch time cost=0.10208535194396973s\n",
            "completed batch 86 of epoch 138. loss=0.006562383845448494. train batch time cost=0.09941792488098145s\n",
            "completed batch 87 of epoch 138. loss=0.0069993361830711365. train batch time cost=0.09619259834289551s\n",
            "completed batch 88 of epoch 138. loss=0.021151989698410034. train batch time cost=0.10251188278198242s\n",
            "completed batch 89 of epoch 138. loss=0.04492933303117752. train batch time cost=0.10212421417236328s\n",
            "completed batch 90 of epoch 138. loss=0.06178798899054527. train batch time cost=0.10166382789611816s\n",
            "completed batch 91 of epoch 138. loss=0.02924679033458233. train batch time cost=0.10184645652770996s\n",
            "completed batch 92 of epoch 138. loss=0.03940447419881821. train batch time cost=0.09544968605041504s\n",
            "completed batch 93 of epoch 138. loss=0.03208301588892937. train batch time cost=0.09549403190612793s\n",
            "completed batch 94 of epoch 138. loss=0.051452502608299255. train batch time cost=0.09441542625427246s\n",
            "completed batch 95 of epoch 138. loss=0.02275320328772068. train batch time cost=0.09638738632202148s\n",
            "completed batch 96 of epoch 138. loss=0.019335106015205383. train batch time cost=0.09538459777832031s\n",
            "completed batch 97 of epoch 138. loss=0.05187513306736946. train batch time cost=0.09648489952087402s\n",
            "completed batch 98 of epoch 138. loss=0.014622541144490242. train batch time cost=0.09510183334350586s\n",
            "completed batch 99 of epoch 138. loss=0.01034008339047432. train batch time cost=0.0950770378112793s\n",
            "completed batch 100 of epoch 138. loss=0.017539862543344498. train batch time cost=0.09595465660095215s\n",
            "completed batch 101 of epoch 138. loss=0.09815700352191925. train batch time cost=0.09842872619628906s\n",
            "completed batch 102 of epoch 138. loss=0.002492072293534875. train batch time cost=0.09594178199768066s\n",
            "completed batch 103 of epoch 138. loss=0.008788746781647205. train batch time cost=0.09744715690612793s\n",
            "completed batch 104 of epoch 138. loss=0.00810007844120264. train batch time cost=0.0944969654083252s\n",
            "completed batch 105 of epoch 138. loss=0.00663777906447649. train batch time cost=0.09476709365844727s\n",
            "completed batch 106 of epoch 138. loss=0.015891054645180702. train batch time cost=0.09635138511657715s\n",
            "completed batch 107 of epoch 138. loss=0.04137830808758736. train batch time cost=0.09703564643859863s\n",
            "completed batch 108 of epoch 138. loss=0.015132606029510498. train batch time cost=0.0958249568939209s\n",
            "completed batch 109 of epoch 138. loss=0.04643230885267258. train batch time cost=0.09751057624816895s\n",
            "completed batch 110 of epoch 138. loss=0.027404269203543663. train batch time cost=0.10032773017883301s\n",
            "completed batch 111 of epoch 138. loss=0.03527915105223656. train batch time cost=0.10211181640625s\n",
            "completed batch 112 of epoch 138. loss=0.020718511193990707. train batch time cost=0.1023867130279541s\n",
            "completed batch 113 of epoch 138. loss=0.012639007531106472. train batch time cost=0.10158014297485352s\n",
            "completed batch 114 of epoch 138. loss=0.00846086349338293. train batch time cost=0.10123658180236816s\n",
            "completed batch 115 of epoch 138. loss=0.04755410552024841. train batch time cost=0.10190224647521973s\n",
            "completed batch 116 of epoch 138. loss=0.01709008775651455. train batch time cost=0.10193538665771484s\n",
            "completed batch 117 of epoch 138. loss=0.0057016401551663876. train batch time cost=0.1018228530883789s\n",
            "completed batch 118 of epoch 138. loss=0.03437276929616928. train batch time cost=0.10605645179748535s\n",
            "completed batch 119 of epoch 138. loss=0.08418282121419907. train batch time cost=0.10253667831420898s\n",
            "completed batch 120 of epoch 138. loss=0.0572955459356308. train batch time cost=0.10348987579345703s\n",
            "completed batch 121 of epoch 138. loss=0.03216148167848587. train batch time cost=0.10132312774658203s\n",
            "completed batch 122 of epoch 138. loss=0.021816326305270195. train batch time cost=0.10147452354431152s\n",
            "completed batch 123 of epoch 138. loss=0.00464621651917696. train batch time cost=0.10195469856262207s\n",
            "completed batch 124 of epoch 138. loss=0.023692278191447258. train batch time cost=0.10194110870361328s\n",
            "completed batch 125 of epoch 138. loss=0.03284550458192825. train batch time cost=0.1020047664642334s\n",
            "completed batch 126 of epoch 138. loss=0.00037130116834305227. train batch time cost=0.03029489517211914s\n",
            "completed test of epoch 138. loss=0.00037130116834305227. accuracy=0.708936595107339. train one epoch time cost=27.431428909301758s, test validation time cost=3.946098804473877\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339]\n",
            "completed batch 1 of epoch 139. loss=0.02493932843208313. train batch time cost=0.09450244903564453s\n",
            "completed batch 2 of epoch 139. loss=0.019256746396422386. train batch time cost=0.09496903419494629s\n",
            "completed batch 3 of epoch 139. loss=0.011526485905051231. train batch time cost=0.09552359580993652s\n",
            "completed batch 4 of epoch 139. loss=0.0186525397002697. train batch time cost=0.09510421752929688s\n",
            "completed batch 5 of epoch 139. loss=0.013007794506847858. train batch time cost=0.09598278999328613s\n",
            "completed batch 6 of epoch 139. loss=0.002392740221694112. train batch time cost=0.09510445594787598s\n",
            "completed batch 7 of epoch 139. loss=0.053495489060878754. train batch time cost=0.09590840339660645s\n",
            "completed batch 8 of epoch 139. loss=0.009560628794133663. train batch time cost=0.09432411193847656s\n",
            "completed batch 9 of epoch 139. loss=0.015007097274065018. train batch time cost=0.09477663040161133s\n",
            "completed batch 10 of epoch 139. loss=0.056577350944280624. train batch time cost=0.09537196159362793s\n",
            "completed batch 11 of epoch 139. loss=0.00446112547069788. train batch time cost=0.09581327438354492s\n",
            "completed batch 12 of epoch 139. loss=0.006967315450310707. train batch time cost=0.09644174575805664s\n",
            "completed batch 13 of epoch 139. loss=0.04017007350921631. train batch time cost=0.09558486938476562s\n",
            "completed batch 14 of epoch 139. loss=0.09355764836072922. train batch time cost=0.09806370735168457s\n",
            "completed batch 15 of epoch 139. loss=0.024126200005412102. train batch time cost=0.09581923484802246s\n",
            "completed batch 16 of epoch 139. loss=0.004279321059584618. train batch time cost=0.09874510765075684s\n",
            "completed batch 17 of epoch 139. loss=0.0017285867361351848. train batch time cost=0.09545755386352539s\n",
            "completed batch 18 of epoch 139. loss=0.10193502902984619. train batch time cost=0.09493136405944824s\n",
            "completed batch 19 of epoch 139. loss=0.003480545710772276. train batch time cost=0.09615874290466309s\n",
            "completed batch 20 of epoch 139. loss=0.03358636796474457. train batch time cost=0.09516215324401855s\n",
            "completed batch 21 of epoch 139. loss=0.024598542600870132. train batch time cost=0.09603714942932129s\n",
            "completed batch 22 of epoch 139. loss=0.03138301894068718. train batch time cost=0.09402871131896973s\n",
            "completed batch 23 of epoch 139. loss=0.06572999805212021. train batch time cost=0.09460234642028809s\n",
            "completed batch 24 of epoch 139. loss=0.04344283416867256. train batch time cost=0.09435677528381348s\n",
            "completed batch 25 of epoch 139. loss=0.006766884587705135. train batch time cost=0.09451484680175781s\n",
            "completed batch 26 of epoch 139. loss=0.09008334577083588. train batch time cost=0.09370279312133789s\n",
            "completed batch 27 of epoch 139. loss=0.020581481978297234. train batch time cost=0.09612059593200684s\n",
            "completed batch 28 of epoch 139. loss=0.020440222695469856. train batch time cost=0.09678316116333008s\n",
            "completed batch 29 of epoch 139. loss=0.017744552344083786. train batch time cost=0.09469771385192871s\n",
            "completed batch 30 of epoch 139. loss=0.03458192199468613. train batch time cost=0.09617447853088379s\n",
            "completed batch 31 of epoch 139. loss=0.03890538215637207. train batch time cost=0.09548354148864746s\n",
            "completed batch 32 of epoch 139. loss=0.08759269118309021. train batch time cost=0.09608650207519531s\n",
            "completed batch 33 of epoch 139. loss=0.048854928463697433. train batch time cost=0.09614086151123047s\n",
            "completed batch 34 of epoch 139. loss=0.09061796218156815. train batch time cost=0.10172915458679199s\n",
            "completed batch 35 of epoch 139. loss=0.0029141302220523357. train batch time cost=0.09595632553100586s\n",
            "completed batch 36 of epoch 139. loss=0.039540406316518784. train batch time cost=0.09521865844726562s\n",
            "completed batch 37 of epoch 139. loss=0.02784213423728943. train batch time cost=0.09557652473449707s\n",
            "completed batch 38 of epoch 139. loss=0.030272813513875008. train batch time cost=0.09599709510803223s\n",
            "completed batch 39 of epoch 139. loss=0.002506118966266513. train batch time cost=0.09535384178161621s\n",
            "completed batch 40 of epoch 139. loss=0.07429221272468567. train batch time cost=0.09502506256103516s\n",
            "completed batch 41 of epoch 139. loss=0.07895857095718384. train batch time cost=0.1002507209777832s\n",
            "completed batch 42 of epoch 139. loss=0.04001332074403763. train batch time cost=0.10149741172790527s\n",
            "completed batch 43 of epoch 139. loss=0.015429774299263954. train batch time cost=0.10284209251403809s\n",
            "completed batch 44 of epoch 139. loss=0.024188170209527016. train batch time cost=0.10268139839172363s\n",
            "completed batch 45 of epoch 139. loss=0.009467147290706635. train batch time cost=0.10240483283996582s\n",
            "completed batch 46 of epoch 139. loss=0.032660964876413345. train batch time cost=0.10232758522033691s\n",
            "completed batch 47 of epoch 139. loss=0.014963990077376366. train batch time cost=0.10447120666503906s\n",
            "completed batch 48 of epoch 139. loss=0.06197650358080864. train batch time cost=0.10447978973388672s\n",
            "completed batch 49 of epoch 139. loss=0.012928081676363945. train batch time cost=0.11092662811279297s\n",
            "completed batch 50 of epoch 139. loss=0.09435833245515823. train batch time cost=0.10038995742797852s\n",
            "completed batch 51 of epoch 139. loss=0.022152621299028397. train batch time cost=0.1028439998626709s\n",
            "completed batch 52 of epoch 139. loss=0.01886962167918682. train batch time cost=0.10410165786743164s\n",
            "completed batch 53 of epoch 139. loss=0.03644547611474991. train batch time cost=0.10199213027954102s\n",
            "completed batch 54 of epoch 139. loss=0.009226808324456215. train batch time cost=0.1037437915802002s\n",
            "completed batch 55 of epoch 139. loss=0.064582958817482. train batch time cost=0.1024785041809082s\n",
            "completed batch 56 of epoch 139. loss=0.012803295627236366. train batch time cost=0.10197091102600098s\n",
            "completed batch 57 of epoch 139. loss=0.005193283781409264. train batch time cost=0.10188436508178711s\n",
            "completed batch 58 of epoch 139. loss=0.005052478983998299. train batch time cost=0.10119748115539551s\n",
            "completed batch 59 of epoch 139. loss=0.01974707841873169. train batch time cost=0.10170388221740723s\n",
            "completed batch 60 of epoch 139. loss=0.02148096077144146. train batch time cost=0.101715087890625s\n",
            "completed batch 61 of epoch 139. loss=0.01094756554812193. train batch time cost=0.10224437713623047s\n",
            "completed batch 62 of epoch 139. loss=0.003482289146631956. train batch time cost=0.10222053527832031s\n",
            "completed batch 63 of epoch 139. loss=0.021715106442570686. train batch time cost=0.1029667854309082s\n",
            "completed batch 64 of epoch 139. loss=0.005895358510315418. train batch time cost=0.1011507511138916s\n",
            "completed batch 65 of epoch 139. loss=0.00452885776758194. train batch time cost=0.10123491287231445s\n",
            "completed batch 66 of epoch 139. loss=0.027240794152021408. train batch time cost=0.1014411449432373s\n",
            "completed batch 67 of epoch 139. loss=0.00457622529938817. train batch time cost=0.09582352638244629s\n",
            "completed batch 68 of epoch 139. loss=0.031138774007558823. train batch time cost=0.09418106079101562s\n",
            "completed batch 69 of epoch 139. loss=0.005710285156965256. train batch time cost=0.09450435638427734s\n",
            "completed batch 70 of epoch 139. loss=0.04268582910299301. train batch time cost=0.09451627731323242s\n",
            "completed batch 71 of epoch 139. loss=0.05251053720712662. train batch time cost=0.0941460132598877s\n",
            "completed batch 72 of epoch 139. loss=0.046472132205963135. train batch time cost=0.09462738037109375s\n",
            "completed batch 73 of epoch 139. loss=0.02310296706855297. train batch time cost=0.09675025939941406s\n",
            "completed batch 74 of epoch 139. loss=0.12904620170593262. train batch time cost=0.09586620330810547s\n",
            "completed batch 75 of epoch 139. loss=0.05331377312541008. train batch time cost=0.09509110450744629s\n",
            "completed batch 76 of epoch 139. loss=0.04214290529489517. train batch time cost=0.09704470634460449s\n",
            "completed batch 77 of epoch 139. loss=0.0055247945711016655. train batch time cost=0.09445333480834961s\n",
            "completed batch 78 of epoch 139. loss=0.016706880182027817. train batch time cost=0.09670877456665039s\n",
            "completed batch 79 of epoch 139. loss=0.04197898879647255. train batch time cost=0.09453582763671875s\n",
            "completed batch 80 of epoch 139. loss=0.013945023529231548. train batch time cost=0.09447789192199707s\n",
            "completed batch 81 of epoch 139. loss=0.10651352256536484. train batch time cost=0.09655189514160156s\n",
            "completed batch 82 of epoch 139. loss=0.02408106066286564. train batch time cost=0.0946798324584961s\n",
            "completed batch 83 of epoch 139. loss=0.019906165078282356. train batch time cost=0.09418988227844238s\n",
            "completed batch 84 of epoch 139. loss=0.014475851319730282. train batch time cost=0.09429764747619629s\n",
            "completed batch 85 of epoch 139. loss=0.03410632535815239. train batch time cost=0.09427404403686523s\n",
            "completed batch 86 of epoch 139. loss=0.021808885037899017. train batch time cost=0.09339404106140137s\n",
            "completed batch 87 of epoch 139. loss=0.07027623057365417. train batch time cost=0.09482574462890625s\n",
            "completed batch 88 of epoch 139. loss=0.0047762137837708. train batch time cost=0.09653329849243164s\n",
            "completed batch 89 of epoch 139. loss=0.017551491037011147. train batch time cost=0.09561896324157715s\n",
            "completed batch 90 of epoch 139. loss=0.08763886243104935. train batch time cost=0.09519052505493164s\n",
            "completed batch 91 of epoch 139. loss=0.0021161341574043036. train batch time cost=0.09446096420288086s\n",
            "completed batch 92 of epoch 139. loss=0.09629591554403305. train batch time cost=0.09453129768371582s\n",
            "completed batch 93 of epoch 139. loss=0.04720563441514969. train batch time cost=0.09534358978271484s\n",
            "completed batch 94 of epoch 139. loss=0.004320492502301931. train batch time cost=0.09406566619873047s\n",
            "completed batch 95 of epoch 139. loss=0.06217706575989723. train batch time cost=0.0962061882019043s\n",
            "completed batch 96 of epoch 139. loss=0.01442955993115902. train batch time cost=0.09910464286804199s\n",
            "completed batch 97 of epoch 139. loss=0.013463262468576431. train batch time cost=0.09529566764831543s\n",
            "completed batch 98 of epoch 139. loss=0.024618295952677727. train batch time cost=0.09760665893554688s\n",
            "completed batch 99 of epoch 139. loss=0.032450977712869644. train batch time cost=0.09930109977722168s\n",
            "completed batch 100 of epoch 139. loss=0.013444708660244942. train batch time cost=0.10107684135437012s\n",
            "completed batch 101 of epoch 139. loss=0.08189687132835388. train batch time cost=0.1004343032836914s\n",
            "completed batch 102 of epoch 139. loss=0.04025263711810112. train batch time cost=0.10077595710754395s\n",
            "completed batch 103 of epoch 139. loss=0.01448565348982811. train batch time cost=0.10167074203491211s\n",
            "completed batch 104 of epoch 139. loss=0.015005151741206646. train batch time cost=0.1047816276550293s\n",
            "completed batch 105 of epoch 139. loss=0.010793359950184822. train batch time cost=0.10207629203796387s\n",
            "completed batch 106 of epoch 139. loss=0.011780261062085629. train batch time cost=0.10270333290100098s\n",
            "completed batch 107 of epoch 139. loss=0.05223097652196884. train batch time cost=0.10187077522277832s\n",
            "completed batch 108 of epoch 139. loss=0.06404881179332733. train batch time cost=0.10307765007019043s\n",
            "completed batch 109 of epoch 139. loss=0.021823052316904068. train batch time cost=0.10143566131591797s\n",
            "completed batch 110 of epoch 139. loss=0.026885762810707092. train batch time cost=0.10987067222595215s\n",
            "completed batch 111 of epoch 139. loss=0.01230368297547102. train batch time cost=0.10254454612731934s\n",
            "completed batch 112 of epoch 139. loss=0.027066737413406372. train batch time cost=0.10250210762023926s\n",
            "completed batch 113 of epoch 139. loss=0.004769672639667988. train batch time cost=0.1028299331665039s\n",
            "completed batch 114 of epoch 139. loss=0.005298443138599396. train batch time cost=0.10183191299438477s\n",
            "completed batch 115 of epoch 139. loss=0.012839280068874359. train batch time cost=0.10422158241271973s\n",
            "completed batch 116 of epoch 139. loss=0.06936065107584. train batch time cost=0.10252118110656738s\n",
            "completed batch 117 of epoch 139. loss=0.00613256823271513. train batch time cost=0.10268521308898926s\n",
            "completed batch 118 of epoch 139. loss=0.004171424545347691. train batch time cost=0.10231709480285645s\n",
            "completed batch 119 of epoch 139. loss=0.011383599601686. train batch time cost=0.10207796096801758s\n",
            "completed batch 120 of epoch 139. loss=0.013400032185018063. train batch time cost=0.10181093215942383s\n",
            "completed batch 121 of epoch 139. loss=0.00955559778958559. train batch time cost=0.10309100151062012s\n",
            "completed batch 122 of epoch 139. loss=0.016563599929213524. train batch time cost=0.10156965255737305s\n",
            "completed batch 123 of epoch 139. loss=0.019329005852341652. train batch time cost=0.10228395462036133s\n",
            "completed batch 124 of epoch 139. loss=0.021681375801563263. train batch time cost=0.10120487213134766s\n",
            "completed batch 125 of epoch 139. loss=0.023695997893810272. train batch time cost=0.10178017616271973s\n",
            "completed batch 126 of epoch 139. loss=0.0043097310699522495. train batch time cost=0.029704809188842773s\n",
            "completed test of epoch 139. loss=0.0043097310699522495. accuracy=0.6874687968047928. train one epoch time cost=27.234503269195557s, test validation time cost=3.8362505435943604\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928]\n",
            "completed batch 1 of epoch 140. loss=0.016097165644168854. train batch time cost=0.10341310501098633s\n",
            "completed batch 2 of epoch 140. loss=0.013618811964988708. train batch time cost=0.1022946834564209s\n",
            "completed batch 3 of epoch 140. loss=0.056360576301813126. train batch time cost=0.1010293960571289s\n",
            "completed batch 4 of epoch 140. loss=0.021792592480778694. train batch time cost=0.10087060928344727s\n",
            "completed batch 5 of epoch 140. loss=0.014646779745817184. train batch time cost=0.10213661193847656s\n",
            "completed batch 6 of epoch 140. loss=0.01217968575656414. train batch time cost=0.1019594669342041s\n",
            "completed batch 7 of epoch 140. loss=0.056886959820985794. train batch time cost=0.10181951522827148s\n",
            "completed batch 8 of epoch 140. loss=0.00531418714672327. train batch time cost=0.10253000259399414s\n",
            "completed batch 9 of epoch 140. loss=0.017063619568943977. train batch time cost=0.10184359550476074s\n",
            "completed batch 10 of epoch 140. loss=0.04505340754985809. train batch time cost=0.10272502899169922s\n",
            "completed batch 11 of epoch 140. loss=0.013182315044105053. train batch time cost=0.10384464263916016s\n",
            "completed batch 12 of epoch 140. loss=0.010552098974585533. train batch time cost=0.10218071937561035s\n",
            "completed batch 13 of epoch 140. loss=0.10732637345790863. train batch time cost=0.10137605667114258s\n",
            "completed batch 14 of epoch 140. loss=0.01684405282139778. train batch time cost=0.1019139289855957s\n",
            "completed batch 15 of epoch 140. loss=0.016430309042334557. train batch time cost=0.10293030738830566s\n",
            "completed batch 16 of epoch 140. loss=0.004589807707816362. train batch time cost=0.10326528549194336s\n",
            "completed batch 17 of epoch 140. loss=0.0029269300866872072. train batch time cost=0.10216760635375977s\n",
            "completed batch 18 of epoch 140. loss=0.016172442585229874. train batch time cost=0.1018216609954834s\n",
            "completed batch 19 of epoch 140. loss=0.03909173607826233. train batch time cost=0.10260200500488281s\n",
            "completed batch 20 of epoch 140. loss=0.0030349979642778635. train batch time cost=0.10396838188171387s\n",
            "completed batch 21 of epoch 140. loss=0.024757862091064453. train batch time cost=0.10646700859069824s\n",
            "completed batch 22 of epoch 140. loss=0.001221147133037448. train batch time cost=0.10377764701843262s\n",
            "completed batch 23 of epoch 140. loss=0.058974768966436386. train batch time cost=0.10252022743225098s\n",
            "completed batch 24 of epoch 140. loss=0.014589495956897736. train batch time cost=0.10262775421142578s\n",
            "completed batch 25 of epoch 140. loss=0.009378235787153244. train batch time cost=0.10249042510986328s\n",
            "completed batch 26 of epoch 140. loss=0.025835566222667694. train batch time cost=0.10186100006103516s\n",
            "completed batch 27 of epoch 140. loss=0.08186861872673035. train batch time cost=0.10175299644470215s\n",
            "completed batch 28 of epoch 140. loss=0.004528713412582874. train batch time cost=0.10280489921569824s\n",
            "completed batch 29 of epoch 140. loss=0.005252298433333635. train batch time cost=0.102325439453125s\n",
            "completed batch 30 of epoch 140. loss=0.0015659084310755134. train batch time cost=0.10308218002319336s\n",
            "completed batch 31 of epoch 140. loss=0.0019018673337996006. train batch time cost=0.10186958312988281s\n",
            "completed batch 32 of epoch 140. loss=0.014485981315374374. train batch time cost=0.10108113288879395s\n",
            "completed batch 33 of epoch 140. loss=0.01260833814740181. train batch time cost=0.10190367698669434s\n",
            "completed batch 34 of epoch 140. loss=0.025156689807772636. train batch time cost=0.1025087833404541s\n",
            "completed batch 35 of epoch 140. loss=0.01563098654150963. train batch time cost=0.10135793685913086s\n",
            "completed batch 36 of epoch 140. loss=0.08345919102430344. train batch time cost=0.10095000267028809s\n",
            "completed batch 37 of epoch 140. loss=0.0024166444782167673. train batch time cost=0.10917878150939941s\n",
            "completed batch 38 of epoch 140. loss=0.013829870149493217. train batch time cost=0.10287809371948242s\n",
            "completed batch 39 of epoch 140. loss=0.008068258874118328. train batch time cost=0.095855712890625s\n",
            "completed batch 40 of epoch 140. loss=0.007740504574030638. train batch time cost=0.09537911415100098s\n",
            "completed batch 41 of epoch 140. loss=0.05720088630914688. train batch time cost=0.09567427635192871s\n",
            "completed batch 42 of epoch 140. loss=0.008172182366251945. train batch time cost=0.09608626365661621s\n",
            "completed batch 43 of epoch 140. loss=0.003311777953058481. train batch time cost=0.0962991714477539s\n",
            "completed batch 44 of epoch 140. loss=0.024627121165394783. train batch time cost=0.09378576278686523s\n",
            "completed batch 45 of epoch 140. loss=0.0030658869072794914. train batch time cost=0.093994140625s\n",
            "completed batch 46 of epoch 140. loss=0.012112331576645374. train batch time cost=0.09398198127746582s\n",
            "completed batch 47 of epoch 140. loss=0.004776305519044399. train batch time cost=0.0954744815826416s\n",
            "completed batch 48 of epoch 140. loss=0.009346780367195606. train batch time cost=0.09505057334899902s\n",
            "completed batch 49 of epoch 140. loss=0.007002050522714853. train batch time cost=0.09383511543273926s\n",
            "completed batch 50 of epoch 140. loss=0.0027753906324505806. train batch time cost=0.09349822998046875s\n",
            "completed batch 51 of epoch 140. loss=0.008793598972260952. train batch time cost=0.09401440620422363s\n",
            "completed batch 52 of epoch 140. loss=0.0018477707635611296. train batch time cost=0.10337209701538086s\n",
            "completed batch 53 of epoch 140. loss=0.01764281839132309. train batch time cost=0.10212516784667969s\n",
            "completed batch 54 of epoch 140. loss=0.0036728631239384413. train batch time cost=0.10049629211425781s\n",
            "completed batch 55 of epoch 140. loss=0.005485928617417812. train batch time cost=0.10022711753845215s\n",
            "completed batch 56 of epoch 140. loss=0.009026480838656425. train batch time cost=0.10151839256286621s\n",
            "completed batch 57 of epoch 140. loss=0.006548288278281689. train batch time cost=0.10079646110534668s\n",
            "completed batch 58 of epoch 140. loss=0.006110264919698238. train batch time cost=0.10117077827453613s\n",
            "completed batch 59 of epoch 140. loss=0.030919499695301056. train batch time cost=0.10096430778503418s\n",
            "completed batch 60 of epoch 140. loss=0.005850075278431177. train batch time cost=0.09514307975769043s\n",
            "completed batch 61 of epoch 140. loss=0.01816360279917717. train batch time cost=0.09898209571838379s\n",
            "completed batch 62 of epoch 140. loss=0.011021926999092102. train batch time cost=0.09553861618041992s\n",
            "completed batch 63 of epoch 140. loss=0.020726459100842476. train batch time cost=0.09667110443115234s\n",
            "completed batch 64 of epoch 140. loss=0.00335049070417881. train batch time cost=0.09634113311767578s\n",
            "completed batch 65 of epoch 140. loss=0.0017267026705667377. train batch time cost=0.09625840187072754s\n",
            "completed batch 66 of epoch 140. loss=0.004393951501697302. train batch time cost=0.09651684761047363s\n",
            "completed batch 67 of epoch 140. loss=0.01564841717481613. train batch time cost=0.09587979316711426s\n",
            "completed batch 68 of epoch 140. loss=0.00915021263062954. train batch time cost=0.09625673294067383s\n",
            "completed batch 69 of epoch 140. loss=0.001891972846351564. train batch time cost=0.09742116928100586s\n",
            "completed batch 70 of epoch 140. loss=0.0026132112834602594. train batch time cost=0.10240459442138672s\n",
            "completed batch 71 of epoch 140. loss=0.0747060775756836. train batch time cost=0.10012602806091309s\n",
            "completed batch 72 of epoch 140. loss=0.001216505654156208. train batch time cost=0.09952473640441895s\n",
            "completed batch 73 of epoch 140. loss=0.005120345391333103. train batch time cost=0.10187816619873047s\n",
            "completed batch 74 of epoch 140. loss=0.008519724011421204. train batch time cost=0.10009431838989258s\n",
            "completed batch 75 of epoch 140. loss=0.006894842721521854. train batch time cost=0.10337090492248535s\n",
            "completed batch 76 of epoch 140. loss=0.033743079751729965. train batch time cost=0.10364341735839844s\n",
            "completed batch 77 of epoch 140. loss=0.013515439815819263. train batch time cost=0.10111308097839355s\n",
            "completed batch 78 of epoch 140. loss=0.012274199165403843. train batch time cost=0.10090899467468262s\n",
            "completed batch 79 of epoch 140. loss=0.0012961976462975144. train batch time cost=0.10251688957214355s\n",
            "completed batch 80 of epoch 140. loss=0.01689319871366024. train batch time cost=0.10233235359191895s\n",
            "completed batch 81 of epoch 140. loss=0.0036985925398766994. train batch time cost=0.10233592987060547s\n",
            "completed batch 82 of epoch 140. loss=0.006265639327466488. train batch time cost=0.10136961936950684s\n",
            "completed batch 83 of epoch 140. loss=0.0064439610578119755. train batch time cost=0.10134649276733398s\n",
            "completed batch 84 of epoch 140. loss=0.08038865774869919. train batch time cost=0.10621857643127441s\n",
            "completed batch 85 of epoch 140. loss=0.030763709917664528. train batch time cost=0.10139346122741699s\n",
            "completed batch 86 of epoch 140. loss=0.022431856021285057. train batch time cost=0.10163712501525879s\n",
            "completed batch 87 of epoch 140. loss=0.032115645706653595. train batch time cost=0.1018521785736084s\n",
            "completed batch 88 of epoch 140. loss=0.009797298349440098. train batch time cost=0.10181212425231934s\n",
            "completed batch 89 of epoch 140. loss=0.010373414494097233. train batch time cost=0.10292434692382812s\n",
            "completed batch 90 of epoch 140. loss=0.02797364629805088. train batch time cost=0.10188746452331543s\n",
            "completed batch 91 of epoch 140. loss=0.01709708943963051. train batch time cost=0.10152411460876465s\n",
            "completed batch 92 of epoch 140. loss=0.054977770894765854. train batch time cost=0.10202836990356445s\n",
            "completed batch 93 of epoch 140. loss=0.015751827508211136. train batch time cost=0.10215139389038086s\n",
            "completed batch 94 of epoch 140. loss=0.005322644952684641. train batch time cost=0.10235953330993652s\n",
            "completed batch 95 of epoch 140. loss=0.009549971669912338. train batch time cost=0.10100269317626953s\n",
            "completed batch 96 of epoch 140. loss=0.00708248745650053. train batch time cost=0.10195064544677734s\n",
            "completed batch 97 of epoch 140. loss=0.05928127467632294. train batch time cost=0.09539437294006348s\n",
            "completed batch 98 of epoch 140. loss=0.008459333330392838. train batch time cost=0.09547066688537598s\n",
            "completed batch 99 of epoch 140. loss=0.013021419756114483. train batch time cost=0.09420943260192871s\n",
            "completed batch 100 of epoch 140. loss=0.0449380949139595. train batch time cost=0.09392356872558594s\n",
            "completed batch 101 of epoch 140. loss=0.0021470049396157265. train batch time cost=0.09355044364929199s\n",
            "completed batch 102 of epoch 140. loss=0.004220760427415371. train batch time cost=0.09470820426940918s\n",
            "completed batch 103 of epoch 140. loss=0.033875685185194016. train batch time cost=0.1014559268951416s\n",
            "completed batch 104 of epoch 140. loss=0.01784447580575943. train batch time cost=0.10167288780212402s\n",
            "completed batch 105 of epoch 140. loss=0.008498926647007465. train batch time cost=0.10032463073730469s\n",
            "completed batch 106 of epoch 140. loss=0.07491255551576614. train batch time cost=0.10167646408081055s\n",
            "completed batch 107 of epoch 140. loss=0.011373470537364483. train batch time cost=0.10252761840820312s\n",
            "completed batch 108 of epoch 140. loss=0.014037407003343105. train batch time cost=0.10302138328552246s\n",
            "completed batch 109 of epoch 140. loss=0.005362208932638168. train batch time cost=0.10199284553527832s\n",
            "completed batch 110 of epoch 140. loss=0.023544566705822945. train batch time cost=0.10182833671569824s\n",
            "completed batch 111 of epoch 140. loss=0.046693652868270874. train batch time cost=0.10225057601928711s\n",
            "completed batch 112 of epoch 140. loss=0.13725924491882324. train batch time cost=0.10260486602783203s\n",
            "completed batch 113 of epoch 140. loss=0.013883472420275211. train batch time cost=0.10122036933898926s\n",
            "completed batch 114 of epoch 140. loss=0.04173489287495613. train batch time cost=0.1035623550415039s\n",
            "completed batch 115 of epoch 140. loss=0.019024156033992767. train batch time cost=0.0963137149810791s\n",
            "completed batch 116 of epoch 140. loss=0.05247165635228157. train batch time cost=0.10239648818969727s\n",
            "completed batch 117 of epoch 140. loss=0.03231269493699074. train batch time cost=0.10405588150024414s\n",
            "completed batch 118 of epoch 140. loss=0.013082843273878098. train batch time cost=0.10238099098205566s\n",
            "completed batch 119 of epoch 140. loss=0.09529087692499161. train batch time cost=0.10295939445495605s\n",
            "completed batch 120 of epoch 140. loss=0.16805002093315125. train batch time cost=0.1023256778717041s\n",
            "completed batch 121 of epoch 140. loss=0.02986272983253002. train batch time cost=0.10256361961364746s\n",
            "completed batch 122 of epoch 140. loss=0.003998544532805681. train batch time cost=0.1017765998840332s\n",
            "completed batch 123 of epoch 140. loss=0.0055861868895590305. train batch time cost=0.10139083862304688s\n",
            "completed batch 124 of epoch 140. loss=0.007439916953444481. train batch time cost=0.10137414932250977s\n",
            "completed batch 125 of epoch 140. loss=0.025245890021324158. train batch time cost=0.09979128837585449s\n",
            "completed batch 126 of epoch 140. loss=0.0005926871090196073. train batch time cost=0.030735015869140625s\n",
            "completed test of epoch 140. loss=0.0005926871090196073. accuracy=0.7004493260109835. train one epoch time cost=27.504007577896118s, test validation time cost=3.916755437850952\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835]\n",
            "completed batch 1 of epoch 141. loss=0.07791876047849655. train batch time cost=0.09825801849365234s\n",
            "completed batch 2 of epoch 141. loss=0.11735851317644119. train batch time cost=0.09568619728088379s\n",
            "completed batch 3 of epoch 141. loss=0.09721460193395615. train batch time cost=0.0956106185913086s\n",
            "completed batch 4 of epoch 141. loss=0.02453995682299137. train batch time cost=0.09547233581542969s\n",
            "completed batch 5 of epoch 141. loss=0.062424611300230026. train batch time cost=0.09594559669494629s\n",
            "completed batch 6 of epoch 141. loss=0.04905393347144127. train batch time cost=0.09615635871887207s\n",
            "completed batch 7 of epoch 141. loss=0.01449825894087553. train batch time cost=0.09479522705078125s\n",
            "completed batch 8 of epoch 141. loss=0.1011165976524353. train batch time cost=0.09435343742370605s\n",
            "completed batch 9 of epoch 141. loss=0.18981428444385529. train batch time cost=0.10209512710571289s\n",
            "completed batch 10 of epoch 141. loss=0.012347024865448475. train batch time cost=0.10136675834655762s\n",
            "completed batch 11 of epoch 141. loss=0.014693740755319595. train batch time cost=0.10529232025146484s\n",
            "completed batch 12 of epoch 141. loss=0.010811811313033104. train batch time cost=0.10125112533569336s\n",
            "completed batch 13 of epoch 141. loss=0.04264893755316734. train batch time cost=0.10219073295593262s\n",
            "completed batch 14 of epoch 141. loss=0.027363469824194908. train batch time cost=0.1022183895111084s\n",
            "completed batch 15 of epoch 141. loss=0.015127163380384445. train batch time cost=0.09582233428955078s\n",
            "completed batch 16 of epoch 141. loss=0.06849242001771927. train batch time cost=0.09571099281311035s\n",
            "completed batch 17 of epoch 141. loss=0.018461739644408226. train batch time cost=0.09479260444641113s\n",
            "completed batch 18 of epoch 141. loss=0.009439726360142231. train batch time cost=0.1015479564666748s\n",
            "completed batch 19 of epoch 141. loss=0.010821356438100338. train batch time cost=0.10210204124450684s\n",
            "completed batch 20 of epoch 141. loss=0.02088080160319805. train batch time cost=0.10085248947143555s\n",
            "completed batch 21 of epoch 141. loss=0.023027904331684113. train batch time cost=0.1009066104888916s\n",
            "completed batch 22 of epoch 141. loss=0.03927801176905632. train batch time cost=0.0969395637512207s\n",
            "completed batch 23 of epoch 141. loss=0.007514366414397955. train batch time cost=0.10281586647033691s\n",
            "completed batch 24 of epoch 141. loss=0.005740941036492586. train batch time cost=0.1013178825378418s\n",
            "completed batch 25 of epoch 141. loss=0.06813342869281769. train batch time cost=0.10280847549438477s\n",
            "completed batch 26 of epoch 141. loss=0.09011750668287277. train batch time cost=0.1018972396850586s\n",
            "completed batch 27 of epoch 141. loss=0.012927547097206116. train batch time cost=0.10100603103637695s\n",
            "completed batch 28 of epoch 141. loss=0.0350191704928875. train batch time cost=0.10127568244934082s\n",
            "completed batch 29 of epoch 141. loss=0.004035424906760454. train batch time cost=0.1008455753326416s\n",
            "completed batch 30 of epoch 141. loss=0.007947788573801517. train batch time cost=0.10086417198181152s\n",
            "completed batch 31 of epoch 141. loss=0.04131269454956055. train batch time cost=0.10034632682800293s\n",
            "completed batch 32 of epoch 141. loss=0.006054388824850321. train batch time cost=0.1014556884765625s\n",
            "completed batch 33 of epoch 141. loss=0.06476950645446777. train batch time cost=0.10232305526733398s\n",
            "completed batch 34 of epoch 141. loss=0.03575202450156212. train batch time cost=0.10334563255310059s\n",
            "completed batch 35 of epoch 141. loss=0.006487633101642132. train batch time cost=0.10212826728820801s\n",
            "completed batch 36 of epoch 141. loss=0.015265804715454578. train batch time cost=0.10132741928100586s\n",
            "completed batch 37 of epoch 141. loss=0.03146105259656906. train batch time cost=0.10354185104370117s\n",
            "completed batch 38 of epoch 141. loss=0.07139661908149719. train batch time cost=0.10234904289245605s\n",
            "completed batch 39 of epoch 141. loss=0.05507565289735794. train batch time cost=0.10234427452087402s\n",
            "completed batch 40 of epoch 141. loss=0.019887031987309456. train batch time cost=0.10208892822265625s\n",
            "completed batch 41 of epoch 141. loss=0.03864424303174019. train batch time cost=0.10185027122497559s\n",
            "completed batch 42 of epoch 141. loss=0.09275157004594803. train batch time cost=0.10095500946044922s\n",
            "completed batch 43 of epoch 141. loss=0.027561962604522705. train batch time cost=0.10013055801391602s\n",
            "completed batch 44 of epoch 141. loss=0.00881254579871893. train batch time cost=0.09854006767272949s\n",
            "completed batch 45 of epoch 141. loss=0.010567398741841316. train batch time cost=0.1020207405090332s\n",
            "completed batch 46 of epoch 141. loss=0.029597952961921692. train batch time cost=0.0995798110961914s\n",
            "completed batch 47 of epoch 141. loss=0.015927357599139214. train batch time cost=0.10193705558776855s\n",
            "completed batch 48 of epoch 141. loss=0.10218143463134766. train batch time cost=0.10283064842224121s\n",
            "completed batch 49 of epoch 141. loss=0.020960472524166107. train batch time cost=0.10163712501525879s\n",
            "completed batch 50 of epoch 141. loss=0.011754349805414677. train batch time cost=0.10337185859680176s\n",
            "completed batch 51 of epoch 141. loss=0.002262894529849291. train batch time cost=0.10214924812316895s\n",
            "completed batch 52 of epoch 141. loss=0.014625861309468746. train batch time cost=0.10138440132141113s\n",
            "completed batch 53 of epoch 141. loss=0.13836778700351715. train batch time cost=0.10159492492675781s\n",
            "completed batch 54 of epoch 141. loss=0.03067893162369728. train batch time cost=0.10206413269042969s\n",
            "completed batch 55 of epoch 141. loss=0.06630785763263702. train batch time cost=0.10204601287841797s\n",
            "completed batch 56 of epoch 141. loss=0.009257310070097446. train batch time cost=0.1011040210723877s\n",
            "completed batch 57 of epoch 141. loss=0.0033572204411029816. train batch time cost=0.10052776336669922s\n",
            "completed batch 58 of epoch 141. loss=0.027576232329010963. train batch time cost=0.10042285919189453s\n",
            "completed batch 59 of epoch 141. loss=0.02429608255624771. train batch time cost=0.09987378120422363s\n",
            "completed batch 60 of epoch 141. loss=0.07322852313518524. train batch time cost=0.10078811645507812s\n",
            "completed batch 61 of epoch 141. loss=0.017119623720645905. train batch time cost=0.10040760040283203s\n",
            "completed batch 62 of epoch 141. loss=0.013634253293275833. train batch time cost=0.10089731216430664s\n",
            "completed batch 63 of epoch 141. loss=0.058148954063653946. train batch time cost=0.1000823974609375s\n",
            "completed batch 64 of epoch 141. loss=0.04450496658682823. train batch time cost=0.10190105438232422s\n",
            "completed batch 65 of epoch 141. loss=0.0037585932295769453. train batch time cost=0.10197687149047852s\n",
            "completed batch 66 of epoch 141. loss=0.00736212357878685. train batch time cost=0.10126733779907227s\n",
            "completed batch 67 of epoch 141. loss=0.014117072336375713. train batch time cost=0.10173416137695312s\n",
            "completed batch 68 of epoch 141. loss=0.0067087141796946526. train batch time cost=0.10052967071533203s\n",
            "completed batch 69 of epoch 141. loss=0.015417019836604595. train batch time cost=0.10076189041137695s\n",
            "completed batch 70 of epoch 141. loss=0.0064629437401890755. train batch time cost=0.1022188663482666s\n",
            "completed batch 71 of epoch 141. loss=0.030027782544493675. train batch time cost=0.10132074356079102s\n",
            "completed batch 72 of epoch 141. loss=0.021734869107604027. train batch time cost=0.10098814964294434s\n",
            "completed batch 73 of epoch 141. loss=0.00909552350640297. train batch time cost=0.10096549987792969s\n",
            "completed batch 74 of epoch 141. loss=0.018287956714630127. train batch time cost=0.10238933563232422s\n",
            "completed batch 75 of epoch 141. loss=0.021801091730594635. train batch time cost=0.1039581298828125s\n",
            "completed batch 76 of epoch 141. loss=0.0034644778352230787. train batch time cost=0.1061406135559082s\n",
            "completed batch 77 of epoch 141. loss=0.06485714018344879. train batch time cost=0.10210776329040527s\n",
            "completed batch 78 of epoch 141. loss=0.01059127226471901. train batch time cost=0.10172462463378906s\n",
            "completed batch 79 of epoch 141. loss=0.031022250652313232. train batch time cost=0.1022188663482666s\n",
            "completed batch 80 of epoch 141. loss=0.0052791135385632515. train batch time cost=0.10288739204406738s\n",
            "completed batch 81 of epoch 141. loss=0.003935395739972591. train batch time cost=0.10213613510131836s\n",
            "completed batch 82 of epoch 141. loss=0.0031202281825244427. train batch time cost=0.10231208801269531s\n",
            "completed batch 83 of epoch 141. loss=0.016946345567703247. train batch time cost=0.1027524471282959s\n",
            "completed batch 84 of epoch 141. loss=0.012282717041671276. train batch time cost=0.10329413414001465s\n",
            "completed batch 85 of epoch 141. loss=0.009222549386322498. train batch time cost=0.10261201858520508s\n",
            "completed batch 86 of epoch 141. loss=0.027691714465618134. train batch time cost=0.10149645805358887s\n",
            "completed batch 87 of epoch 141. loss=0.011432190425693989. train batch time cost=0.10222077369689941s\n",
            "completed batch 88 of epoch 141. loss=0.010829717852175236. train batch time cost=0.10179567337036133s\n",
            "completed batch 89 of epoch 141. loss=0.009538337588310242. train batch time cost=0.10251092910766602s\n",
            "completed batch 90 of epoch 141. loss=0.011742552742362022. train batch time cost=0.10190820693969727s\n",
            "completed batch 91 of epoch 141. loss=0.042079415172338486. train batch time cost=0.10296273231506348s\n",
            "completed batch 92 of epoch 141. loss=0.001596903777681291. train batch time cost=0.09585785865783691s\n",
            "completed batch 93 of epoch 141. loss=0.004653754644095898. train batch time cost=0.0955209732055664s\n",
            "completed batch 94 of epoch 141. loss=0.012947744689881802. train batch time cost=0.09544682502746582s\n",
            "completed batch 95 of epoch 141. loss=0.005149468779563904. train batch time cost=0.10618448257446289s\n",
            "completed batch 96 of epoch 141. loss=0.024477533996105194. train batch time cost=0.10175442695617676s\n",
            "completed batch 97 of epoch 141. loss=0.0048729232512414455. train batch time cost=0.10106992721557617s\n",
            "completed batch 98 of epoch 141. loss=0.014795203693211079. train batch time cost=0.10222983360290527s\n",
            "completed batch 99 of epoch 141. loss=0.01218347530812025. train batch time cost=0.10279059410095215s\n",
            "completed batch 100 of epoch 141. loss=0.06902933865785599. train batch time cost=0.10102558135986328s\n",
            "completed batch 101 of epoch 141. loss=0.0035270974040031433. train batch time cost=0.10401678085327148s\n",
            "completed batch 102 of epoch 141. loss=0.07384321093559265. train batch time cost=0.10473012924194336s\n",
            "completed batch 103 of epoch 141. loss=0.004478985443711281. train batch time cost=0.1031808853149414s\n",
            "completed batch 104 of epoch 141. loss=0.0010826581856235862. train batch time cost=0.11485052108764648s\n",
            "completed batch 105 of epoch 141. loss=0.006392012350261211. train batch time cost=0.10264396667480469s\n",
            "completed batch 106 of epoch 141. loss=0.013960705138742924. train batch time cost=0.10315060615539551s\n",
            "completed batch 107 of epoch 141. loss=0.05597051605582237. train batch time cost=0.10258913040161133s\n",
            "completed batch 108 of epoch 141. loss=0.012582720257341862. train batch time cost=0.10254836082458496s\n",
            "completed batch 109 of epoch 141. loss=0.001862611505202949. train batch time cost=0.10190987586975098s\n",
            "completed batch 110 of epoch 141. loss=0.006357328034937382. train batch time cost=0.10280132293701172s\n",
            "completed batch 111 of epoch 141. loss=0.024555187672376633. train batch time cost=0.10255742073059082s\n",
            "completed batch 112 of epoch 141. loss=0.0805877074599266. train batch time cost=0.1028740406036377s\n",
            "completed batch 113 of epoch 141. loss=0.0509735532104969. train batch time cost=0.10686182975769043s\n",
            "completed batch 114 of epoch 141. loss=0.009691399522125721. train batch time cost=0.10117554664611816s\n",
            "completed batch 115 of epoch 141. loss=0.011439325287938118. train batch time cost=0.1006922721862793s\n",
            "completed batch 116 of epoch 141. loss=0.020652171224355698. train batch time cost=0.10057830810546875s\n",
            "completed batch 117 of epoch 141. loss=0.01386904064565897. train batch time cost=0.10083842277526855s\n",
            "completed batch 118 of epoch 141. loss=0.005673225969076157. train batch time cost=0.10303473472595215s\n",
            "completed batch 119 of epoch 141. loss=0.009862823411822319. train batch time cost=0.10139822959899902s\n",
            "completed batch 120 of epoch 141. loss=0.018472785130143166. train batch time cost=0.10045194625854492s\n",
            "completed batch 121 of epoch 141. loss=0.26970940828323364. train batch time cost=0.10107731819152832s\n",
            "completed batch 122 of epoch 141. loss=0.07089735567569733. train batch time cost=0.10100293159484863s\n",
            "completed batch 123 of epoch 141. loss=0.029630569741129875. train batch time cost=0.10164499282836914s\n",
            "completed batch 124 of epoch 141. loss=0.16328507661819458. train batch time cost=0.10052061080932617s\n",
            "completed batch 125 of epoch 141. loss=0.008649473078548908. train batch time cost=0.10122442245483398s\n",
            "completed batch 126 of epoch 141. loss=4.559686658467399e-06. train batch time cost=0.02929067611694336s\n",
            "completed test of epoch 141. loss=4.559686658467399e-06. accuracy=0.655516724912631. train one epoch time cost=27.604153156280518s, test validation time cost=3.812046527862549\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835, 0.655516724912631]\n",
            "completed batch 1 of epoch 142. loss=0.022990118712186813. train batch time cost=0.10078859329223633s\n",
            "completed batch 2 of epoch 142. loss=0.08367723971605301. train batch time cost=0.10317111015319824s\n",
            "completed batch 3 of epoch 142. loss=0.033456951379776. train batch time cost=0.10095715522766113s\n",
            "completed batch 4 of epoch 142. loss=0.04102117940783501. train batch time cost=0.10147500038146973s\n",
            "completed batch 5 of epoch 142. loss=0.015815632417798042. train batch time cost=0.10174012184143066s\n",
            "completed batch 6 of epoch 142. loss=0.12125235795974731. train batch time cost=0.10277843475341797s\n",
            "completed batch 7 of epoch 142. loss=0.06058567762374878. train batch time cost=0.10216927528381348s\n",
            "completed batch 8 of epoch 142. loss=0.0519857220351696. train batch time cost=0.10389137268066406s\n",
            "completed batch 9 of epoch 142. loss=0.1788557469844818. train batch time cost=0.10309576988220215s\n",
            "completed batch 10 of epoch 142. loss=0.09157724678516388. train batch time cost=0.10160040855407715s\n",
            "completed batch 11 of epoch 142. loss=0.07862618565559387. train batch time cost=0.10186982154846191s\n",
            "completed batch 12 of epoch 142. loss=0.019697124138474464. train batch time cost=0.10195040702819824s\n",
            "completed batch 13 of epoch 142. loss=0.07019686698913574. train batch time cost=0.1034538745880127s\n",
            "completed batch 14 of epoch 142. loss=0.11781715601682663. train batch time cost=0.10278630256652832s\n",
            "completed batch 15 of epoch 142. loss=0.07413973659276962. train batch time cost=0.10401463508605957s\n",
            "completed batch 16 of epoch 142. loss=0.02749522775411606. train batch time cost=0.10300564765930176s\n",
            "completed batch 17 of epoch 142. loss=0.06429214030504227. train batch time cost=0.10202383995056152s\n",
            "completed batch 18 of epoch 142. loss=0.1685696542263031. train batch time cost=0.10208511352539062s\n",
            "completed batch 19 of epoch 142. loss=0.024661235511302948. train batch time cost=0.1026163101196289s\n",
            "completed batch 20 of epoch 142. loss=0.008675802499055862. train batch time cost=0.10157442092895508s\n",
            "completed batch 21 of epoch 142. loss=0.23273536562919617. train batch time cost=0.10228252410888672s\n",
            "completed batch 22 of epoch 142. loss=0.13450054824352264. train batch time cost=0.10045218467712402s\n",
            "completed batch 23 of epoch 142. loss=0.28970587253570557. train batch time cost=0.10079121589660645s\n",
            "completed batch 24 of epoch 142. loss=0.06399740278720856. train batch time cost=0.09598898887634277s\n",
            "completed batch 25 of epoch 142. loss=0.07431677728891373. train batch time cost=0.09494352340698242s\n",
            "completed batch 26 of epoch 142. loss=0.04409640654921532. train batch time cost=0.0943138599395752s\n",
            "completed batch 27 of epoch 142. loss=0.09075111895799637. train batch time cost=0.09484457969665527s\n",
            "completed batch 28 of epoch 142. loss=0.019417786970734596. train batch time cost=0.09452152252197266s\n",
            "completed batch 29 of epoch 142. loss=0.028044376522302628. train batch time cost=0.09480071067810059s\n",
            "completed batch 30 of epoch 142. loss=0.0397568941116333. train batch time cost=0.09413814544677734s\n",
            "completed batch 31 of epoch 142. loss=0.0859479159116745. train batch time cost=0.0944981575012207s\n",
            "completed batch 32 of epoch 142. loss=0.047703713178634644. train batch time cost=0.09502243995666504s\n",
            "completed batch 33 of epoch 142. loss=0.18737198412418365. train batch time cost=0.09492874145507812s\n",
            "completed batch 34 of epoch 142. loss=0.17951060831546783. train batch time cost=0.09469270706176758s\n",
            "completed batch 35 of epoch 142. loss=0.1152089387178421. train batch time cost=0.09479761123657227s\n",
            "completed batch 36 of epoch 142. loss=0.19545114040374756. train batch time cost=0.09541511535644531s\n",
            "completed batch 37 of epoch 142. loss=0.10445111244916916. train batch time cost=0.09614205360412598s\n",
            "completed batch 38 of epoch 142. loss=0.01866949535906315. train batch time cost=0.09505033493041992s\n",
            "completed batch 39 of epoch 142. loss=0.0876227542757988. train batch time cost=0.0956568717956543s\n",
            "completed batch 40 of epoch 142. loss=0.0397891029715538. train batch time cost=0.09814882278442383s\n",
            "completed batch 41 of epoch 142. loss=0.08852151036262512. train batch time cost=0.09505558013916016s\n",
            "completed batch 42 of epoch 142. loss=0.014890847727656364. train batch time cost=0.09624886512756348s\n",
            "completed batch 43 of epoch 142. loss=0.07609381526708603. train batch time cost=0.09549856185913086s\n",
            "completed batch 44 of epoch 142. loss=0.04047084599733353. train batch time cost=0.09649085998535156s\n",
            "completed batch 45 of epoch 142. loss=0.08786484599113464. train batch time cost=0.09546828269958496s\n",
            "completed batch 46 of epoch 142. loss=0.011190609075129032. train batch time cost=0.09502983093261719s\n",
            "completed batch 47 of epoch 142. loss=0.15547645092010498. train batch time cost=0.09598994255065918s\n",
            "completed batch 48 of epoch 142. loss=0.07011020183563232. train batch time cost=0.09633970260620117s\n",
            "completed batch 49 of epoch 142. loss=0.08854067325592041. train batch time cost=0.09506583213806152s\n",
            "completed batch 50 of epoch 142. loss=0.2191009670495987. train batch time cost=0.0944223403930664s\n",
            "completed batch 51 of epoch 142. loss=0.01928536780178547. train batch time cost=0.0956871509552002s\n",
            "completed batch 52 of epoch 142. loss=0.05206916481256485. train batch time cost=0.09618806838989258s\n",
            "completed batch 53 of epoch 142. loss=0.2559351623058319. train batch time cost=0.09463691711425781s\n",
            "completed batch 54 of epoch 142. loss=0.04581538587808609. train batch time cost=0.09416484832763672s\n",
            "completed batch 55 of epoch 142. loss=0.16033588349819183. train batch time cost=0.09524250030517578s\n",
            "completed batch 56 of epoch 142. loss=0.0732358917593956. train batch time cost=0.09505128860473633s\n",
            "completed batch 57 of epoch 142. loss=0.06281985342502594. train batch time cost=0.10150432586669922s\n",
            "completed batch 58 of epoch 142. loss=0.0357341542840004. train batch time cost=0.10138106346130371s\n",
            "completed batch 59 of epoch 142. loss=0.05929214879870415. train batch time cost=0.1020512580871582s\n",
            "completed batch 60 of epoch 142. loss=0.06766168028116226. train batch time cost=0.10313916206359863s\n",
            "completed batch 61 of epoch 142. loss=0.036177583038806915. train batch time cost=0.09934878349304199s\n",
            "completed batch 62 of epoch 142. loss=0.09908705949783325. train batch time cost=0.09615135192871094s\n",
            "completed batch 63 of epoch 142. loss=0.09492208063602448. train batch time cost=0.09741520881652832s\n",
            "completed batch 64 of epoch 142. loss=0.04779466986656189. train batch time cost=0.09503579139709473s\n",
            "completed batch 65 of epoch 142. loss=0.04753176495432854. train batch time cost=0.10077691078186035s\n",
            "completed batch 66 of epoch 142. loss=0.017383825033903122. train batch time cost=0.10198187828063965s\n",
            "completed batch 67 of epoch 142. loss=0.18465755879878998. train batch time cost=0.10094642639160156s\n",
            "completed batch 68 of epoch 142. loss=0.1362951397895813. train batch time cost=0.10171246528625488s\n",
            "completed batch 69 of epoch 142. loss=0.027695707976818085. train batch time cost=0.10530328750610352s\n",
            "completed batch 70 of epoch 142. loss=0.06621812283992767. train batch time cost=0.10076451301574707s\n",
            "completed batch 71 of epoch 142. loss=0.2035304754972458. train batch time cost=0.10102200508117676s\n",
            "completed batch 72 of epoch 142. loss=0.035928551107645035. train batch time cost=0.10252499580383301s\n",
            "completed batch 73 of epoch 142. loss=0.08298923075199127. train batch time cost=0.1017599105834961s\n",
            "completed batch 74 of epoch 142. loss=0.09380415827035904. train batch time cost=0.10469222068786621s\n",
            "completed batch 75 of epoch 142. loss=0.22861823439598083. train batch time cost=0.10304617881774902s\n",
            "completed batch 76 of epoch 142. loss=0.14203502237796783. train batch time cost=0.10438799858093262s\n",
            "completed batch 77 of epoch 142. loss=0.06974206119775772. train batch time cost=0.10004067420959473s\n",
            "completed batch 78 of epoch 142. loss=0.10212262719869614. train batch time cost=0.10129714012145996s\n",
            "completed batch 79 of epoch 142. loss=0.09814448654651642. train batch time cost=0.10018014907836914s\n",
            "completed batch 80 of epoch 142. loss=0.19343875348567963. train batch time cost=0.10183238983154297s\n",
            "completed batch 81 of epoch 142. loss=0.053497880697250366. train batch time cost=0.10019207000732422s\n",
            "completed batch 82 of epoch 142. loss=0.03160854056477547. train batch time cost=0.1023244857788086s\n",
            "completed batch 83 of epoch 142. loss=0.08138763159513474. train batch time cost=0.10419774055480957s\n",
            "completed batch 84 of epoch 142. loss=0.0424669086933136. train batch time cost=0.10249948501586914s\n",
            "completed batch 85 of epoch 142. loss=0.06966635584831238. train batch time cost=0.10014033317565918s\n",
            "completed batch 86 of epoch 142. loss=0.04204520955681801. train batch time cost=0.09488916397094727s\n",
            "completed batch 87 of epoch 142. loss=0.039061274379491806. train batch time cost=0.09424662590026855s\n",
            "completed batch 88 of epoch 142. loss=0.027764184400439262. train batch time cost=0.09497666358947754s\n",
            "completed batch 89 of epoch 142. loss=0.1058056503534317. train batch time cost=0.09441494941711426s\n",
            "completed batch 90 of epoch 142. loss=0.06766162812709808. train batch time cost=0.09699320793151855s\n",
            "completed batch 91 of epoch 142. loss=0.03672727197408676. train batch time cost=0.09479999542236328s\n",
            "completed batch 92 of epoch 142. loss=0.07227951288223267. train batch time cost=0.09433436393737793s\n",
            "completed batch 93 of epoch 142. loss=0.10243716835975647. train batch time cost=0.09383797645568848s\n",
            "completed batch 94 of epoch 142. loss=0.05086786299943924. train batch time cost=0.09510612487792969s\n",
            "completed batch 95 of epoch 142. loss=0.06331903487443924. train batch time cost=0.09579658508300781s\n",
            "completed batch 96 of epoch 142. loss=0.029039917513728142. train batch time cost=0.0993645191192627s\n",
            "completed batch 97 of epoch 142. loss=0.05265682935714722. train batch time cost=0.09525537490844727s\n",
            "completed batch 98 of epoch 142. loss=0.012319261208176613. train batch time cost=0.09481000900268555s\n",
            "completed batch 99 of epoch 142. loss=0.012186645530164242. train batch time cost=0.09607720375061035s\n",
            "completed batch 100 of epoch 142. loss=0.01623217947781086. train batch time cost=0.09481167793273926s\n",
            "completed batch 101 of epoch 142. loss=0.12535543739795685. train batch time cost=0.09624385833740234s\n",
            "completed batch 102 of epoch 142. loss=0.04567670449614525. train batch time cost=0.0994255542755127s\n",
            "completed batch 103 of epoch 142. loss=0.08097581565380096. train batch time cost=0.0948333740234375s\n",
            "completed batch 104 of epoch 142. loss=0.018299533054232597. train batch time cost=0.09442615509033203s\n",
            "completed batch 105 of epoch 142. loss=0.02406790480017662. train batch time cost=0.09554004669189453s\n",
            "completed batch 106 of epoch 142. loss=0.06806206703186035. train batch time cost=0.09623861312866211s\n",
            "completed batch 107 of epoch 142. loss=0.035570111125707626. train batch time cost=0.09463286399841309s\n",
            "completed batch 108 of epoch 142. loss=0.05096796154975891. train batch time cost=0.0944373607635498s\n",
            "completed batch 109 of epoch 142. loss=0.041216377168893814. train batch time cost=0.09515070915222168s\n",
            "completed batch 110 of epoch 142. loss=0.0032068726141005754. train batch time cost=0.09763312339782715s\n",
            "completed batch 111 of epoch 142. loss=0.05121401324868202. train batch time cost=0.0960843563079834s\n",
            "completed batch 112 of epoch 142. loss=0.13003899157047272. train batch time cost=0.09538865089416504s\n",
            "completed batch 113 of epoch 142. loss=0.023988500237464905. train batch time cost=0.09558296203613281s\n",
            "completed batch 114 of epoch 142. loss=0.05934825539588928. train batch time cost=0.09563016891479492s\n",
            "completed batch 115 of epoch 142. loss=0.019485197961330414. train batch time cost=0.09510922431945801s\n",
            "completed batch 116 of epoch 142. loss=0.14165575802326202. train batch time cost=0.09664654731750488s\n",
            "completed batch 117 of epoch 142. loss=0.028561700135469437. train batch time cost=0.09612107276916504s\n",
            "completed batch 118 of epoch 142. loss=0.022114895284175873. train batch time cost=0.09534358978271484s\n",
            "completed batch 119 of epoch 142. loss=0.013616610318422318. train batch time cost=0.09472036361694336s\n",
            "completed batch 120 of epoch 142. loss=0.11659824848175049. train batch time cost=0.09652304649353027s\n",
            "completed batch 121 of epoch 142. loss=0.03572925925254822. train batch time cost=0.0958101749420166s\n",
            "completed batch 122 of epoch 142. loss=0.019489537924528122. train batch time cost=0.09703397750854492s\n",
            "completed batch 123 of epoch 142. loss=0.05673115700483322. train batch time cost=0.10044193267822266s\n",
            "completed batch 124 of epoch 142. loss=0.053101811558008194. train batch time cost=0.10179424285888672s\n",
            "completed batch 125 of epoch 142. loss=0.026926901191473007. train batch time cost=0.10194039344787598s\n",
            "completed batch 126 of epoch 142. loss=0.001683498383499682. train batch time cost=0.030354738235473633s\n",
            "completed test of epoch 142. loss=0.001683498383499682. accuracy=0.655516724912631. train one epoch time cost=27.21061897277832s, test validation time cost=3.9648337364196777\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835, 0.655516724912631, 0.655516724912631]\n",
            "completed batch 1 of epoch 143. loss=0.03476238623261452. train batch time cost=0.09545779228210449s\n",
            "completed batch 2 of epoch 143. loss=0.2586842477321625. train batch time cost=0.09656476974487305s\n",
            "completed batch 3 of epoch 143. loss=0.08751196414232254. train batch time cost=0.09738755226135254s\n",
            "completed batch 4 of epoch 143. loss=0.02814522199332714. train batch time cost=0.09546065330505371s\n",
            "completed batch 5 of epoch 143. loss=0.21638372540473938. train batch time cost=0.09533238410949707s\n",
            "completed batch 6 of epoch 143. loss=0.038648467510938644. train batch time cost=0.0974571704864502s\n",
            "completed batch 7 of epoch 143. loss=0.01352726574987173. train batch time cost=0.09602713584899902s\n",
            "completed batch 8 of epoch 143. loss=0.0671282485127449. train batch time cost=0.09984922409057617s\n",
            "completed batch 9 of epoch 143. loss=0.04089031368494034. train batch time cost=0.1008613109588623s\n",
            "completed batch 10 of epoch 143. loss=0.008816134184598923. train batch time cost=0.1022024154663086s\n",
            "completed batch 11 of epoch 143. loss=0.09408461302518845. train batch time cost=0.10160565376281738s\n",
            "completed batch 12 of epoch 143. loss=0.11047172546386719. train batch time cost=0.1020655632019043s\n",
            "completed batch 13 of epoch 143. loss=0.0881892517209053. train batch time cost=0.10042548179626465s\n",
            "completed batch 14 of epoch 143. loss=0.08569526672363281. train batch time cost=0.10227131843566895s\n",
            "completed batch 15 of epoch 143. loss=0.125170037150383. train batch time cost=0.11117911338806152s\n",
            "completed batch 16 of epoch 143. loss=0.01716059260070324. train batch time cost=0.10114789009094238s\n",
            "completed batch 17 of epoch 143. loss=0.0906195342540741. train batch time cost=0.1013035774230957s\n",
            "completed batch 18 of epoch 143. loss=0.05634406581521034. train batch time cost=0.10111474990844727s\n",
            "completed batch 19 of epoch 143. loss=0.01985996402800083. train batch time cost=0.1024625301361084s\n",
            "completed batch 20 of epoch 143. loss=0.014157265424728394. train batch time cost=0.10191035270690918s\n",
            "completed batch 21 of epoch 143. loss=0.019861454144120216. train batch time cost=0.10227560997009277s\n",
            "completed batch 22 of epoch 143. loss=0.07833346724510193. train batch time cost=0.10171985626220703s\n",
            "completed batch 23 of epoch 143. loss=0.026741115376353264. train batch time cost=0.10194706916809082s\n",
            "completed batch 24 of epoch 143. loss=0.06445147097110748. train batch time cost=0.10251379013061523s\n",
            "completed batch 25 of epoch 143. loss=0.019368302077054977. train batch time cost=0.10250568389892578s\n",
            "completed batch 26 of epoch 143. loss=0.025067128241062164. train batch time cost=0.1014699935913086s\n",
            "completed batch 27 of epoch 143. loss=0.14762961864471436. train batch time cost=0.10103368759155273s\n",
            "completed batch 28 of epoch 143. loss=0.02955688163638115. train batch time cost=0.1028299331665039s\n",
            "completed batch 29 of epoch 143. loss=0.022684862837195396. train batch time cost=0.10382652282714844s\n",
            "completed batch 30 of epoch 143. loss=0.09078016132116318. train batch time cost=0.10292911529541016s\n",
            "completed batch 31 of epoch 143. loss=0.14421075582504272. train batch time cost=0.10103344917297363s\n",
            "completed batch 32 of epoch 143. loss=0.028681807219982147. train batch time cost=0.1012260913848877s\n",
            "completed batch 33 of epoch 143. loss=0.07129233330488205. train batch time cost=0.10236144065856934s\n",
            "completed batch 34 of epoch 143. loss=0.035545822232961655. train batch time cost=0.10185480117797852s\n",
            "completed batch 35 of epoch 143. loss=0.043160535395145416. train batch time cost=0.09881210327148438s\n",
            "completed batch 36 of epoch 143. loss=0.05975916609168053. train batch time cost=0.09893488883972168s\n",
            "completed batch 37 of epoch 143. loss=0.09893954545259476. train batch time cost=0.10228943824768066s\n",
            "completed batch 38 of epoch 143. loss=0.01100218016654253. train batch time cost=0.10186290740966797s\n",
            "completed batch 39 of epoch 143. loss=0.06500083953142166. train batch time cost=0.10103821754455566s\n",
            "completed batch 40 of epoch 143. loss=0.04572179540991783. train batch time cost=0.10250973701477051s\n",
            "completed batch 41 of epoch 143. loss=0.1658763289451599. train batch time cost=0.10035300254821777s\n",
            "completed batch 42 of epoch 143. loss=0.03363776579499245. train batch time cost=0.10214972496032715s\n",
            "completed batch 43 of epoch 143. loss=0.019408689811825752. train batch time cost=0.1014246940612793s\n",
            "completed batch 44 of epoch 143. loss=0.11222127825021744. train batch time cost=0.10198116302490234s\n",
            "completed batch 45 of epoch 143. loss=0.00856423657387495. train batch time cost=0.0999765396118164s\n",
            "completed batch 46 of epoch 143. loss=0.06507297605276108. train batch time cost=0.10095930099487305s\n",
            "completed batch 47 of epoch 143. loss=0.012712380848824978. train batch time cost=0.09604978561401367s\n",
            "completed batch 48 of epoch 143. loss=0.016656501218676567. train batch time cost=0.09743618965148926s\n",
            "completed batch 49 of epoch 143. loss=0.08529897034168243. train batch time cost=0.09502816200256348s\n",
            "completed batch 50 of epoch 143. loss=0.03277438133955002. train batch time cost=0.09530997276306152s\n",
            "completed batch 51 of epoch 143. loss=0.035853881388902664. train batch time cost=0.09546613693237305s\n",
            "completed batch 52 of epoch 143. loss=0.012394221499562263. train batch time cost=0.09935975074768066s\n",
            "completed batch 53 of epoch 143. loss=0.08605082333087921. train batch time cost=0.09549784660339355s\n",
            "completed batch 54 of epoch 143. loss=0.06752495467662811. train batch time cost=0.09497404098510742s\n",
            "completed batch 55 of epoch 143. loss=0.015112582594156265. train batch time cost=0.096282958984375s\n",
            "completed batch 56 of epoch 143. loss=0.034085713326931. train batch time cost=0.09583091735839844s\n",
            "completed batch 57 of epoch 143. loss=0.018022920936346054. train batch time cost=0.09820747375488281s\n",
            "completed batch 58 of epoch 143. loss=0.09832422435283661. train batch time cost=0.09464478492736816s\n",
            "completed batch 59 of epoch 143. loss=0.053275175392627716. train batch time cost=0.09479141235351562s\n",
            "completed batch 60 of epoch 143. loss=0.14063741266727448. train batch time cost=0.09377884864807129s\n",
            "completed batch 61 of epoch 143. loss=0.003419548273086548. train batch time cost=0.09621858596801758s\n",
            "completed batch 62 of epoch 143. loss=0.06990199536085129. train batch time cost=0.10264778137207031s\n",
            "completed batch 63 of epoch 143. loss=0.039924874901771545. train batch time cost=0.10262084007263184s\n",
            "completed batch 64 of epoch 143. loss=0.047488804906606674. train batch time cost=0.10286974906921387s\n",
            "completed batch 65 of epoch 143. loss=0.02322237752377987. train batch time cost=0.09508824348449707s\n",
            "completed batch 66 of epoch 143. loss=0.06138979271054268. train batch time cost=0.09595370292663574s\n",
            "completed batch 67 of epoch 143. loss=0.03672316297888756. train batch time cost=0.09444427490234375s\n",
            "completed batch 68 of epoch 143. loss=0.02617798000574112. train batch time cost=0.09487295150756836s\n",
            "completed batch 69 of epoch 143. loss=0.03750668466091156. train batch time cost=0.09376692771911621s\n",
            "completed batch 70 of epoch 143. loss=0.013006685301661491. train batch time cost=0.09586811065673828s\n",
            "completed batch 71 of epoch 143. loss=0.019364170730113983. train batch time cost=0.09538888931274414s\n",
            "completed batch 72 of epoch 143. loss=0.009149120189249516. train batch time cost=0.09510445594787598s\n",
            "completed batch 73 of epoch 143. loss=0.026905542239546776. train batch time cost=0.09517955780029297s\n",
            "completed batch 74 of epoch 143. loss=0.020765045657753944. train batch time cost=0.09486770629882812s\n",
            "completed batch 75 of epoch 143. loss=0.0490100122988224. train batch time cost=0.09584736824035645s\n",
            "completed batch 76 of epoch 143. loss=0.012971592135727406. train batch time cost=0.1008749008178711s\n",
            "completed batch 77 of epoch 143. loss=0.04732741788029671. train batch time cost=0.0939335823059082s\n",
            "completed batch 78 of epoch 143. loss=0.03152673318982124. train batch time cost=0.09414005279541016s\n",
            "completed batch 79 of epoch 143. loss=0.04599848762154579. train batch time cost=0.09475874900817871s\n",
            "completed batch 80 of epoch 143. loss=0.024035468697547913. train batch time cost=0.09674310684204102s\n",
            "completed batch 81 of epoch 143. loss=0.05464249849319458. train batch time cost=0.09541201591491699s\n",
            "completed batch 82 of epoch 143. loss=0.038265425711870193. train batch time cost=0.09516406059265137s\n",
            "completed batch 83 of epoch 143. loss=0.07449200749397278. train batch time cost=0.09508919715881348s\n",
            "completed batch 84 of epoch 143. loss=0.04942702129483223. train batch time cost=0.09489655494689941s\n",
            "completed batch 85 of epoch 143. loss=0.05783609673380852. train batch time cost=0.09563899040222168s\n",
            "completed batch 86 of epoch 143. loss=0.016002533957362175. train batch time cost=0.09566211700439453s\n",
            "completed batch 87 of epoch 143. loss=0.01540717575699091. train batch time cost=0.09516119956970215s\n",
            "completed batch 88 of epoch 143. loss=0.020368732511997223. train batch time cost=0.09485983848571777s\n",
            "completed batch 89 of epoch 143. loss=0.015882065519690514. train batch time cost=0.09749889373779297s\n",
            "completed batch 90 of epoch 143. loss=0.036161284893751144. train batch time cost=0.09805846214294434s\n",
            "completed batch 91 of epoch 143. loss=0.03773212432861328. train batch time cost=0.09546828269958496s\n",
            "completed batch 92 of epoch 143. loss=0.03792322799563408. train batch time cost=0.09525156021118164s\n",
            "completed batch 93 of epoch 143. loss=0.04321352392435074. train batch time cost=0.0965278148651123s\n",
            "completed batch 94 of epoch 143. loss=0.017941443249583244. train batch time cost=0.09987711906433105s\n",
            "completed batch 95 of epoch 143. loss=0.028546743094921112. train batch time cost=0.10735440254211426s\n",
            "completed batch 96 of epoch 143. loss=0.044154562056064606. train batch time cost=0.1015477180480957s\n",
            "completed batch 97 of epoch 143. loss=0.0665300115942955. train batch time cost=0.10191845893859863s\n",
            "completed batch 98 of epoch 143. loss=0.07105374336242676. train batch time cost=0.1023244857788086s\n",
            "completed batch 99 of epoch 143. loss=0.09729482978582382. train batch time cost=0.10530352592468262s\n",
            "completed batch 100 of epoch 143. loss=0.06726555526256561. train batch time cost=0.10306143760681152s\n",
            "completed batch 101 of epoch 143. loss=0.017738904803991318. train batch time cost=0.10222935676574707s\n",
            "completed batch 102 of epoch 143. loss=0.035911377519369125. train batch time cost=0.10274410247802734s\n",
            "completed batch 103 of epoch 143. loss=0.005380196031183004. train batch time cost=0.10238885879516602s\n",
            "completed batch 104 of epoch 143. loss=0.0275126863270998. train batch time cost=0.10654354095458984s\n",
            "completed batch 105 of epoch 143. loss=0.04456552118062973. train batch time cost=0.10095524787902832s\n",
            "completed batch 106 of epoch 143. loss=0.22123366594314575. train batch time cost=0.10047483444213867s\n",
            "completed batch 107 of epoch 143. loss=0.10329711437225342. train batch time cost=0.10148096084594727s\n",
            "completed batch 108 of epoch 143. loss=0.006335901562124491. train batch time cost=0.10265088081359863s\n",
            "completed batch 109 of epoch 143. loss=0.059163011610507965. train batch time cost=0.10290884971618652s\n",
            "completed batch 110 of epoch 143. loss=0.013343233615159988. train batch time cost=0.1021728515625s\n",
            "completed batch 111 of epoch 143. loss=0.020670007914304733. train batch time cost=0.09518933296203613s\n",
            "completed batch 112 of epoch 143. loss=0.03989623859524727. train batch time cost=0.09665727615356445s\n",
            "completed batch 113 of epoch 143. loss=0.11813227832317352. train batch time cost=0.09634995460510254s\n",
            "completed batch 114 of epoch 143. loss=0.0041679441928863525. train batch time cost=0.09755730628967285s\n",
            "completed batch 115 of epoch 143. loss=0.00828794576227665. train batch time cost=0.09545731544494629s\n",
            "completed batch 116 of epoch 143. loss=0.11388777941465378. train batch time cost=0.09579038619995117s\n",
            "completed batch 117 of epoch 143. loss=0.08894375711679459. train batch time cost=0.0985720157623291s\n",
            "completed batch 118 of epoch 143. loss=0.0216838326305151. train batch time cost=0.10332179069519043s\n",
            "completed batch 119 of epoch 143. loss=0.18296562135219574. train batch time cost=0.10330462455749512s\n",
            "completed batch 120 of epoch 143. loss=0.009601989760994911. train batch time cost=0.10259437561035156s\n",
            "completed batch 121 of epoch 143. loss=0.009779036976397038. train batch time cost=0.10354089736938477s\n",
            "completed batch 122 of epoch 143. loss=0.02049650251865387. train batch time cost=0.10293149948120117s\n",
            "completed batch 123 of epoch 143. loss=0.016318434849381447. train batch time cost=0.10155749320983887s\n",
            "completed batch 124 of epoch 143. loss=0.0324426107108593. train batch time cost=0.1018826961517334s\n",
            "completed batch 125 of epoch 143. loss=0.021317066624760628. train batch time cost=0.1030271053314209s\n",
            "completed batch 126 of epoch 143. loss=0.0026602856814861298. train batch time cost=0.03253507614135742s\n",
            "completed test of epoch 143. loss=0.0026602856814861298. accuracy=0.6984523215177234. train one epoch time cost=27.34015130996704s, test validation time cost=3.840127468109131\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835, 0.655516724912631, 0.655516724912631, 0.6984523215177234]\n",
            "completed batch 1 of epoch 144. loss=0.05368269234895706. train batch time cost=0.09670805931091309s\n",
            "completed batch 2 of epoch 144. loss=0.013940487056970596. train batch time cost=0.09755110740661621s\n",
            "completed batch 3 of epoch 144. loss=0.03364073857665062. train batch time cost=0.09583592414855957s\n",
            "completed batch 4 of epoch 144. loss=0.03583540767431259. train batch time cost=0.0955512523651123s\n",
            "completed batch 5 of epoch 144. loss=0.09721818566322327. train batch time cost=0.09522199630737305s\n",
            "completed batch 6 of epoch 144. loss=0.017384760081768036. train batch time cost=0.09645819664001465s\n",
            "completed batch 7 of epoch 144. loss=0.01998659037053585. train batch time cost=0.10558533668518066s\n",
            "completed batch 8 of epoch 144. loss=0.004493932239711285. train batch time cost=0.10240864753723145s\n",
            "completed batch 9 of epoch 144. loss=0.07209522277116776. train batch time cost=0.1031494140625s\n",
            "completed batch 10 of epoch 144. loss=0.012713311240077019. train batch time cost=0.10439205169677734s\n",
            "completed batch 11 of epoch 144. loss=0.06728199869394302. train batch time cost=0.10326266288757324s\n",
            "completed batch 12 of epoch 144. loss=0.020361924543976784. train batch time cost=0.10259103775024414s\n",
            "completed batch 13 of epoch 144. loss=0.20449307560920715. train batch time cost=0.10134410858154297s\n",
            "completed batch 14 of epoch 144. loss=0.03961917385458946. train batch time cost=0.10216474533081055s\n",
            "completed batch 15 of epoch 144. loss=0.05009571835398674. train batch time cost=0.1012873649597168s\n",
            "completed batch 16 of epoch 144. loss=0.02221040241420269. train batch time cost=0.10464119911193848s\n",
            "completed batch 17 of epoch 144. loss=0.03321514651179314. train batch time cost=0.1037900447845459s\n",
            "completed batch 18 of epoch 144. loss=0.01508319191634655. train batch time cost=0.10055422782897949s\n",
            "completed batch 19 of epoch 144. loss=0.01284455880522728. train batch time cost=0.10033798217773438s\n",
            "completed batch 20 of epoch 144. loss=0.09571503102779388. train batch time cost=0.09583330154418945s\n",
            "completed batch 21 of epoch 144. loss=0.04637042433023453. train batch time cost=0.10152411460876465s\n",
            "completed batch 22 of epoch 144. loss=0.14372935891151428. train batch time cost=0.10114097595214844s\n",
            "completed batch 23 of epoch 144. loss=0.010773248039186. train batch time cost=0.10110163688659668s\n",
            "completed batch 24 of epoch 144. loss=0.06455203145742416. train batch time cost=0.10259318351745605s\n",
            "completed batch 25 of epoch 144. loss=0.060834143310785294. train batch time cost=0.10117435455322266s\n",
            "completed batch 26 of epoch 144. loss=0.011569257825613022. train batch time cost=0.10124588012695312s\n",
            "completed batch 27 of epoch 144. loss=0.009083712473511696. train batch time cost=0.10136127471923828s\n",
            "completed batch 28 of epoch 144. loss=0.018967868760228157. train batch time cost=0.10092449188232422s\n",
            "completed batch 29 of epoch 144. loss=0.009513208642601967. train batch time cost=0.10296225547790527s\n",
            "completed batch 30 of epoch 144. loss=0.020109299570322037. train batch time cost=0.10189175605773926s\n",
            "completed batch 31 of epoch 144. loss=0.1080072894692421. train batch time cost=0.10102200508117676s\n",
            "completed batch 32 of epoch 144. loss=0.10840561240911484. train batch time cost=0.10062766075134277s\n",
            "completed batch 33 of epoch 144. loss=0.016841333359479904. train batch time cost=0.09715795516967773s\n",
            "completed batch 34 of epoch 144. loss=0.029215171933174133. train batch time cost=0.09637665748596191s\n",
            "completed batch 35 of epoch 144. loss=0.007147540338337421. train batch time cost=0.0953373908996582s\n",
            "completed batch 36 of epoch 144. loss=0.009844380430877209. train batch time cost=0.09617376327514648s\n",
            "completed batch 37 of epoch 144. loss=0.020052704960107803. train batch time cost=0.09490489959716797s\n",
            "completed batch 38 of epoch 144. loss=0.02038000524044037. train batch time cost=0.09599709510803223s\n",
            "completed batch 39 of epoch 144. loss=0.14552536606788635. train batch time cost=0.09661126136779785s\n",
            "completed batch 40 of epoch 144. loss=0.05659973621368408. train batch time cost=0.0958244800567627s\n",
            "completed batch 41 of epoch 144. loss=0.06041907146573067. train batch time cost=0.09544563293457031s\n",
            "completed batch 42 of epoch 144. loss=0.035840850323438644. train batch time cost=0.09557127952575684s\n",
            "completed batch 43 of epoch 144. loss=0.006955526769161224. train batch time cost=0.09508037567138672s\n",
            "completed batch 44 of epoch 144. loss=0.053873855620622635. train batch time cost=0.09813809394836426s\n",
            "completed batch 45 of epoch 144. loss=0.03572136163711548. train batch time cost=0.09579229354858398s\n",
            "completed batch 46 of epoch 144. loss=0.008757960051298141. train batch time cost=0.10308527946472168s\n",
            "completed batch 47 of epoch 144. loss=0.014196498319506645. train batch time cost=0.1027684211730957s\n",
            "completed batch 48 of epoch 144. loss=0.10761339962482452. train batch time cost=0.10246157646179199s\n",
            "completed batch 49 of epoch 144. loss=0.013720427639782429. train batch time cost=0.10228824615478516s\n",
            "completed batch 50 of epoch 144. loss=0.03527675196528435. train batch time cost=0.10165810585021973s\n",
            "completed batch 51 of epoch 144. loss=0.012355495244264603. train batch time cost=0.10372304916381836s\n",
            "completed batch 52 of epoch 144. loss=0.03739960119128227. train batch time cost=0.10280227661132812s\n",
            "completed batch 53 of epoch 144. loss=0.012324287556111813. train batch time cost=0.10298705101013184s\n",
            "completed batch 54 of epoch 144. loss=0.058453213423490524. train batch time cost=0.10268783569335938s\n",
            "completed batch 55 of epoch 144. loss=0.07447923719882965. train batch time cost=0.10264778137207031s\n",
            "completed batch 56 of epoch 144. loss=0.04919150099158287. train batch time cost=0.1026458740234375s\n",
            "completed batch 57 of epoch 144. loss=0.023821260780096054. train batch time cost=0.10270881652832031s\n",
            "completed batch 58 of epoch 144. loss=0.027560319751501083. train batch time cost=0.10248327255249023s\n",
            "completed batch 59 of epoch 144. loss=0.04536368325352669. train batch time cost=0.10251450538635254s\n",
            "completed batch 60 of epoch 144. loss=0.04817500710487366. train batch time cost=0.1031029224395752s\n",
            "completed batch 61 of epoch 144. loss=0.02450641803443432. train batch time cost=0.10205626487731934s\n",
            "completed batch 62 of epoch 144. loss=0.03498589247465134. train batch time cost=0.10275745391845703s\n",
            "completed batch 63 of epoch 144. loss=0.029835833236575127. train batch time cost=0.10534119606018066s\n",
            "completed batch 64 of epoch 144. loss=0.008600925095379353. train batch time cost=0.09854602813720703s\n",
            "completed batch 65 of epoch 144. loss=0.01761138066649437. train batch time cost=0.10248517990112305s\n",
            "completed batch 66 of epoch 144. loss=0.034675903618335724. train batch time cost=0.09965801239013672s\n",
            "completed batch 67 of epoch 144. loss=0.0032286939676851034. train batch time cost=0.10028839111328125s\n",
            "completed batch 68 of epoch 144. loss=0.010568270459771156. train batch time cost=0.09975099563598633s\n",
            "completed batch 69 of epoch 144. loss=0.007437116000801325. train batch time cost=0.09826207160949707s\n",
            "completed batch 70 of epoch 144. loss=0.03137601166963577. train batch time cost=0.10286903381347656s\n",
            "completed batch 71 of epoch 144. loss=0.01084041502326727. train batch time cost=0.10256123542785645s\n",
            "completed batch 72 of epoch 144. loss=0.06046653538942337. train batch time cost=0.10255551338195801s\n",
            "completed batch 73 of epoch 144. loss=0.015027637593448162. train batch time cost=0.10264182090759277s\n",
            "completed batch 74 of epoch 144. loss=0.10355433821678162. train batch time cost=0.10255193710327148s\n",
            "completed batch 75 of epoch 144. loss=0.04706909880042076. train batch time cost=0.10424518585205078s\n",
            "completed batch 76 of epoch 144. loss=0.023875782266259193. train batch time cost=0.10324549674987793s\n",
            "completed batch 77 of epoch 144. loss=0.017188111320137978. train batch time cost=0.09508180618286133s\n",
            "completed batch 78 of epoch 144. loss=0.00831835251301527. train batch time cost=0.10118246078491211s\n",
            "completed batch 79 of epoch 144. loss=0.022202562540769577. train batch time cost=0.10282468795776367s\n",
            "completed batch 80 of epoch 144. loss=0.023798974230885506. train batch time cost=0.10128307342529297s\n",
            "completed batch 81 of epoch 144. loss=0.037602748721838. train batch time cost=0.10103821754455566s\n",
            "completed batch 82 of epoch 144. loss=0.00713538471609354. train batch time cost=0.10062098503112793s\n",
            "completed batch 83 of epoch 144. loss=0.11500507593154907. train batch time cost=0.10136532783508301s\n",
            "completed batch 84 of epoch 144. loss=0.0028437708970159292. train batch time cost=0.10235333442687988s\n",
            "completed batch 85 of epoch 144. loss=0.01703549735248089. train batch time cost=0.10202836990356445s\n",
            "completed batch 86 of epoch 144. loss=0.005518524907529354. train batch time cost=0.10196352005004883s\n",
            "completed batch 87 of epoch 144. loss=0.030633166432380676. train batch time cost=0.10010433197021484s\n",
            "completed batch 88 of epoch 144. loss=0.019807273522019386. train batch time cost=0.10111045837402344s\n",
            "completed batch 89 of epoch 144. loss=0.023958848789334297. train batch time cost=0.10317826271057129s\n",
            "completed batch 90 of epoch 144. loss=0.3280816972255707. train batch time cost=0.10234689712524414s\n",
            "completed batch 91 of epoch 144. loss=0.004115191753953695. train batch time cost=0.10188961029052734s\n",
            "completed batch 92 of epoch 144. loss=0.004733180161565542. train batch time cost=0.10352826118469238s\n",
            "completed batch 93 of epoch 144. loss=0.015055157244205475. train batch time cost=0.10178661346435547s\n",
            "completed batch 94 of epoch 144. loss=0.0052521713078022. train batch time cost=0.10253214836120605s\n",
            "completed batch 95 of epoch 144. loss=0.01045206654816866. train batch time cost=0.10172510147094727s\n",
            "completed batch 96 of epoch 144. loss=0.011427123099565506. train batch time cost=0.10045552253723145s\n",
            "completed batch 97 of epoch 144. loss=0.021137040108442307. train batch time cost=0.10282254219055176s\n",
            "completed batch 98 of epoch 144. loss=0.0214835274964571. train batch time cost=0.10206317901611328s\n",
            "completed batch 99 of epoch 144. loss=0.009874200448393822. train batch time cost=0.10060262680053711s\n",
            "completed batch 100 of epoch 144. loss=0.007008886896073818. train batch time cost=0.10234832763671875s\n",
            "completed batch 101 of epoch 144. loss=0.15547898411750793. train batch time cost=0.10121750831604004s\n",
            "completed batch 102 of epoch 144. loss=0.03575128689408302. train batch time cost=0.10155034065246582s\n",
            "completed batch 103 of epoch 144. loss=0.029559612274169922. train batch time cost=0.10289955139160156s\n",
            "completed batch 104 of epoch 144. loss=0.03396323323249817. train batch time cost=0.10214066505432129s\n",
            "completed batch 105 of epoch 144. loss=0.03001784346997738. train batch time cost=0.10199284553527832s\n",
            "completed batch 106 of epoch 144. loss=0.013166244141757488. train batch time cost=0.10185933113098145s\n",
            "completed batch 107 of epoch 144. loss=0.020301377400755882. train batch time cost=0.10086584091186523s\n",
            "completed batch 108 of epoch 144. loss=0.029323726892471313. train batch time cost=0.10144972801208496s\n",
            "completed batch 109 of epoch 144. loss=0.018023349344730377. train batch time cost=0.10142755508422852s\n",
            "completed batch 110 of epoch 144. loss=0.009070762433111668. train batch time cost=0.10143542289733887s\n",
            "completed batch 111 of epoch 144. loss=0.01727207563817501. train batch time cost=0.10194015502929688s\n",
            "completed batch 112 of epoch 144. loss=0.056295838207006454. train batch time cost=0.10234713554382324s\n",
            "completed batch 113 of epoch 144. loss=0.01973099075257778. train batch time cost=0.10183954238891602s\n",
            "completed batch 114 of epoch 144. loss=0.022638490423560143. train batch time cost=0.0960233211517334s\n",
            "completed batch 115 of epoch 144. loss=0.03161841258406639. train batch time cost=0.10048723220825195s\n",
            "completed batch 116 of epoch 144. loss=0.03664267808198929. train batch time cost=0.09577727317810059s\n",
            "completed batch 117 of epoch 144. loss=0.0019182631513103843. train batch time cost=0.09522151947021484s\n",
            "completed batch 118 of epoch 144. loss=0.005481925327330828. train batch time cost=0.09513163566589355s\n",
            "completed batch 119 of epoch 144. loss=0.04892769455909729. train batch time cost=0.09523224830627441s\n",
            "completed batch 120 of epoch 144. loss=0.013263327069580555. train batch time cost=0.10213351249694824s\n",
            "completed batch 121 of epoch 144. loss=0.006841820664703846. train batch time cost=0.10234212875366211s\n",
            "completed batch 122 of epoch 144. loss=0.050393667072057724. train batch time cost=0.10192084312438965s\n",
            "completed batch 123 of epoch 144. loss=0.0030944584868848324. train batch time cost=0.10380244255065918s\n",
            "completed batch 124 of epoch 144. loss=0.02045229636132717. train batch time cost=0.10125470161437988s\n",
            "completed batch 125 of epoch 144. loss=0.017331767827272415. train batch time cost=0.10449934005737305s\n",
            "completed batch 126 of epoch 144. loss=0.00014896990614943206. train batch time cost=0.030546903610229492s\n",
            "completed test of epoch 144. loss=0.00014896990614943206. accuracy=0.6939590614078882. train one epoch time cost=27.536378860473633s, test validation time cost=3.911506175994873\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835, 0.655516724912631, 0.655516724912631, 0.6984523215177234, 0.6939590614078882]\n",
            "completed batch 1 of epoch 145. loss=0.009834772907197475. train batch time cost=0.09580874443054199s\n",
            "completed batch 2 of epoch 145. loss=0.04394844174385071. train batch time cost=0.09602475166320801s\n",
            "completed batch 3 of epoch 145. loss=0.012644853442907333. train batch time cost=0.09601163864135742s\n",
            "completed batch 4 of epoch 145. loss=0.011662772856652737. train batch time cost=0.09493827819824219s\n",
            "completed batch 5 of epoch 145. loss=0.026566190645098686. train batch time cost=0.09638452529907227s\n",
            "completed batch 6 of epoch 145. loss=0.007012708578258753. train batch time cost=0.09612917900085449s\n",
            "completed batch 7 of epoch 145. loss=0.005546669475734234. train batch time cost=0.09536027908325195s\n",
            "completed batch 8 of epoch 145. loss=0.03238126263022423. train batch time cost=0.0962374210357666s\n",
            "completed batch 9 of epoch 145. loss=0.03729419410228729. train batch time cost=0.09618473052978516s\n",
            "completed batch 10 of epoch 145. loss=0.04242939502000809. train batch time cost=0.09997892379760742s\n",
            "completed batch 11 of epoch 145. loss=0.09508008509874344. train batch time cost=0.1026310920715332s\n",
            "completed batch 12 of epoch 145. loss=0.01965329423546791. train batch time cost=0.09695148468017578s\n",
            "completed batch 13 of epoch 145. loss=0.01986250840127468. train batch time cost=0.09609627723693848s\n",
            "completed batch 14 of epoch 145. loss=0.027517426759004593. train batch time cost=0.09623193740844727s\n",
            "completed batch 15 of epoch 145. loss=0.05238064378499985. train batch time cost=0.09680819511413574s\n",
            "completed batch 16 of epoch 145. loss=0.036205001175403595. train batch time cost=0.09617471694946289s\n",
            "completed batch 17 of epoch 145. loss=0.004672273527830839. train batch time cost=0.0959322452545166s\n",
            "completed batch 18 of epoch 145. loss=0.0635123923420906. train batch time cost=0.09479475021362305s\n",
            "completed batch 19 of epoch 145. loss=0.008902130648493767. train batch time cost=0.09507131576538086s\n",
            "completed batch 20 of epoch 145. loss=0.003911436069756746. train batch time cost=0.09709000587463379s\n",
            "completed batch 21 of epoch 145. loss=0.0025734035298228264. train batch time cost=0.09549736976623535s\n",
            "completed batch 22 of epoch 145. loss=0.008199892938137054. train batch time cost=0.1020195484161377s\n",
            "completed batch 23 of epoch 145. loss=0.07866402715444565. train batch time cost=0.10273289680480957s\n",
            "completed batch 24 of epoch 145. loss=0.03337785229086876. train batch time cost=0.1031033992767334s\n",
            "completed batch 25 of epoch 145. loss=0.01637827232480049. train batch time cost=0.1012578010559082s\n",
            "completed batch 26 of epoch 145. loss=0.004118872806429863. train batch time cost=0.10258650779724121s\n",
            "completed batch 27 of epoch 145. loss=0.018738312646746635. train batch time cost=0.10457539558410645s\n",
            "completed batch 28 of epoch 145. loss=0.00915275514125824. train batch time cost=0.10197591781616211s\n",
            "completed batch 29 of epoch 145. loss=0.012765923514962196. train batch time cost=0.10321688652038574s\n",
            "completed batch 30 of epoch 145. loss=0.003705872455611825. train batch time cost=0.10145211219787598s\n",
            "completed batch 31 of epoch 145. loss=0.051943451166152954. train batch time cost=0.10185956954956055s\n",
            "completed batch 32 of epoch 145. loss=0.006470745429396629. train batch time cost=0.10214900970458984s\n",
            "completed batch 33 of epoch 145. loss=0.01367927435785532. train batch time cost=0.10474061965942383s\n",
            "completed batch 34 of epoch 145. loss=0.043221816420555115. train batch time cost=0.0985252857208252s\n",
            "completed batch 35 of epoch 145. loss=0.02497846633195877. train batch time cost=0.1008749008178711s\n",
            "completed batch 36 of epoch 145. loss=0.019657233729958534. train batch time cost=0.101165771484375s\n",
            "completed batch 37 of epoch 145. loss=0.006099691614508629. train batch time cost=0.10145759582519531s\n",
            "completed batch 38 of epoch 145. loss=0.022414475679397583. train batch time cost=0.10177373886108398s\n",
            "completed batch 39 of epoch 145. loss=0.03285769000649452. train batch time cost=0.10363602638244629s\n",
            "completed batch 40 of epoch 145. loss=0.004921949468553066. train batch time cost=0.10416293144226074s\n",
            "completed batch 41 of epoch 145. loss=0.0021500401198863983. train batch time cost=0.10417938232421875s\n",
            "completed batch 42 of epoch 145. loss=0.02816804312169552. train batch time cost=0.10264158248901367s\n",
            "completed batch 43 of epoch 145. loss=0.005245406646281481. train batch time cost=0.10273480415344238s\n",
            "completed batch 44 of epoch 145. loss=0.017350677400827408. train batch time cost=0.10414528846740723s\n",
            "completed batch 45 of epoch 145. loss=0.006958599202334881. train batch time cost=0.10321044921875s\n",
            "completed batch 46 of epoch 145. loss=0.020688194781541824. train batch time cost=0.1006782054901123s\n",
            "completed batch 47 of epoch 145. loss=0.010386498644948006. train batch time cost=0.10219073295593262s\n",
            "completed batch 48 of epoch 145. loss=0.0055943867191672325. train batch time cost=0.10125136375427246s\n",
            "completed batch 49 of epoch 145. loss=0.02053574100136757. train batch time cost=0.10060954093933105s\n",
            "completed batch 50 of epoch 145. loss=0.03661338984966278. train batch time cost=0.1010441780090332s\n",
            "completed batch 51 of epoch 145. loss=0.006113756448030472. train batch time cost=0.09400582313537598s\n",
            "completed batch 52 of epoch 145. loss=0.010366523638367653. train batch time cost=0.09460091590881348s\n",
            "completed batch 53 of epoch 145. loss=0.05580531433224678. train batch time cost=0.0948340892791748s\n",
            "completed batch 54 of epoch 145. loss=0.01810818538069725. train batch time cost=0.09477066993713379s\n",
            "completed batch 55 of epoch 145. loss=0.012328773736953735. train batch time cost=0.09528803825378418s\n",
            "completed batch 56 of epoch 145. loss=0.0064796749502420425. train batch time cost=0.09494280815124512s\n",
            "completed batch 57 of epoch 145. loss=0.0036820517852902412. train batch time cost=0.09481096267700195s\n",
            "completed batch 58 of epoch 145. loss=0.007526665925979614. train batch time cost=0.0957632064819336s\n",
            "completed batch 59 of epoch 145. loss=0.0011025579879060388. train batch time cost=0.09693384170532227s\n",
            "completed batch 60 of epoch 145. loss=0.02629810757935047. train batch time cost=0.09473133087158203s\n",
            "completed batch 61 of epoch 145. loss=0.006552962586283684. train batch time cost=0.09592723846435547s\n",
            "completed batch 62 of epoch 145. loss=0.014862380921840668. train batch time cost=0.09517145156860352s\n",
            "completed batch 63 of epoch 145. loss=0.0035194631200283766. train batch time cost=0.09561944007873535s\n",
            "completed batch 64 of epoch 145. loss=0.005685118958353996. train batch time cost=0.0960838794708252s\n",
            "completed batch 65 of epoch 145. loss=0.0019176308996975422. train batch time cost=0.09638524055480957s\n",
            "completed batch 66 of epoch 145. loss=0.027138246223330498. train batch time cost=0.09557008743286133s\n",
            "completed batch 67 of epoch 145. loss=0.006404017098248005. train batch time cost=0.0958857536315918s\n",
            "completed batch 68 of epoch 145. loss=0.010753444395959377. train batch time cost=0.0985252857208252s\n",
            "completed batch 69 of epoch 145. loss=0.020646357908844948. train batch time cost=0.09546685218811035s\n",
            "completed batch 70 of epoch 145. loss=0.03268260508775711. train batch time cost=0.09772443771362305s\n",
            "completed batch 71 of epoch 145. loss=0.023478208109736443. train batch time cost=0.09595322608947754s\n",
            "completed batch 72 of epoch 145. loss=0.010427454486489296. train batch time cost=0.09739494323730469s\n",
            "completed batch 73 of epoch 145. loss=0.014548858627676964. train batch time cost=0.09542083740234375s\n",
            "completed batch 74 of epoch 145. loss=0.021358085796236992. train batch time cost=0.09607100486755371s\n",
            "completed batch 75 of epoch 145. loss=0.06205688789486885. train batch time cost=0.09570193290710449s\n",
            "completed batch 76 of epoch 145. loss=0.014314677566289902. train batch time cost=0.0951986312866211s\n",
            "completed batch 77 of epoch 145. loss=0.06175313889980316. train batch time cost=0.10245156288146973s\n",
            "completed batch 78 of epoch 145. loss=0.00222903024405241. train batch time cost=0.10227489471435547s\n",
            "completed batch 79 of epoch 145. loss=0.008709054440259933. train batch time cost=0.1012108325958252s\n",
            "completed batch 80 of epoch 145. loss=0.062467481940984726. train batch time cost=0.10334348678588867s\n",
            "completed batch 81 of epoch 145. loss=0.03719114884734154. train batch time cost=0.10274600982666016s\n",
            "completed batch 82 of epoch 145. loss=0.021059898659586906. train batch time cost=0.10257554054260254s\n",
            "completed batch 83 of epoch 145. loss=0.006062579806894064. train batch time cost=0.10187077522277832s\n",
            "completed batch 84 of epoch 145. loss=0.005668547470122576. train batch time cost=0.10134625434875488s\n",
            "completed batch 85 of epoch 145. loss=0.0061233676970005035. train batch time cost=0.10331225395202637s\n",
            "completed batch 86 of epoch 145. loss=0.01456565409898758. train batch time cost=0.10259413719177246s\n",
            "completed batch 87 of epoch 145. loss=0.03292519599199295. train batch time cost=0.10149645805358887s\n",
            "completed batch 88 of epoch 145. loss=0.039295539259910583. train batch time cost=0.1009063720703125s\n",
            "completed batch 89 of epoch 145. loss=0.026210924610495567. train batch time cost=0.10109257698059082s\n",
            "completed batch 90 of epoch 145. loss=0.005772826727479696. train batch time cost=0.10454297065734863s\n",
            "completed batch 91 of epoch 145. loss=0.004013365600258112. train batch time cost=0.10350728034973145s\n",
            "completed batch 92 of epoch 145. loss=0.009143688715994358. train batch time cost=0.10408186912536621s\n",
            "completed batch 93 of epoch 145. loss=0.018249820917844772. train batch time cost=0.10304927825927734s\n",
            "completed batch 94 of epoch 145. loss=0.020682435482740402. train batch time cost=0.10269737243652344s\n",
            "completed batch 95 of epoch 145. loss=0.01801135577261448. train batch time cost=0.10409760475158691s\n",
            "completed batch 96 of epoch 145. loss=0.02210393361747265. train batch time cost=0.10077023506164551s\n",
            "completed batch 97 of epoch 145. loss=0.018903829157352448. train batch time cost=0.10091209411621094s\n",
            "completed batch 98 of epoch 145. loss=0.055030159652233124. train batch time cost=0.10257720947265625s\n",
            "completed batch 99 of epoch 145. loss=0.00498049333691597. train batch time cost=0.10298991203308105s\n",
            "completed batch 100 of epoch 145. loss=0.001873125322163105. train batch time cost=0.10303068161010742s\n",
            "completed batch 101 of epoch 145. loss=0.013030396774411201. train batch time cost=0.10255694389343262s\n",
            "completed batch 102 of epoch 145. loss=0.009284593164920807. train batch time cost=0.10208535194396973s\n",
            "completed batch 103 of epoch 145. loss=0.01095067709684372. train batch time cost=0.10192656517028809s\n",
            "completed batch 104 of epoch 145. loss=0.006306490860879421. train batch time cost=0.10204267501831055s\n",
            "completed batch 105 of epoch 145. loss=0.08348336815834045. train batch time cost=0.10142397880554199s\n",
            "completed batch 106 of epoch 145. loss=0.03134845197200775. train batch time cost=0.10094785690307617s\n",
            "completed batch 107 of epoch 145. loss=0.006588846445083618. train batch time cost=0.10148811340332031s\n",
            "completed batch 108 of epoch 145. loss=0.019052330404520035. train batch time cost=0.10170745849609375s\n",
            "completed batch 109 of epoch 145. loss=0.002891456475481391. train batch time cost=0.10279321670532227s\n",
            "completed batch 110 of epoch 145. loss=0.015870647504925728. train batch time cost=0.10243582725524902s\n",
            "completed batch 111 of epoch 145. loss=0.004145494196563959. train batch time cost=0.10202455520629883s\n",
            "completed batch 112 of epoch 145. loss=0.12586942315101624. train batch time cost=0.10251450538635254s\n",
            "completed batch 113 of epoch 145. loss=0.0025352295488119125. train batch time cost=0.10343456268310547s\n",
            "completed batch 114 of epoch 145. loss=0.0030371348839253187. train batch time cost=0.10248351097106934s\n",
            "completed batch 115 of epoch 145. loss=0.011122241616249084. train batch time cost=0.10191869735717773s\n",
            "completed batch 116 of epoch 145. loss=0.022928649559617043. train batch time cost=0.10317111015319824s\n",
            "completed batch 117 of epoch 145. loss=0.021464724093675613. train batch time cost=0.10152268409729004s\n",
            "completed batch 118 of epoch 145. loss=0.015277724713087082. train batch time cost=0.1011497974395752s\n",
            "completed batch 119 of epoch 145. loss=0.004235061816871166. train batch time cost=0.1029667854309082s\n",
            "completed batch 120 of epoch 145. loss=0.009940369985997677. train batch time cost=0.10288810729980469s\n",
            "completed batch 121 of epoch 145. loss=0.05057292431592941. train batch time cost=0.10307168960571289s\n",
            "completed batch 122 of epoch 145. loss=0.10770507901906967. train batch time cost=0.10210013389587402s\n",
            "completed batch 123 of epoch 145. loss=0.060726508498191833. train batch time cost=0.1024332046508789s\n",
            "completed batch 124 of epoch 145. loss=0.00694291153922677. train batch time cost=0.10154938697814941s\n",
            "completed batch 125 of epoch 145. loss=0.015712246298789978. train batch time cost=0.10139012336730957s\n",
            "completed batch 126 of epoch 145. loss=0.00032531097531318665. train batch time cost=0.030215740203857422s\n",
            "completed test of epoch 145. loss=0.00032531097531318665. accuracy=0.7134298552171743. train one epoch time cost=27.432496786117554s, test validation time cost=3.816927194595337\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835, 0.655516724912631, 0.655516724912631, 0.6984523215177234, 0.6939590614078882, 0.7134298552171743]\n",
            "completed batch 1 of epoch 146. loss=0.033269256353378296. train batch time cost=0.09456062316894531s\n",
            "completed batch 2 of epoch 146. loss=0.005704164505004883. train batch time cost=0.09561562538146973s\n",
            "completed batch 3 of epoch 146. loss=0.03896637260913849. train batch time cost=0.09881234169006348s\n",
            "completed batch 4 of epoch 146. loss=0.004459654912352562. train batch time cost=0.09914326667785645s\n",
            "completed batch 5 of epoch 146. loss=0.020459501072764397. train batch time cost=0.10015654563903809s\n",
            "completed batch 6 of epoch 146. loss=0.15795104205608368. train batch time cost=0.10136175155639648s\n",
            "completed batch 7 of epoch 146. loss=0.04324854165315628. train batch time cost=0.10252237319946289s\n",
            "completed batch 8 of epoch 146. loss=0.005345380865037441. train batch time cost=0.10328507423400879s\n",
            "completed batch 9 of epoch 146. loss=0.022734815254807472. train batch time cost=0.10224628448486328s\n",
            "completed batch 10 of epoch 146. loss=0.0035334706772118807. train batch time cost=0.09945034980773926s\n",
            "completed batch 11 of epoch 146. loss=0.012634487822651863. train batch time cost=0.10346078872680664s\n",
            "completed batch 12 of epoch 146. loss=0.0038514533080160618. train batch time cost=0.1046142578125s\n",
            "completed batch 13 of epoch 146. loss=0.025888284668326378. train batch time cost=0.10328030586242676s\n",
            "completed batch 14 of epoch 146. loss=0.010674755088984966. train batch time cost=0.10174250602722168s\n",
            "completed batch 15 of epoch 146. loss=0.004282142035663128. train batch time cost=0.10143852233886719s\n",
            "completed batch 16 of epoch 146. loss=0.006419196259230375. train batch time cost=0.10181331634521484s\n",
            "completed batch 17 of epoch 146. loss=0.07102672755718231. train batch time cost=0.1021268367767334s\n",
            "completed batch 18 of epoch 146. loss=0.003561321645975113. train batch time cost=0.1012582778930664s\n",
            "completed batch 19 of epoch 146. loss=0.009074701927602291. train batch time cost=0.1019129753112793s\n",
            "completed batch 20 of epoch 146. loss=0.025803769007325172. train batch time cost=0.10144543647766113s\n",
            "completed batch 21 of epoch 146. loss=0.004578649532049894. train batch time cost=0.10311484336853027s\n",
            "completed batch 22 of epoch 146. loss=0.004485861398279667. train batch time cost=0.10260820388793945s\n",
            "completed batch 23 of epoch 146. loss=0.010788008570671082. train batch time cost=0.10276031494140625s\n",
            "completed batch 24 of epoch 146. loss=0.009857475757598877. train batch time cost=0.10158324241638184s\n",
            "completed batch 25 of epoch 146. loss=0.003944593481719494. train batch time cost=0.10353493690490723s\n",
            "completed batch 26 of epoch 146. loss=0.0016932490980252624. train batch time cost=0.10127425193786621s\n",
            "completed batch 27 of epoch 146. loss=0.010648973286151886. train batch time cost=0.10075092315673828s\n",
            "completed batch 28 of epoch 146. loss=0.0032012169249355793. train batch time cost=0.1008615493774414s\n",
            "completed batch 29 of epoch 146. loss=0.00807895977050066. train batch time cost=0.10049700736999512s\n",
            "completed batch 30 of epoch 146. loss=0.0002571212244220078. train batch time cost=0.09695982933044434s\n",
            "completed batch 31 of epoch 146. loss=0.0036873717326670885. train batch time cost=0.09710931777954102s\n",
            "completed batch 32 of epoch 146. loss=0.004375041928142309. train batch time cost=0.09578227996826172s\n",
            "completed batch 33 of epoch 146. loss=0.0030055325478315353. train batch time cost=0.09532952308654785s\n",
            "completed batch 34 of epoch 146. loss=0.008118913508951664. train batch time cost=0.09578061103820801s\n",
            "completed batch 35 of epoch 146. loss=0.0009750098688527942. train batch time cost=0.10150432586669922s\n",
            "completed batch 36 of epoch 146. loss=0.0042452323250472546. train batch time cost=0.09451889991760254s\n",
            "completed batch 37 of epoch 146. loss=0.00395295349881053. train batch time cost=0.09420156478881836s\n",
            "completed batch 38 of epoch 146. loss=0.0032000227365642786. train batch time cost=0.09411048889160156s\n",
            "completed batch 39 of epoch 146. loss=0.0034000494051724672. train batch time cost=0.09485769271850586s\n",
            "completed batch 40 of epoch 146. loss=0.0049920580349862576. train batch time cost=0.09605145454406738s\n",
            "completed batch 41 of epoch 146. loss=0.0036300048232078552. train batch time cost=0.09511446952819824s\n",
            "completed batch 42 of epoch 146. loss=0.0014143601292744279. train batch time cost=0.09810900688171387s\n",
            "completed batch 43 of epoch 146. loss=0.005344296805560589. train batch time cost=0.09524369239807129s\n",
            "completed batch 44 of epoch 146. loss=0.004122854210436344. train batch time cost=0.09608697891235352s\n",
            "completed batch 45 of epoch 146. loss=0.006951524410396814. train batch time cost=0.09727787971496582s\n",
            "completed batch 46 of epoch 146. loss=0.0008718669414520264. train batch time cost=0.09622883796691895s\n",
            "completed batch 47 of epoch 146. loss=0.012517999857664108. train batch time cost=0.09563803672790527s\n",
            "completed batch 48 of epoch 146. loss=0.00189635856077075. train batch time cost=0.09621000289916992s\n",
            "completed batch 49 of epoch 146. loss=0.005836568772792816. train batch time cost=0.09547805786132812s\n",
            "completed batch 50 of epoch 146. loss=0.003130207769572735. train batch time cost=0.09593081474304199s\n",
            "completed batch 51 of epoch 146. loss=0.0015505452174693346. train batch time cost=0.09481096267700195s\n",
            "completed batch 52 of epoch 146. loss=0.019662870094180107. train batch time cost=0.09474539756774902s\n",
            "completed batch 53 of epoch 146. loss=0.016083775088191032. train batch time cost=0.09899282455444336s\n",
            "completed batch 54 of epoch 146. loss=0.0047492533922195435. train batch time cost=0.09438729286193848s\n",
            "completed batch 55 of epoch 146. loss=0.03627561777830124. train batch time cost=0.09489607810974121s\n",
            "completed batch 56 of epoch 146. loss=0.0035376858431845903. train batch time cost=0.09403705596923828s\n",
            "completed batch 57 of epoch 146. loss=0.021068518981337547. train batch time cost=0.09431171417236328s\n",
            "completed batch 58 of epoch 146. loss=0.0034640224184840918. train batch time cost=0.09816288948059082s\n",
            "completed batch 59 of epoch 146. loss=0.011748675256967545. train batch time cost=0.09703326225280762s\n",
            "completed batch 60 of epoch 146. loss=0.008137484081089497. train batch time cost=0.10128283500671387s\n",
            "completed batch 61 of epoch 146. loss=0.0008876325446180999. train batch time cost=0.10231399536132812s\n",
            "completed batch 62 of epoch 146. loss=0.003952842205762863. train batch time cost=0.10003328323364258s\n",
            "completed batch 63 of epoch 146. loss=0.0015525458147749305. train batch time cost=0.10102009773254395s\n",
            "completed batch 64 of epoch 146. loss=0.006698132026940584. train batch time cost=0.10148358345031738s\n",
            "completed batch 65 of epoch 146. loss=0.0011652455432340503. train batch time cost=0.1014549732208252s\n",
            "completed batch 66 of epoch 146. loss=0.0352792851626873. train batch time cost=0.10187315940856934s\n",
            "completed batch 67 of epoch 146. loss=0.005670204758644104. train batch time cost=0.10316348075866699s\n",
            "completed batch 68 of epoch 146. loss=0.0017149836057797074. train batch time cost=0.1071171760559082s\n",
            "completed batch 69 of epoch 146. loss=0.0008186004706658423. train batch time cost=0.09924197196960449s\n",
            "completed batch 70 of epoch 146. loss=0.002278186148032546. train batch time cost=0.09730315208435059s\n",
            "completed batch 71 of epoch 146. loss=0.009694522246718407. train batch time cost=0.09523653984069824s\n",
            "completed batch 72 of epoch 146. loss=0.005141500383615494. train batch time cost=0.09429121017456055s\n",
            "completed batch 73 of epoch 146. loss=0.03731514886021614. train batch time cost=0.09705948829650879s\n",
            "completed batch 74 of epoch 146. loss=0.003485457506030798. train batch time cost=0.09528374671936035s\n",
            "completed batch 75 of epoch 146. loss=0.037394750863313675. train batch time cost=0.10189962387084961s\n",
            "completed batch 76 of epoch 146. loss=0.08373744040727615. train batch time cost=0.10185480117797852s\n",
            "completed batch 77 of epoch 146. loss=0.0020948508754372597. train batch time cost=0.1021430492401123s\n",
            "completed batch 78 of epoch 146. loss=0.004520483780652285. train batch time cost=0.10302948951721191s\n",
            "completed batch 79 of epoch 146. loss=0.010058404877781868. train batch time cost=0.09488606452941895s\n",
            "completed batch 80 of epoch 146. loss=0.0010277931578457355. train batch time cost=0.09527254104614258s\n",
            "completed batch 81 of epoch 146. loss=0.019228719174861908. train batch time cost=0.09475994110107422s\n",
            "completed batch 82 of epoch 146. loss=0.022638192400336266. train batch time cost=0.09517455101013184s\n",
            "completed batch 83 of epoch 146. loss=0.007968204095959663. train batch time cost=0.10063529014587402s\n",
            "completed batch 84 of epoch 146. loss=0.018380239605903625. train batch time cost=0.10078048706054688s\n",
            "completed batch 85 of epoch 146. loss=0.04647708684206009. train batch time cost=0.10090112686157227s\n",
            "completed batch 86 of epoch 146. loss=0.028745008632540703. train batch time cost=0.10481929779052734s\n",
            "completed batch 87 of epoch 146. loss=0.008089343085885048. train batch time cost=0.10382223129272461s\n",
            "completed batch 88 of epoch 146. loss=0.012524702586233616. train batch time cost=0.10116934776306152s\n",
            "completed batch 89 of epoch 146. loss=0.003822319908067584. train batch time cost=0.10173249244689941s\n",
            "completed batch 90 of epoch 146. loss=0.01465019304305315. train batch time cost=0.10222530364990234s\n",
            "completed batch 91 of epoch 146. loss=0.004897219594568014. train batch time cost=0.10154390335083008s\n",
            "completed batch 92 of epoch 146. loss=0.08295746892690659. train batch time cost=0.10262918472290039s\n",
            "completed batch 93 of epoch 146. loss=0.0448170006275177. train batch time cost=0.10041189193725586s\n",
            "completed batch 94 of epoch 146. loss=0.0037203996907919645. train batch time cost=0.10220932960510254s\n",
            "completed batch 95 of epoch 146. loss=0.016562679782509804. train batch time cost=0.10100865364074707s\n",
            "completed batch 96 of epoch 146. loss=0.02182866260409355. train batch time cost=0.1015312671661377s\n",
            "completed batch 97 of epoch 146. loss=0.03058626502752304. train batch time cost=0.10176324844360352s\n",
            "completed batch 98 of epoch 146. loss=0.008660894818603992. train batch time cost=0.1012420654296875s\n",
            "completed batch 99 of epoch 146. loss=0.021233173087239265. train batch time cost=0.10479044914245605s\n",
            "completed batch 100 of epoch 146. loss=0.02614046260714531. train batch time cost=0.10273623466491699s\n",
            "completed batch 101 of epoch 146. loss=0.004961799364537001. train batch time cost=0.10155463218688965s\n",
            "completed batch 102 of epoch 146. loss=0.012744947336614132. train batch time cost=0.10064864158630371s\n",
            "completed batch 103 of epoch 146. loss=0.011978154070675373. train batch time cost=0.10148286819458008s\n",
            "completed batch 104 of epoch 146. loss=0.029053162783384323. train batch time cost=0.10212016105651855s\n",
            "completed batch 105 of epoch 146. loss=0.057041067630052567. train batch time cost=0.10149717330932617s\n",
            "completed batch 106 of epoch 146. loss=0.00485581997781992. train batch time cost=0.10045814514160156s\n",
            "completed batch 107 of epoch 146. loss=0.01170569472014904. train batch time cost=0.10120368003845215s\n",
            "completed batch 108 of epoch 146. loss=0.034093864262104034. train batch time cost=0.10202455520629883s\n",
            "completed batch 109 of epoch 146. loss=0.007263400126248598. train batch time cost=0.10339951515197754s\n",
            "completed batch 110 of epoch 146. loss=0.045205257833004. train batch time cost=0.1011199951171875s\n",
            "completed batch 111 of epoch 146. loss=0.002456848043948412. train batch time cost=0.10219264030456543s\n",
            "completed batch 112 of epoch 146. loss=0.011699680238962173. train batch time cost=0.10227441787719727s\n",
            "completed batch 113 of epoch 146. loss=0.03696330264210701. train batch time cost=0.10168027877807617s\n",
            "completed batch 114 of epoch 146. loss=0.06680621206760406. train batch time cost=0.10133790969848633s\n",
            "completed batch 115 of epoch 146. loss=0.007917055860161781. train batch time cost=0.1006925106048584s\n",
            "completed batch 116 of epoch 146. loss=0.02781549096107483. train batch time cost=0.1018068790435791s\n",
            "completed batch 117 of epoch 146. loss=0.015025644563138485. train batch time cost=0.10169124603271484s\n",
            "completed batch 118 of epoch 146. loss=0.0056845624931156635. train batch time cost=0.10187196731567383s\n",
            "completed batch 119 of epoch 146. loss=0.014206737279891968. train batch time cost=0.10268592834472656s\n",
            "completed batch 120 of epoch 146. loss=0.006424961611628532. train batch time cost=0.1022493839263916s\n",
            "completed batch 121 of epoch 146. loss=0.11540907621383667. train batch time cost=0.10443758964538574s\n",
            "completed batch 122 of epoch 146. loss=0.01689719408750534. train batch time cost=0.10223269462585449s\n",
            "completed batch 123 of epoch 146. loss=0.08070620149374008. train batch time cost=0.10353684425354004s\n",
            "completed batch 124 of epoch 146. loss=0.0028732139617204666. train batch time cost=0.10128974914550781s\n",
            "completed batch 125 of epoch 146. loss=0.045738060027360916. train batch time cost=0.1010591983795166s\n",
            "completed batch 126 of epoch 146. loss=0.000613524520304054. train batch time cost=0.031116485595703125s\n",
            "completed test of epoch 146. loss=0.000613524520304054. accuracy=0.6859710434348477. train one epoch time cost=27.432796239852905s, test validation time cost=3.8510642051696777\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835, 0.655516724912631, 0.655516724912631, 0.6984523215177234, 0.6939590614078882, 0.7134298552171743, 0.6859710434348477]\n",
            "completed batch 1 of epoch 147. loss=0.0014983979053795338. train batch time cost=0.09556245803833008s\n",
            "completed batch 2 of epoch 147. loss=0.04561780393123627. train batch time cost=0.09502816200256348s\n",
            "completed batch 3 of epoch 147. loss=0.007429010234773159. train batch time cost=0.09741616249084473s\n",
            "completed batch 4 of epoch 147. loss=0.005342287477105856. train batch time cost=0.09506058692932129s\n",
            "completed batch 5 of epoch 147. loss=0.01043335534632206. train batch time cost=0.09487080574035645s\n",
            "completed batch 6 of epoch 147. loss=0.005780587904155254. train batch time cost=0.09464311599731445s\n",
            "completed batch 7 of epoch 147. loss=0.026429761201143265. train batch time cost=0.09553074836730957s\n",
            "completed batch 8 of epoch 147. loss=0.012706647627055645. train batch time cost=0.09803032875061035s\n",
            "completed batch 9 of epoch 147. loss=0.025845622643828392. train batch time cost=0.09523725509643555s\n",
            "completed batch 10 of epoch 147. loss=0.022272933274507523. train batch time cost=0.10096025466918945s\n",
            "completed batch 11 of epoch 147. loss=0.013323107734322548. train batch time cost=0.10242676734924316s\n",
            "completed batch 12 of epoch 147. loss=0.12105071544647217. train batch time cost=0.10331606864929199s\n",
            "completed batch 13 of epoch 147. loss=0.13301049172878265. train batch time cost=0.1016089916229248s\n",
            "completed batch 14 of epoch 147. loss=0.0053602359257638454. train batch time cost=0.10064983367919922s\n",
            "completed batch 15 of epoch 147. loss=0.006668680347502232. train batch time cost=0.1009986400604248s\n",
            "completed batch 16 of epoch 147. loss=0.027357544749975204. train batch time cost=0.10190081596374512s\n",
            "completed batch 17 of epoch 147. loss=0.011553291231393814. train batch time cost=0.10204768180847168s\n",
            "completed batch 18 of epoch 147. loss=0.004250301513820887. train batch time cost=0.10181450843811035s\n",
            "completed batch 19 of epoch 147. loss=0.19139711558818817. train batch time cost=0.10112857818603516s\n",
            "completed batch 20 of epoch 147. loss=0.008728484623134136. train batch time cost=0.10346174240112305s\n",
            "completed batch 21 of epoch 147. loss=0.004014828708022833. train batch time cost=0.10172057151794434s\n",
            "completed batch 22 of epoch 147. loss=0.00419789832085371. train batch time cost=0.10225415229797363s\n",
            "completed batch 23 of epoch 147. loss=0.0032203251030296087. train batch time cost=0.10179853439331055s\n",
            "completed batch 24 of epoch 147. loss=0.0017740431940183043. train batch time cost=0.10358858108520508s\n",
            "completed batch 25 of epoch 147. loss=0.005078913178294897. train batch time cost=0.10139107704162598s\n",
            "completed batch 26 of epoch 147. loss=0.0068777319975197315. train batch time cost=0.10125851631164551s\n",
            "completed batch 27 of epoch 147. loss=0.012075561098754406. train batch time cost=0.1014089584350586s\n",
            "completed batch 28 of epoch 147. loss=0.008076898753643036. train batch time cost=0.10461235046386719s\n",
            "completed batch 29 of epoch 147. loss=0.02160455472767353. train batch time cost=0.10330915451049805s\n",
            "completed batch 30 of epoch 147. loss=6.448753993026912e-05. train batch time cost=0.10359334945678711s\n",
            "completed batch 31 of epoch 147. loss=0.01921459101140499. train batch time cost=0.10264039039611816s\n",
            "completed batch 32 of epoch 147. loss=0.11177711933851242. train batch time cost=0.10282373428344727s\n",
            "completed batch 33 of epoch 147. loss=0.015015912242233753. train batch time cost=0.10168766975402832s\n",
            "completed batch 34 of epoch 147. loss=0.03419483080506325. train batch time cost=0.1016836166381836s\n",
            "completed batch 35 of epoch 147. loss=0.01418888010084629. train batch time cost=0.10225343704223633s\n",
            "completed batch 36 of epoch 147. loss=0.08519509434700012. train batch time cost=0.10238790512084961s\n",
            "completed batch 37 of epoch 147. loss=0.10575409233570099. train batch time cost=0.10317635536193848s\n",
            "completed batch 38 of epoch 147. loss=0.003987221512943506. train batch time cost=0.10250377655029297s\n",
            "completed batch 39 of epoch 147. loss=0.016722235828638077. train batch time cost=0.10241889953613281s\n",
            "completed batch 40 of epoch 147. loss=0.00544267613440752. train batch time cost=0.09871506690979004s\n",
            "completed batch 41 of epoch 147. loss=0.025968318805098534. train batch time cost=0.09604048728942871s\n",
            "completed batch 42 of epoch 147. loss=0.028350386768579483. train batch time cost=0.11010932922363281s\n",
            "completed batch 43 of epoch 147. loss=0.017286166548728943. train batch time cost=0.10352134704589844s\n",
            "completed batch 44 of epoch 147. loss=0.006636659149080515. train batch time cost=0.10354351997375488s\n",
            "completed batch 45 of epoch 147. loss=0.001480665523558855. train batch time cost=0.10169863700866699s\n",
            "completed batch 46 of epoch 147. loss=0.024940472096204758. train batch time cost=0.10151267051696777s\n",
            "completed batch 47 of epoch 147. loss=0.05162050575017929. train batch time cost=0.1003274917602539s\n",
            "completed batch 48 of epoch 147. loss=0.00974000059068203. train batch time cost=0.10197591781616211s\n",
            "completed batch 49 of epoch 147. loss=0.010223698802292347. train batch time cost=0.10254883766174316s\n",
            "completed batch 50 of epoch 147. loss=0.007617651950567961. train batch time cost=0.1026163101196289s\n",
            "completed batch 51 of epoch 147. loss=0.003073106985539198. train batch time cost=0.10193443298339844s\n",
            "completed batch 52 of epoch 147. loss=0.04322699457406998. train batch time cost=0.10139942169189453s\n",
            "completed batch 53 of epoch 147. loss=0.08455238491296768. train batch time cost=0.10143780708312988s\n",
            "completed batch 54 of epoch 147. loss=0.06117505207657814. train batch time cost=0.10093903541564941s\n",
            "completed batch 55 of epoch 147. loss=0.013533666729927063. train batch time cost=0.10192370414733887s\n",
            "completed batch 56 of epoch 147. loss=0.024635786190629005. train batch time cost=0.10071301460266113s\n",
            "completed batch 57 of epoch 147. loss=0.005315606947988272. train batch time cost=0.10174894332885742s\n",
            "completed batch 58 of epoch 147. loss=0.0596134290099144. train batch time cost=0.10269498825073242s\n",
            "completed batch 59 of epoch 147. loss=0.03097742423415184. train batch time cost=0.10141444206237793s\n",
            "completed batch 60 of epoch 147. loss=0.013787121511995792. train batch time cost=0.10099673271179199s\n",
            "completed batch 61 of epoch 147. loss=0.023162147030234337. train batch time cost=0.1015622615814209s\n",
            "completed batch 62 of epoch 147. loss=0.05159679427742958. train batch time cost=0.10107731819152832s\n",
            "completed batch 63 of epoch 147. loss=0.0051386430859565735. train batch time cost=0.10084152221679688s\n",
            "completed batch 64 of epoch 147. loss=0.054908450692892075. train batch time cost=0.10128474235534668s\n",
            "completed batch 65 of epoch 147. loss=0.12258589267730713. train batch time cost=0.10289835929870605s\n",
            "completed batch 66 of epoch 147. loss=0.00751994363963604. train batch time cost=0.10278797149658203s\n",
            "completed batch 67 of epoch 147. loss=0.004962552804499865. train batch time cost=0.10207295417785645s\n",
            "completed batch 68 of epoch 147. loss=0.17196203768253326. train batch time cost=0.10299158096313477s\n",
            "completed batch 69 of epoch 147. loss=0.031084492802619934. train batch time cost=0.10278630256652832s\n",
            "completed batch 70 of epoch 147. loss=0.0040877945721149445. train batch time cost=0.10373187065124512s\n",
            "completed batch 71 of epoch 147. loss=0.01897563226521015. train batch time cost=0.10195660591125488s\n",
            "completed batch 72 of epoch 147. loss=0.023036692291498184. train batch time cost=0.10141992568969727s\n",
            "completed batch 73 of epoch 147. loss=0.024613600224256516. train batch time cost=0.10208773612976074s\n",
            "completed batch 74 of epoch 147. loss=0.02387337014079094. train batch time cost=0.10052800178527832s\n",
            "completed batch 75 of epoch 147. loss=0.02725127898156643. train batch time cost=0.10072612762451172s\n",
            "completed batch 76 of epoch 147. loss=0.03552595525979996. train batch time cost=0.10066962242126465s\n",
            "completed batch 77 of epoch 147. loss=0.021586162969470024. train batch time cost=0.10186052322387695s\n",
            "completed batch 78 of epoch 147. loss=0.11682073771953583. train batch time cost=0.1002953052520752s\n",
            "completed batch 79 of epoch 147. loss=0.01787327043712139. train batch time cost=0.10341286659240723s\n",
            "completed batch 80 of epoch 147. loss=0.031062256544828415. train batch time cost=0.1032865047454834s\n",
            "completed batch 81 of epoch 147. loss=0.04881348833441734. train batch time cost=0.1029667854309082s\n",
            "completed batch 82 of epoch 147. loss=0.016559338197112083. train batch time cost=0.10152363777160645s\n",
            "completed batch 83 of epoch 147. loss=0.11039409786462784. train batch time cost=0.10078144073486328s\n",
            "completed batch 84 of epoch 147. loss=0.03130026161670685. train batch time cost=0.10413503646850586s\n",
            "completed batch 85 of epoch 147. loss=0.013811253011226654. train batch time cost=0.10207724571228027s\n",
            "completed batch 86 of epoch 147. loss=0.017462745308876038. train batch time cost=0.10271286964416504s\n",
            "completed batch 87 of epoch 147. loss=0.013362836092710495. train batch time cost=0.10117435455322266s\n",
            "completed batch 88 of epoch 147. loss=0.05811510980129242. train batch time cost=0.10125732421875s\n",
            "completed batch 89 of epoch 147. loss=0.11030937731266022. train batch time cost=0.1016838550567627s\n",
            "completed batch 90 of epoch 147. loss=0.030729422345757484. train batch time cost=0.1014854907989502s\n",
            "completed batch 91 of epoch 147. loss=0.014538541436195374. train batch time cost=0.10007500648498535s\n",
            "completed batch 92 of epoch 147. loss=0.014596136286854744. train batch time cost=0.10003328323364258s\n",
            "completed batch 93 of epoch 147. loss=0.004999054595828056. train batch time cost=0.10094952583312988s\n",
            "completed batch 94 of epoch 147. loss=0.030331013724207878. train batch time cost=0.10265088081359863s\n",
            "completed batch 95 of epoch 147. loss=0.016917290166020393. train batch time cost=0.1049649715423584s\n",
            "completed batch 96 of epoch 147. loss=0.016085932031273842. train batch time cost=0.10258007049560547s\n",
            "completed batch 97 of epoch 147. loss=0.12884581089019775. train batch time cost=0.10160112380981445s\n",
            "completed batch 98 of epoch 147. loss=0.023100439459085464. train batch time cost=0.10314202308654785s\n",
            "completed batch 99 of epoch 147. loss=0.0017857009079307318. train batch time cost=0.10264396667480469s\n",
            "completed batch 100 of epoch 147. loss=0.06820596754550934. train batch time cost=0.10172080993652344s\n",
            "completed batch 101 of epoch 147. loss=0.0308166965842247. train batch time cost=0.10158801078796387s\n",
            "completed batch 102 of epoch 147. loss=0.032564062625169754. train batch time cost=0.10159111022949219s\n",
            "completed batch 103 of epoch 147. loss=0.13304142653942108. train batch time cost=0.10224747657775879s\n",
            "completed batch 104 of epoch 147. loss=0.003254669951274991. train batch time cost=0.10290098190307617s\n",
            "completed batch 105 of epoch 147. loss=0.0251254141330719. train batch time cost=0.10148763656616211s\n",
            "completed batch 106 of epoch 147. loss=0.12827394902706146. train batch time cost=0.10156059265136719s\n",
            "completed batch 107 of epoch 147. loss=0.104067362844944. train batch time cost=0.10237431526184082s\n",
            "completed batch 108 of epoch 147. loss=0.042870502918958664. train batch time cost=0.1025545597076416s\n",
            "completed batch 109 of epoch 147. loss=0.01975812204182148. train batch time cost=0.10271549224853516s\n",
            "completed batch 110 of epoch 147. loss=0.011385872028768063. train batch time cost=0.1020810604095459s\n",
            "completed batch 111 of epoch 147. loss=0.2542760670185089. train batch time cost=0.1062321662902832s\n",
            "completed batch 112 of epoch 147. loss=0.03132497891783714. train batch time cost=0.1028447151184082s\n",
            "completed batch 113 of epoch 147. loss=0.011961662210524082. train batch time cost=0.10153317451477051s\n",
            "completed batch 114 of epoch 147. loss=0.023950593546032906. train batch time cost=0.1016991138458252s\n",
            "completed batch 115 of epoch 147. loss=0.015399878844618797. train batch time cost=0.1025693416595459s\n",
            "completed batch 116 of epoch 147. loss=0.02019442617893219. train batch time cost=0.10213923454284668s\n",
            "completed batch 117 of epoch 147. loss=0.08763577789068222. train batch time cost=0.10280585289001465s\n",
            "completed batch 118 of epoch 147. loss=0.008665584959089756. train batch time cost=0.10269284248352051s\n",
            "completed batch 119 of epoch 147. loss=0.11371197551488876. train batch time cost=0.10167860984802246s\n",
            "completed batch 120 of epoch 147. loss=0.08659964054822922. train batch time cost=0.10088610649108887s\n",
            "completed batch 121 of epoch 147. loss=0.09538453817367554. train batch time cost=0.1019439697265625s\n",
            "completed batch 122 of epoch 147. loss=0.09153568744659424. train batch time cost=0.10193204879760742s\n",
            "completed batch 123 of epoch 147. loss=0.028211796656250954. train batch time cost=0.10243368148803711s\n",
            "completed batch 124 of epoch 147. loss=0.026325451210141182. train batch time cost=0.10152387619018555s\n",
            "completed batch 125 of epoch 147. loss=0.08293714374303818. train batch time cost=0.10090279579162598s\n",
            "completed batch 126 of epoch 147. loss=0.0028404071927070618. train batch time cost=0.029599428176879883s\n",
            "completed test of epoch 147. loss=0.0028404071927070618. accuracy=0.5906140788816775. train one epoch time cost=27.64205527305603s, test validation time cost=3.805696487426758\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835, 0.655516724912631, 0.655516724912631, 0.6984523215177234, 0.6939590614078882, 0.7134298552171743, 0.6859710434348477, 0.5906140788816775]\n",
            "completed batch 1 of epoch 148. loss=0.02330031618475914. train batch time cost=0.10154342651367188s\n",
            "completed batch 2 of epoch 148. loss=0.03813279792666435. train batch time cost=0.10185551643371582s\n",
            "completed batch 3 of epoch 148. loss=0.26394861936569214. train batch time cost=0.10134577751159668s\n",
            "completed batch 4 of epoch 148. loss=0.01954418048262596. train batch time cost=0.10321450233459473s\n",
            "completed batch 5 of epoch 148. loss=0.08443780988454819. train batch time cost=0.10128188133239746s\n",
            "completed batch 6 of epoch 148. loss=0.07743413001298904. train batch time cost=0.10002279281616211s\n",
            "completed batch 7 of epoch 148. loss=0.19795414805412292. train batch time cost=0.09677290916442871s\n",
            "completed batch 8 of epoch 148. loss=0.011018081568181515. train batch time cost=0.10180497169494629s\n",
            "completed batch 9 of epoch 148. loss=0.01950184628367424. train batch time cost=0.10109972953796387s\n",
            "completed batch 10 of epoch 148. loss=0.10008697211742401. train batch time cost=0.10258364677429199s\n",
            "completed batch 11 of epoch 148. loss=0.02952965721487999. train batch time cost=0.10213327407836914s\n",
            "completed batch 12 of epoch 148. loss=0.061690982431173325. train batch time cost=0.10192418098449707s\n",
            "completed batch 13 of epoch 148. loss=0.027729446068406105. train batch time cost=0.10045337677001953s\n",
            "completed batch 14 of epoch 148. loss=0.03999469801783562. train batch time cost=0.10005760192871094s\n",
            "completed batch 15 of epoch 148. loss=0.04041929543018341. train batch time cost=0.1010589599609375s\n",
            "completed batch 16 of epoch 148. loss=0.024875707924365997. train batch time cost=0.10106730461120605s\n",
            "completed batch 17 of epoch 148. loss=0.009993788786232471. train batch time cost=0.10133171081542969s\n",
            "completed batch 18 of epoch 148. loss=0.011851143091917038. train batch time cost=0.10193276405334473s\n",
            "completed batch 19 of epoch 148. loss=0.002487380523234606. train batch time cost=0.10199213027954102s\n",
            "completed batch 20 of epoch 148. loss=0.03617294132709503. train batch time cost=0.10197854042053223s\n",
            "completed batch 21 of epoch 148. loss=0.007379451300948858. train batch time cost=0.10176944732666016s\n",
            "completed batch 22 of epoch 148. loss=0.010000141337513924. train batch time cost=0.10294270515441895s\n",
            "completed batch 23 of epoch 148. loss=0.007956149987876415. train batch time cost=0.10120081901550293s\n",
            "completed batch 24 of epoch 148. loss=0.040641263127326965. train batch time cost=0.101470947265625s\n",
            "completed batch 25 of epoch 148. loss=0.01650969125330448. train batch time cost=0.10187840461730957s\n",
            "completed batch 26 of epoch 148. loss=0.13846498727798462. train batch time cost=0.10224795341491699s\n",
            "completed batch 27 of epoch 148. loss=0.008696505799889565. train batch time cost=0.1039273738861084s\n",
            "completed batch 28 of epoch 148. loss=0.02898486703634262. train batch time cost=0.10097169876098633s\n",
            "completed batch 29 of epoch 148. loss=0.35130345821380615. train batch time cost=0.10762524604797363s\n",
            "completed batch 30 of epoch 148. loss=0.005647827405482531. train batch time cost=0.10183477401733398s\n",
            "completed batch 31 of epoch 148. loss=0.11355124413967133. train batch time cost=0.1005098819732666s\n",
            "completed batch 32 of epoch 148. loss=0.0606350302696228. train batch time cost=0.09992361068725586s\n",
            "completed batch 33 of epoch 148. loss=0.0816250815987587. train batch time cost=0.1018819808959961s\n",
            "completed batch 34 of epoch 148. loss=0.015043494291603565. train batch time cost=0.10195708274841309s\n",
            "completed batch 35 of epoch 148. loss=0.11899037659168243. train batch time cost=0.1014866828918457s\n",
            "completed batch 36 of epoch 148. loss=0.02366720326244831. train batch time cost=0.10088634490966797s\n",
            "completed batch 37 of epoch 148. loss=0.05248366296291351. train batch time cost=0.1014094352722168s\n",
            "completed batch 38 of epoch 148. loss=0.010041266679763794. train batch time cost=0.10283803939819336s\n",
            "completed batch 39 of epoch 148. loss=0.03914349898695946. train batch time cost=0.10211706161499023s\n",
            "completed batch 40 of epoch 148. loss=0.013457195833325386. train batch time cost=0.10238242149353027s\n",
            "completed batch 41 of epoch 148. loss=0.01148448046296835. train batch time cost=0.1012880802154541s\n",
            "completed batch 42 of epoch 148. loss=0.054643332958221436. train batch time cost=0.10128307342529297s\n",
            "completed batch 43 of epoch 148. loss=0.02744879387319088. train batch time cost=0.10115694999694824s\n",
            "completed batch 44 of epoch 148. loss=0.07718969881534576. train batch time cost=0.10083270072937012s\n",
            "completed batch 45 of epoch 148. loss=0.011536762118339539. train batch time cost=0.10110068321228027s\n",
            "completed batch 46 of epoch 148. loss=0.05351759120821953. train batch time cost=0.09995555877685547s\n",
            "completed batch 47 of epoch 148. loss=0.12202642112970352. train batch time cost=0.1009519100189209s\n",
            "completed batch 48 of epoch 148. loss=0.02668972872197628. train batch time cost=0.10140252113342285s\n",
            "completed batch 49 of epoch 148. loss=0.0937488004565239. train batch time cost=0.10228538513183594s\n",
            "completed batch 50 of epoch 148. loss=0.044790495187044144. train batch time cost=0.10206985473632812s\n",
            "completed batch 51 of epoch 148. loss=0.03002224862575531. train batch time cost=0.10375237464904785s\n",
            "completed batch 52 of epoch 148. loss=0.2197730541229248. train batch time cost=0.1031491756439209s\n",
            "completed batch 53 of epoch 148. loss=0.0250064879655838. train batch time cost=0.10234522819519043s\n",
            "completed batch 54 of epoch 148. loss=0.09060145169496536. train batch time cost=0.1014244556427002s\n",
            "completed batch 55 of epoch 148. loss=0.18127162754535675. train batch time cost=0.10127878189086914s\n",
            "completed batch 56 of epoch 148. loss=0.033140599727630615. train batch time cost=0.1047213077545166s\n",
            "completed batch 57 of epoch 148. loss=0.012427438981831074. train batch time cost=0.10211586952209473s\n",
            "completed batch 58 of epoch 148. loss=0.0509076789021492. train batch time cost=0.10684084892272949s\n",
            "completed batch 59 of epoch 148. loss=0.0326305627822876. train batch time cost=0.10442280769348145s\n",
            "completed batch 60 of epoch 148. loss=0.042975254356861115. train batch time cost=0.10217857360839844s\n",
            "completed batch 61 of epoch 148. loss=0.05554595962166786. train batch time cost=0.09945297241210938s\n",
            "completed batch 62 of epoch 148. loss=0.008481360971927643. train batch time cost=0.09931492805480957s\n",
            "completed batch 63 of epoch 148. loss=0.01439654640853405. train batch time cost=0.1011648178100586s\n",
            "completed batch 64 of epoch 148. loss=0.021735982969403267. train batch time cost=0.1013176441192627s\n",
            "completed batch 65 of epoch 148. loss=0.10232558846473694. train batch time cost=0.10117888450622559s\n",
            "completed batch 66 of epoch 148. loss=0.08083026111125946. train batch time cost=0.10274267196655273s\n",
            "completed batch 67 of epoch 148. loss=0.03380126133561134. train batch time cost=0.1022942066192627s\n",
            "completed batch 68 of epoch 148. loss=0.025522684678435326. train batch time cost=0.10252165794372559s\n",
            "completed batch 69 of epoch 148. loss=0.03222150355577469. train batch time cost=0.10200881958007812s\n",
            "completed batch 70 of epoch 148. loss=0.024242060258984566. train batch time cost=0.10227036476135254s\n",
            "completed batch 71 of epoch 148. loss=0.027250630781054497. train batch time cost=0.10247182846069336s\n",
            "completed batch 72 of epoch 148. loss=0.05917787551879883. train batch time cost=0.10146784782409668s\n",
            "completed batch 73 of epoch 148. loss=0.06770745664834976. train batch time cost=0.10012340545654297s\n",
            "completed batch 74 of epoch 148. loss=0.03466285020112991. train batch time cost=0.10086631774902344s\n",
            "completed batch 75 of epoch 148. loss=0.07024632394313812. train batch time cost=0.10112261772155762s\n",
            "completed batch 76 of epoch 148. loss=0.07303831726312637. train batch time cost=0.10479211807250977s\n",
            "completed batch 77 of epoch 148. loss=0.045767903327941895. train batch time cost=0.10280585289001465s\n",
            "completed batch 78 of epoch 148. loss=0.040342651307582855. train batch time cost=0.10193252563476562s\n",
            "completed batch 79 of epoch 148. loss=0.04367668554186821. train batch time cost=0.10123372077941895s\n",
            "completed batch 80 of epoch 148. loss=0.08477609604597092. train batch time cost=0.1015768051147461s\n",
            "completed batch 81 of epoch 148. loss=0.04120982065796852. train batch time cost=0.1011202335357666s\n",
            "completed batch 82 of epoch 148. loss=0.08899819850921631. train batch time cost=0.10075235366821289s\n",
            "completed batch 83 of epoch 148. loss=0.024722130969166756. train batch time cost=0.10066795349121094s\n",
            "completed batch 84 of epoch 148. loss=0.06470827013254166. train batch time cost=0.10107421875s\n",
            "completed batch 85 of epoch 148. loss=0.10274671018123627. train batch time cost=0.1038048267364502s\n",
            "completed batch 86 of epoch 148. loss=0.06761180609464645. train batch time cost=0.10053849220275879s\n",
            "completed batch 87 of epoch 148. loss=0.07995736598968506. train batch time cost=0.10161542892456055s\n",
            "completed batch 88 of epoch 148. loss=0.0238737091422081. train batch time cost=0.1019754409790039s\n",
            "completed batch 89 of epoch 148. loss=0.016077689826488495. train batch time cost=0.10291862487792969s\n",
            "completed batch 90 of epoch 148. loss=0.0392589196562767. train batch time cost=0.10375332832336426s\n",
            "completed batch 91 of epoch 148. loss=0.015783309936523438. train batch time cost=0.10146951675415039s\n",
            "completed batch 92 of epoch 148. loss=0.022164899855852127. train batch time cost=0.10200142860412598s\n",
            "completed batch 93 of epoch 148. loss=0.5617957711219788. train batch time cost=0.10156655311584473s\n",
            "completed batch 94 of epoch 148. loss=0.08194983750581741. train batch time cost=0.10260725021362305s\n",
            "completed batch 95 of epoch 148. loss=0.012442543171346188. train batch time cost=0.10120296478271484s\n",
            "completed batch 96 of epoch 148. loss=0.03030220977962017. train batch time cost=0.10231208801269531s\n",
            "completed batch 97 of epoch 148. loss=0.030244411900639534. train batch time cost=0.1017758846282959s\n",
            "completed batch 98 of epoch 148. loss=0.031070496886968613. train batch time cost=0.10245418548583984s\n",
            "completed batch 99 of epoch 148. loss=0.06692221015691757. train batch time cost=0.10331058502197266s\n",
            "completed batch 100 of epoch 148. loss=0.07411117106676102. train batch time cost=0.10141515731811523s\n",
            "completed batch 101 of epoch 148. loss=0.12038560956716537. train batch time cost=0.10306286811828613s\n",
            "completed batch 102 of epoch 148. loss=0.0352446585893631. train batch time cost=0.10215234756469727s\n",
            "completed batch 103 of epoch 148. loss=0.25717708468437195. train batch time cost=0.10228753089904785s\n",
            "completed batch 104 of epoch 148. loss=0.020416665822267532. train batch time cost=0.10094523429870605s\n",
            "completed batch 105 of epoch 148. loss=0.013430849649012089. train batch time cost=0.1003260612487793s\n",
            "completed batch 106 of epoch 148. loss=0.14280515909194946. train batch time cost=0.1005101203918457s\n",
            "completed batch 107 of epoch 148. loss=0.018912436440587044. train batch time cost=0.10118699073791504s\n",
            "completed batch 108 of epoch 148. loss=0.11841832846403122. train batch time cost=0.1022636890411377s\n",
            "completed batch 109 of epoch 148. loss=0.14757804572582245. train batch time cost=0.1014547348022461s\n",
            "completed batch 110 of epoch 148. loss=0.034226130694150925. train batch time cost=0.10028934478759766s\n",
            "completed batch 111 of epoch 148. loss=0.22297894954681396. train batch time cost=0.10068225860595703s\n",
            "completed batch 112 of epoch 148. loss=0.0721253901720047. train batch time cost=0.10299253463745117s\n",
            "completed batch 113 of epoch 148. loss=0.012636065483093262. train batch time cost=0.10098886489868164s\n",
            "completed batch 114 of epoch 148. loss=0.09321904182434082. train batch time cost=0.1018521785736084s\n",
            "completed batch 115 of epoch 148. loss=0.04397432133555412. train batch time cost=0.10082221031188965s\n",
            "completed batch 116 of epoch 148. loss=0.2299088090658188. train batch time cost=0.10080623626708984s\n",
            "completed batch 117 of epoch 148. loss=0.039502497762441635. train batch time cost=0.10229945182800293s\n",
            "completed batch 118 of epoch 148. loss=0.16427850723266602. train batch time cost=0.10083317756652832s\n",
            "completed batch 119 of epoch 148. loss=0.017961937934160233. train batch time cost=0.10016870498657227s\n",
            "completed batch 120 of epoch 148. loss=0.24727803468704224. train batch time cost=0.10138106346130371s\n",
            "completed batch 121 of epoch 148. loss=0.06792562454938889. train batch time cost=0.10296154022216797s\n",
            "completed batch 122 of epoch 148. loss=0.018957210704684258. train batch time cost=0.10355162620544434s\n",
            "completed batch 123 of epoch 148. loss=0.03025445155799389. train batch time cost=0.10057616233825684s\n",
            "completed batch 124 of epoch 148. loss=0.039057932794094086. train batch time cost=0.10123682022094727s\n",
            "completed batch 125 of epoch 148. loss=0.12483831495046616. train batch time cost=0.10181212425231934s\n",
            "completed batch 126 of epoch 148. loss=0.24234622716903687. train batch time cost=0.029743671417236328s\n",
            "completed test of epoch 148. loss=0.24234622716903687. accuracy=0.707938092860709. train one epoch time cost=27.65606665611267s, test validation time cost=3.8729946613311768\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835, 0.655516724912631, 0.655516724912631, 0.6984523215177234, 0.6939590614078882, 0.7134298552171743, 0.6859710434348477, 0.5906140788816775, 0.707938092860709]\n",
            "completed batch 1 of epoch 149. loss=0.15335781872272491. train batch time cost=0.09547901153564453s\n",
            "completed batch 2 of epoch 149. loss=0.20060738921165466. train batch time cost=0.09969520568847656s\n",
            "completed batch 3 of epoch 149. loss=0.1091754212975502. train batch time cost=0.09821963310241699s\n",
            "completed batch 4 of epoch 149. loss=0.04008381813764572. train batch time cost=0.09506678581237793s\n",
            "completed batch 5 of epoch 149. loss=0.11866326630115509. train batch time cost=0.09631752967834473s\n",
            "completed batch 6 of epoch 149. loss=0.10242917388677597. train batch time cost=0.09702110290527344s\n",
            "completed batch 7 of epoch 149. loss=0.10985154658555984. train batch time cost=0.09681296348571777s\n",
            "completed batch 8 of epoch 149. loss=0.1780078262090683. train batch time cost=0.09708094596862793s\n",
            "completed batch 9 of epoch 149. loss=0.4558434784412384. train batch time cost=0.09595274925231934s\n",
            "completed batch 10 of epoch 149. loss=0.10467659682035446. train batch time cost=0.0966181755065918s\n",
            "completed batch 11 of epoch 149. loss=0.1539490967988968. train batch time cost=0.09618020057678223s\n",
            "completed batch 12 of epoch 149. loss=0.11640472710132599. train batch time cost=0.09513187408447266s\n",
            "completed batch 13 of epoch 149. loss=0.2150217890739441. train batch time cost=0.09542417526245117s\n",
            "completed batch 14 of epoch 149. loss=0.04030543193221092. train batch time cost=0.09458303451538086s\n",
            "completed batch 15 of epoch 149. loss=0.06332871317863464. train batch time cost=0.09586095809936523s\n",
            "completed batch 16 of epoch 149. loss=0.14138580858707428. train batch time cost=0.10218954086303711s\n",
            "completed batch 17 of epoch 149. loss=0.3711029291152954. train batch time cost=0.10262775421142578s\n",
            "completed batch 18 of epoch 149. loss=0.08808915317058563. train batch time cost=0.10117149353027344s\n",
            "completed batch 19 of epoch 149. loss=0.024517938494682312. train batch time cost=0.10164928436279297s\n",
            "completed batch 20 of epoch 149. loss=0.05639344081282616. train batch time cost=0.10370159149169922s\n",
            "completed batch 21 of epoch 149. loss=0.11482303589582443. train batch time cost=0.10861611366271973s\n",
            "completed batch 22 of epoch 149. loss=0.10616368055343628. train batch time cost=0.1026601791381836s\n",
            "completed batch 23 of epoch 149. loss=0.07951797544956207. train batch time cost=0.10229611396789551s\n",
            "completed batch 24 of epoch 149. loss=0.06326858699321747. train batch time cost=0.1026906967163086s\n",
            "completed batch 25 of epoch 149. loss=0.09075921028852463. train batch time cost=0.10339736938476562s\n",
            "completed batch 26 of epoch 149. loss=0.05099044740200043. train batch time cost=0.10353755950927734s\n",
            "completed batch 27 of epoch 149. loss=0.19956780970096588. train batch time cost=0.10193300247192383s\n",
            "completed batch 28 of epoch 149. loss=0.1294761449098587. train batch time cost=0.10294342041015625s\n",
            "completed batch 29 of epoch 149. loss=0.007797315716743469. train batch time cost=0.10257196426391602s\n",
            "completed batch 30 of epoch 149. loss=0.0586102232336998. train batch time cost=0.1030423641204834s\n",
            "completed batch 31 of epoch 149. loss=0.0948726236820221. train batch time cost=0.10161733627319336s\n",
            "completed batch 32 of epoch 149. loss=0.08418356627225876. train batch time cost=0.10184049606323242s\n",
            "completed batch 33 of epoch 149. loss=0.024503687396645546. train batch time cost=0.09986042976379395s\n",
            "completed batch 34 of epoch 149. loss=0.1214730367064476. train batch time cost=0.10227608680725098s\n",
            "completed batch 35 of epoch 149. loss=0.12189358472824097. train batch time cost=0.10414671897888184s\n",
            "completed batch 36 of epoch 149. loss=0.012793428264558315. train batch time cost=0.10181498527526855s\n",
            "completed batch 37 of epoch 149. loss=0.08132556080818176. train batch time cost=0.10206198692321777s\n",
            "completed batch 38 of epoch 149. loss=0.026633433997631073. train batch time cost=0.10223054885864258s\n",
            "completed batch 39 of epoch 149. loss=0.10319710522890091. train batch time cost=0.10228204727172852s\n",
            "completed batch 40 of epoch 149. loss=0.016742678359150887. train batch time cost=0.10213351249694824s\n",
            "completed batch 41 of epoch 149. loss=0.05114474147558212. train batch time cost=0.10095691680908203s\n",
            "completed batch 42 of epoch 149. loss=0.08413203060626984. train batch time cost=0.10279512405395508s\n",
            "completed batch 43 of epoch 149. loss=0.1171363964676857. train batch time cost=0.101959228515625s\n",
            "completed batch 44 of epoch 149. loss=0.06823558360338211. train batch time cost=0.1028604507446289s\n",
            "completed batch 45 of epoch 149. loss=0.19496369361877441. train batch time cost=0.10014200210571289s\n",
            "completed batch 46 of epoch 149. loss=0.06362316757440567. train batch time cost=0.10104823112487793s\n",
            "completed batch 47 of epoch 149. loss=0.047717850655317307. train batch time cost=0.10122132301330566s\n",
            "completed batch 48 of epoch 149. loss=0.1166175976395607. train batch time cost=0.10208654403686523s\n",
            "completed batch 49 of epoch 149. loss=0.04099984094500542. train batch time cost=0.10566115379333496s\n",
            "completed batch 50 of epoch 149. loss=0.0862298533320427. train batch time cost=0.10120987892150879s\n",
            "completed batch 51 of epoch 149. loss=0.02486267127096653. train batch time cost=0.10155653953552246s\n",
            "completed batch 52 of epoch 149. loss=0.10888337343931198. train batch time cost=0.1017160415649414s\n",
            "completed batch 53 of epoch 149. loss=0.030558740720152855. train batch time cost=0.10207653045654297s\n",
            "completed batch 54 of epoch 149. loss=0.07126648724079132. train batch time cost=0.10162496566772461s\n",
            "completed batch 55 of epoch 149. loss=0.041547566652297974. train batch time cost=0.10108399391174316s\n",
            "completed batch 56 of epoch 149. loss=0.021957803517580032. train batch time cost=0.10362100601196289s\n",
            "completed batch 57 of epoch 149. loss=0.1101878434419632. train batch time cost=0.10128927230834961s\n",
            "completed batch 58 of epoch 149. loss=0.03662465140223503. train batch time cost=0.10108423233032227s\n",
            "completed batch 59 of epoch 149. loss=0.08990370482206345. train batch time cost=0.10074043273925781s\n",
            "completed batch 60 of epoch 149. loss=0.09639505296945572. train batch time cost=0.1005859375s\n",
            "completed batch 61 of epoch 149. loss=0.01720467582345009. train batch time cost=0.1028604507446289s\n",
            "completed batch 62 of epoch 149. loss=0.03544137626886368. train batch time cost=0.102081298828125s\n",
            "completed batch 63 of epoch 149. loss=0.02049485594034195. train batch time cost=0.10334467887878418s\n",
            "completed batch 64 of epoch 149. loss=0.022614968940615654. train batch time cost=0.1007835865020752s\n",
            "completed batch 65 of epoch 149. loss=0.019081968814134598. train batch time cost=0.1017606258392334s\n",
            "completed batch 66 of epoch 149. loss=0.03386757895350456. train batch time cost=0.10267901420593262s\n",
            "completed batch 67 of epoch 149. loss=0.00938703678548336. train batch time cost=0.10223150253295898s\n",
            "completed batch 68 of epoch 149. loss=0.02113991230726242. train batch time cost=0.10107803344726562s\n",
            "completed batch 69 of epoch 149. loss=0.02153823897242546. train batch time cost=0.10116457939147949s\n",
            "completed batch 70 of epoch 149. loss=0.022265147417783737. train batch time cost=0.10311269760131836s\n",
            "completed batch 71 of epoch 149. loss=0.1498386114835739. train batch time cost=0.10219597816467285s\n",
            "completed batch 72 of epoch 149. loss=0.02740376815199852. train batch time cost=0.1009213924407959s\n",
            "completed batch 73 of epoch 149. loss=0.044843390583992004. train batch time cost=0.10160398483276367s\n",
            "completed batch 74 of epoch 149. loss=0.05879199132323265. train batch time cost=0.10144853591918945s\n",
            "completed batch 75 of epoch 149. loss=0.03138548135757446. train batch time cost=0.10399365425109863s\n",
            "completed batch 76 of epoch 149. loss=0.029535509645938873. train batch time cost=0.10205912590026855s\n",
            "completed batch 77 of epoch 149. loss=0.010392965748906136. train batch time cost=0.10149836540222168s\n",
            "completed batch 78 of epoch 149. loss=0.225838765501976. train batch time cost=0.09594917297363281s\n",
            "completed batch 79 of epoch 149. loss=0.016977693885564804. train batch time cost=0.09560728073120117s\n",
            "completed batch 80 of epoch 149. loss=0.04688544571399689. train batch time cost=0.10123372077941895s\n",
            "completed batch 81 of epoch 149. loss=0.06697069108486176. train batch time cost=0.09524893760681152s\n",
            "completed batch 82 of epoch 149. loss=0.020921295508742332. train batch time cost=0.10284018516540527s\n",
            "completed batch 83 of epoch 149. loss=0.062029965221881866. train batch time cost=0.10131525993347168s\n",
            "completed batch 84 of epoch 149. loss=0.021470770239830017. train batch time cost=0.10179400444030762s\n",
            "completed batch 85 of epoch 149. loss=0.0548870787024498. train batch time cost=0.1007237434387207s\n",
            "completed batch 86 of epoch 149. loss=0.05054895579814911. train batch time cost=0.10158920288085938s\n",
            "completed batch 87 of epoch 149. loss=0.045842934399843216. train batch time cost=0.10133123397827148s\n",
            "completed batch 88 of epoch 149. loss=0.014821397140622139. train batch time cost=0.1021270751953125s\n",
            "completed batch 89 of epoch 149. loss=0.014985420741140842. train batch time cost=0.10263252258300781s\n",
            "completed batch 90 of epoch 149. loss=0.046280886977910995. train batch time cost=0.10092329978942871s\n",
            "completed batch 91 of epoch 149. loss=0.017102805897593498. train batch time cost=0.1014103889465332s\n",
            "completed batch 92 of epoch 149. loss=0.016879471018910408. train batch time cost=0.09784269332885742s\n",
            "completed batch 93 of epoch 149. loss=0.020035993307828903. train batch time cost=0.09516215324401855s\n",
            "completed batch 94 of epoch 149. loss=0.021658465266227722. train batch time cost=0.09451770782470703s\n",
            "completed batch 95 of epoch 149. loss=0.04329999163746834. train batch time cost=0.0939643383026123s\n",
            "completed batch 96 of epoch 149. loss=0.04484394192695618. train batch time cost=0.09481310844421387s\n",
            "completed batch 97 of epoch 149. loss=0.03006746992468834. train batch time cost=0.09509158134460449s\n",
            "completed batch 98 of epoch 149. loss=0.019056789577007294. train batch time cost=0.09540438652038574s\n",
            "completed batch 99 of epoch 149. loss=0.026847615838050842. train batch time cost=0.09547066688537598s\n",
            "completed batch 100 of epoch 149. loss=0.014112552627921104. train batch time cost=0.09463715553283691s\n",
            "completed batch 101 of epoch 149. loss=0.09137497842311859. train batch time cost=0.09478759765625s\n",
            "completed batch 102 of epoch 149. loss=0.035901665687561035. train batch time cost=0.0957021713256836s\n",
            "completed batch 103 of epoch 149. loss=0.02412070892751217. train batch time cost=0.10158538818359375s\n",
            "completed batch 104 of epoch 149. loss=0.0024130542296916246. train batch time cost=0.10201025009155273s\n",
            "completed batch 105 of epoch 149. loss=0.043393757194280624. train batch time cost=0.10094666481018066s\n",
            "completed batch 106 of epoch 149. loss=0.008547818288207054. train batch time cost=0.10092926025390625s\n",
            "completed batch 107 of epoch 149. loss=0.037080176174640656. train batch time cost=0.10097694396972656s\n",
            "completed batch 108 of epoch 149. loss=0.026597363874316216. train batch time cost=0.10138058662414551s\n",
            "completed batch 109 of epoch 149. loss=0.0854559987783432. train batch time cost=0.10118532180786133s\n",
            "completed batch 110 of epoch 149. loss=0.003337643574923277. train batch time cost=0.10125613212585449s\n",
            "completed batch 111 of epoch 149. loss=0.018703283742070198. train batch time cost=0.1022944450378418s\n",
            "completed batch 112 of epoch 149. loss=0.09311118721961975. train batch time cost=0.10203719139099121s\n",
            "completed batch 113 of epoch 149. loss=0.025820398703217506. train batch time cost=0.10222649574279785s\n",
            "completed batch 114 of epoch 149. loss=0.0027641563210636377. train batch time cost=0.10096263885498047s\n",
            "completed batch 115 of epoch 149. loss=0.00273995497263968. train batch time cost=0.10151100158691406s\n",
            "completed batch 116 of epoch 149. loss=0.007995334453880787. train batch time cost=0.10209131240844727s\n",
            "completed batch 117 of epoch 149. loss=0.0033264353405684233. train batch time cost=0.10214567184448242s\n",
            "completed batch 118 of epoch 149. loss=0.027736349031329155. train batch time cost=0.10235595703125s\n",
            "completed batch 119 of epoch 149. loss=0.052812352776527405. train batch time cost=0.10258698463439941s\n",
            "completed batch 120 of epoch 149. loss=0.007232332602143288. train batch time cost=0.10422348976135254s\n",
            "completed batch 121 of epoch 149. loss=0.004290144424885511. train batch time cost=0.1029672622680664s\n",
            "completed batch 122 of epoch 149. loss=0.008016790263354778. train batch time cost=0.10201907157897949s\n",
            "completed batch 123 of epoch 149. loss=0.011134052649140358. train batch time cost=0.10164141654968262s\n",
            "completed batch 124 of epoch 149. loss=0.004603971261531115. train batch time cost=0.10141801834106445s\n",
            "completed batch 125 of epoch 149. loss=0.010613478720188141. train batch time cost=0.10271215438842773s\n",
            "completed batch 126 of epoch 149. loss=0.0001134210906457156. train batch time cost=0.02977776527404785s\n",
            "completed test of epoch 149. loss=0.0001134210906457156. accuracy=0.7398901647528707. train one epoch time cost=27.51375699043274s, test validation time cost=3.864065170288086\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835, 0.655516724912631, 0.655516724912631, 0.6984523215177234, 0.6939590614078882, 0.7134298552171743, 0.6859710434348477, 0.5906140788816775, 0.707938092860709, 0.7398901647528707]\n",
            "completed batch 1 of epoch 150. loss=0.018754303455352783. train batch time cost=0.1014547348022461s\n",
            "completed batch 2 of epoch 150. loss=0.09640461951494217. train batch time cost=0.10186886787414551s\n",
            "completed batch 3 of epoch 150. loss=0.013851107098162174. train batch time cost=0.10160136222839355s\n",
            "completed batch 4 of epoch 150. loss=0.015412417240440845. train batch time cost=0.10273170471191406s\n",
            "completed batch 5 of epoch 150. loss=0.021781787276268005. train batch time cost=0.10262227058410645s\n",
            "completed batch 6 of epoch 150. loss=0.00863772351294756. train batch time cost=0.10176706314086914s\n",
            "completed batch 7 of epoch 150. loss=0.05088675767183304. train batch time cost=0.10184359550476074s\n",
            "completed batch 8 of epoch 150. loss=0.010797507129609585. train batch time cost=0.10156440734863281s\n",
            "completed batch 9 of epoch 150. loss=0.010168979875743389. train batch time cost=0.1010286808013916s\n",
            "completed batch 10 of epoch 150. loss=0.007004349492490292. train batch time cost=0.10186290740966797s\n",
            "completed batch 11 of epoch 150. loss=0.012335044331848621. train batch time cost=0.10243344306945801s\n",
            "completed batch 12 of epoch 150. loss=0.1276317983865738. train batch time cost=0.10134196281433105s\n",
            "completed batch 13 of epoch 150. loss=0.00936058908700943. train batch time cost=0.10174202919006348s\n",
            "completed batch 14 of epoch 150. loss=0.03297117352485657. train batch time cost=0.10252571105957031s\n",
            "completed batch 15 of epoch 150. loss=0.08507776260375977. train batch time cost=0.1031954288482666s\n",
            "completed batch 16 of epoch 150. loss=0.02768940106034279. train batch time cost=0.1025993824005127s\n",
            "completed batch 17 of epoch 150. loss=0.015349771827459335. train batch time cost=0.10221314430236816s\n",
            "completed batch 18 of epoch 150. loss=0.10888427495956421. train batch time cost=0.10252523422241211s\n",
            "completed batch 19 of epoch 150. loss=0.02112477272748947. train batch time cost=0.1027228832244873s\n",
            "completed batch 20 of epoch 150. loss=0.005152077879756689. train batch time cost=0.1017613410949707s\n",
            "completed batch 21 of epoch 150. loss=0.00795296486467123. train batch time cost=0.1009979248046875s\n",
            "completed batch 22 of epoch 150. loss=0.014323819428682327. train batch time cost=0.10231995582580566s\n",
            "completed batch 23 of epoch 150. loss=0.02504831925034523. train batch time cost=0.10348773002624512s\n",
            "completed batch 24 of epoch 150. loss=0.046809013932943344. train batch time cost=0.1021730899810791s\n",
            "completed batch 25 of epoch 150. loss=0.01674414984881878. train batch time cost=0.10125899314880371s\n",
            "completed batch 26 of epoch 150. loss=0.015301376581192017. train batch time cost=0.10155344009399414s\n",
            "completed batch 27 of epoch 150. loss=0.04997368901968002. train batch time cost=0.10286355018615723s\n",
            "completed batch 28 of epoch 150. loss=0.031089944764971733. train batch time cost=0.1029820442199707s\n",
            "completed batch 29 of epoch 150. loss=0.046965546905994415. train batch time cost=0.10352373123168945s\n",
            "completed batch 30 of epoch 150. loss=0.016158852726221085. train batch time cost=0.10330319404602051s\n",
            "completed batch 31 of epoch 150. loss=0.024833660572767258. train batch time cost=0.10179853439331055s\n",
            "completed batch 32 of epoch 150. loss=0.047325123101472855. train batch time cost=0.10156583786010742s\n",
            "completed batch 33 of epoch 150. loss=0.044353410601615906. train batch time cost=0.10055184364318848s\n",
            "completed batch 34 of epoch 150. loss=0.004292245488613844. train batch time cost=0.0958409309387207s\n",
            "completed batch 35 of epoch 150. loss=0.005992816761136055. train batch time cost=0.09473323822021484s\n",
            "completed batch 36 of epoch 150. loss=0.014508395455777645. train batch time cost=0.09531831741333008s\n",
            "completed batch 37 of epoch 150. loss=0.021762143820524216. train batch time cost=0.09677267074584961s\n",
            "completed batch 38 of epoch 150. loss=0.016020987182855606. train batch time cost=0.09711742401123047s\n",
            "completed batch 39 of epoch 150. loss=0.00455063208937645. train batch time cost=0.0976569652557373s\n",
            "completed batch 40 of epoch 150. loss=0.01108571793884039. train batch time cost=0.09574127197265625s\n",
            "completed batch 41 of epoch 150. loss=0.005957525223493576. train batch time cost=0.10097694396972656s\n",
            "completed batch 42 of epoch 150. loss=0.052185721695423126. train batch time cost=0.09572529792785645s\n",
            "completed batch 43 of epoch 150. loss=0.013831356540322304. train batch time cost=0.09584426879882812s\n",
            "completed batch 44 of epoch 150. loss=0.021645396947860718. train batch time cost=0.09614920616149902s\n",
            "completed batch 45 of epoch 150. loss=0.00255763391032815. train batch time cost=0.0952451229095459s\n",
            "completed batch 46 of epoch 150. loss=0.00858386792242527. train batch time cost=0.09590482711791992s\n",
            "completed batch 47 of epoch 150. loss=0.004525474272668362. train batch time cost=0.09560036659240723s\n",
            "completed batch 48 of epoch 150. loss=0.005721497815102339. train batch time cost=0.09685611724853516s\n",
            "completed batch 49 of epoch 150. loss=0.03219651058316231. train batch time cost=0.0951542854309082s\n",
            "completed batch 50 of epoch 150. loss=0.01802520453929901. train batch time cost=0.0950322151184082s\n",
            "completed batch 51 of epoch 150. loss=0.03146806359291077. train batch time cost=0.0975651741027832s\n",
            "completed batch 52 of epoch 150. loss=0.007958424277603626. train batch time cost=0.09884476661682129s\n",
            "completed batch 53 of epoch 150. loss=0.008466415107250214. train batch time cost=0.10365700721740723s\n",
            "completed batch 54 of epoch 150. loss=0.014320692978799343. train batch time cost=0.10387325286865234s\n",
            "completed batch 55 of epoch 150. loss=0.042581234127283096. train batch time cost=0.10445308685302734s\n",
            "completed batch 56 of epoch 150. loss=0.03527415171265602. train batch time cost=0.10284113883972168s\n",
            "completed batch 57 of epoch 150. loss=0.010854780673980713. train batch time cost=0.10315632820129395s\n",
            "completed batch 58 of epoch 150. loss=0.040548257529735565. train batch time cost=0.10287737846374512s\n",
            "completed batch 59 of epoch 150. loss=0.007782614789903164. train batch time cost=0.10392379760742188s\n",
            "completed batch 60 of epoch 150. loss=0.006395931821316481. train batch time cost=0.10292744636535645s\n",
            "completed batch 61 of epoch 150. loss=0.020632227882742882. train batch time cost=0.10320568084716797s\n",
            "completed batch 62 of epoch 150. loss=0.030877133831381798. train batch time cost=0.10280847549438477s\n",
            "completed batch 63 of epoch 150. loss=0.0012262804666534066. train batch time cost=0.10124993324279785s\n",
            "completed batch 64 of epoch 150. loss=0.011666272766888142. train batch time cost=0.10219073295593262s\n",
            "completed batch 65 of epoch 150. loss=0.0031143042724579573. train batch time cost=0.1021275520324707s\n",
            "completed batch 66 of epoch 150. loss=0.008854626677930355. train batch time cost=0.10226202011108398s\n",
            "completed batch 67 of epoch 150. loss=0.0048965527676045895. train batch time cost=0.10096406936645508s\n",
            "completed batch 68 of epoch 150. loss=0.00758082652464509. train batch time cost=0.10279297828674316s\n",
            "completed batch 69 of epoch 150. loss=0.0022015594877302647. train batch time cost=0.10788893699645996s\n",
            "completed batch 70 of epoch 150. loss=0.032384224236011505. train batch time cost=0.09727716445922852s\n",
            "completed batch 71 of epoch 150. loss=0.007172175217419863. train batch time cost=0.09649085998535156s\n",
            "completed batch 72 of epoch 150. loss=0.005503790453076363. train batch time cost=0.09701037406921387s\n",
            "completed batch 73 of epoch 150. loss=0.012583629228174686. train batch time cost=0.1023859977722168s\n",
            "completed batch 74 of epoch 150. loss=0.05456334352493286. train batch time cost=0.1004946231842041s\n",
            "completed batch 75 of epoch 150. loss=0.002910271054133773. train batch time cost=0.10061120986938477s\n",
            "completed batch 76 of epoch 150. loss=0.0043222918175160885. train batch time cost=0.09978818893432617s\n",
            "completed batch 77 of epoch 150. loss=0.01098719984292984. train batch time cost=0.10006093978881836s\n",
            "completed batch 78 of epoch 150. loss=0.02646028622984886. train batch time cost=0.10182309150695801s\n",
            "completed batch 79 of epoch 150. loss=0.007898052223026752. train batch time cost=0.10170865058898926s\n",
            "completed batch 80 of epoch 150. loss=0.04176272451877594. train batch time cost=0.10073733329772949s\n",
            "completed batch 81 of epoch 150. loss=0.011637222021818161. train batch time cost=0.10117077827453613s\n",
            "completed batch 82 of epoch 150. loss=0.005212125368416309. train batch time cost=0.09995317459106445s\n",
            "completed batch 83 of epoch 150. loss=0.010484704747796059. train batch time cost=0.10049939155578613s\n",
            "completed batch 84 of epoch 150. loss=0.12441246956586838. train batch time cost=0.1006765365600586s\n",
            "completed batch 85 of epoch 150. loss=0.010088683106005192. train batch time cost=0.10061097145080566s\n",
            "completed batch 86 of epoch 150. loss=0.002156999194994569. train batch time cost=0.10002422332763672s\n",
            "completed batch 87 of epoch 150. loss=0.024813728407025337. train batch time cost=0.10114288330078125s\n",
            "completed batch 88 of epoch 150. loss=0.037544578313827515. train batch time cost=0.10179972648620605s\n",
            "completed batch 89 of epoch 150. loss=0.009679428301751614. train batch time cost=0.10216522216796875s\n",
            "completed batch 90 of epoch 150. loss=0.0073089697398245335. train batch time cost=0.09983325004577637s\n",
            "completed batch 91 of epoch 150. loss=0.008819297887384892. train batch time cost=0.09726071357727051s\n",
            "completed batch 92 of epoch 150. loss=0.001806720276363194. train batch time cost=0.09635448455810547s\n",
            "completed batch 93 of epoch 150. loss=0.005719953216612339. train batch time cost=0.09495902061462402s\n",
            "completed batch 94 of epoch 150. loss=0.013412305153906345. train batch time cost=0.09571123123168945s\n",
            "completed batch 95 of epoch 150. loss=0.025706669315695763. train batch time cost=0.0941166877746582s\n",
            "completed batch 96 of epoch 150. loss=0.01822020672261715. train batch time cost=0.09409022331237793s\n",
            "completed batch 97 of epoch 150. loss=0.04910038039088249. train batch time cost=0.09533405303955078s\n",
            "completed batch 98 of epoch 150. loss=0.049503520131111145. train batch time cost=0.09418535232543945s\n",
            "completed batch 99 of epoch 150. loss=0.009045206010341644. train batch time cost=0.09419941902160645s\n",
            "completed batch 100 of epoch 150. loss=0.009586388245224953. train batch time cost=0.09640240669250488s\n",
            "completed batch 101 of epoch 150. loss=0.16554228961467743. train batch time cost=0.09541773796081543s\n",
            "completed batch 102 of epoch 150. loss=0.038052815943956375. train batch time cost=0.10256147384643555s\n",
            "completed batch 103 of epoch 150. loss=0.01744695007801056. train batch time cost=0.10155177116394043s\n",
            "completed batch 104 of epoch 150. loss=0.037515949457883835. train batch time cost=0.10137677192687988s\n",
            "completed batch 105 of epoch 150. loss=0.07916326075792313. train batch time cost=0.10122561454772949s\n",
            "completed batch 106 of epoch 150. loss=0.008378737606108189. train batch time cost=0.10171842575073242s\n",
            "completed batch 107 of epoch 150. loss=0.0028945193625986576. train batch time cost=0.10230231285095215s\n",
            "completed batch 108 of epoch 150. loss=0.0075958725064992905. train batch time cost=0.10178637504577637s\n",
            "completed batch 109 of epoch 150. loss=0.0025883205235004425. train batch time cost=0.10215115547180176s\n",
            "completed batch 110 of epoch 150. loss=0.02950463443994522. train batch time cost=0.10109663009643555s\n",
            "completed batch 111 of epoch 150. loss=0.09983164817094803. train batch time cost=0.1033782958984375s\n",
            "completed batch 112 of epoch 150. loss=0.02183801308274269. train batch time cost=0.10115551948547363s\n",
            "completed batch 113 of epoch 150. loss=0.004799208138138056. train batch time cost=0.10076642036437988s\n",
            "completed batch 114 of epoch 150. loss=0.002539670327678323. train batch time cost=0.10318589210510254s\n",
            "completed batch 115 of epoch 150. loss=0.019450165331363678. train batch time cost=0.1038064956665039s\n",
            "completed batch 116 of epoch 150. loss=0.010852648876607418. train batch time cost=0.10303449630737305s\n",
            "completed batch 117 of epoch 150. loss=0.001975852996110916. train batch time cost=0.10507726669311523s\n",
            "completed batch 118 of epoch 150. loss=0.005057053174823523. train batch time cost=0.10056638717651367s\n",
            "completed batch 119 of epoch 150. loss=0.00603888463228941. train batch time cost=0.10095715522766113s\n",
            "completed batch 120 of epoch 150. loss=0.00742507167160511. train batch time cost=0.10080075263977051s\n",
            "completed batch 121 of epoch 150. loss=0.014235984534025192. train batch time cost=0.10051608085632324s\n",
            "completed batch 122 of epoch 150. loss=0.06480073928833008. train batch time cost=0.10063886642456055s\n",
            "completed batch 123 of epoch 150. loss=0.004234349820762873. train batch time cost=0.10018062591552734s\n",
            "completed batch 124 of epoch 150. loss=0.009085764177143574. train batch time cost=0.10177898406982422s\n",
            "completed batch 125 of epoch 150. loss=0.011874721385538578. train batch time cost=0.10274386405944824s\n",
            "completed batch 126 of epoch 150. loss=0.00041470766882412136. train batch time cost=0.02955913543701172s\n",
            "completed test of epoch 150. loss=0.00041470766882412136. accuracy=0.6949575636545182. train one epoch time cost=27.48966383934021s, test validation time cost=3.810765504837036\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835, 0.655516724912631, 0.655516724912631, 0.6984523215177234, 0.6939590614078882, 0.7134298552171743, 0.6859710434348477, 0.5906140788816775, 0.707938092860709, 0.7398901647528707, 0.6949575636545182]\n",
            "completed batch 1 of epoch 151. loss=0.003505406901240349. train batch time cost=0.101806640625s\n",
            "completed batch 2 of epoch 151. loss=0.028239458799362183. train batch time cost=0.10147404670715332s\n",
            "completed batch 3 of epoch 151. loss=0.005827503278851509. train batch time cost=0.10226774215698242s\n",
            "completed batch 4 of epoch 151. loss=0.02243664488196373. train batch time cost=0.10184574127197266s\n",
            "completed batch 5 of epoch 151. loss=0.0056017376482486725. train batch time cost=0.09607100486755371s\n",
            "completed batch 6 of epoch 151. loss=0.018267087638378143. train batch time cost=0.09617185592651367s\n",
            "completed batch 7 of epoch 151. loss=0.05820143595337868. train batch time cost=0.09545469284057617s\n",
            "completed batch 8 of epoch 151. loss=0.005868788342922926. train batch time cost=0.09559750556945801s\n",
            "completed batch 9 of epoch 151. loss=0.076615110039711. train batch time cost=0.09583663940429688s\n",
            "completed batch 10 of epoch 151. loss=0.23915724456310272. train batch time cost=0.09672260284423828s\n",
            "completed batch 11 of epoch 151. loss=0.03167829290032387. train batch time cost=0.09525513648986816s\n",
            "completed batch 12 of epoch 151. loss=0.02212289720773697. train batch time cost=0.09520673751831055s\n",
            "completed batch 13 of epoch 151. loss=0.020709915086627007. train batch time cost=0.09581661224365234s\n",
            "completed batch 14 of epoch 151. loss=0.02278180979192257. train batch time cost=0.09567952156066895s\n",
            "completed batch 15 of epoch 151. loss=0.02987353876233101. train batch time cost=0.09694147109985352s\n",
            "completed batch 16 of epoch 151. loss=0.008955197408795357. train batch time cost=0.0949716567993164s\n",
            "completed batch 17 of epoch 151. loss=0.004129540640860796. train batch time cost=0.09517407417297363s\n",
            "completed batch 18 of epoch 151. loss=0.018750648945569992. train batch time cost=0.09700989723205566s\n",
            "completed batch 19 of epoch 151. loss=0.02149219997227192. train batch time cost=0.09470033645629883s\n",
            "completed batch 20 of epoch 151. loss=0.008477441035211086. train batch time cost=0.09543085098266602s\n",
            "completed batch 21 of epoch 151. loss=0.011907188221812248. train batch time cost=0.09463620185852051s\n",
            "completed batch 22 of epoch 151. loss=0.0055223796516656876. train batch time cost=0.09596467018127441s\n",
            "completed batch 23 of epoch 151. loss=0.013075897470116615. train batch time cost=0.09706854820251465s\n",
            "completed batch 24 of epoch 151. loss=0.014037901535630226. train batch time cost=0.10019612312316895s\n",
            "completed batch 25 of epoch 151. loss=0.0054451096802949905. train batch time cost=0.10237669944763184s\n",
            "completed batch 26 of epoch 151. loss=0.05717374384403229. train batch time cost=0.0990145206451416s\n",
            "completed batch 27 of epoch 151. loss=0.0018163431668654084. train batch time cost=0.10046815872192383s\n",
            "completed batch 28 of epoch 151. loss=0.009747423231601715. train batch time cost=0.10108733177185059s\n",
            "completed batch 29 of epoch 151. loss=0.004426420666277409. train batch time cost=0.10375308990478516s\n",
            "completed batch 30 of epoch 151. loss=0.00579682644456625. train batch time cost=0.09842538833618164s\n",
            "completed batch 31 of epoch 151. loss=0.027245812118053436. train batch time cost=0.10327935218811035s\n",
            "completed batch 32 of epoch 151. loss=0.016305914148688316. train batch time cost=0.10160207748413086s\n",
            "completed batch 33 of epoch 151. loss=0.012161575257778168. train batch time cost=0.10307884216308594s\n",
            "completed batch 34 of epoch 151. loss=0.055403705686330795. train batch time cost=0.10364270210266113s\n",
            "completed batch 35 of epoch 151. loss=0.026878925040364265. train batch time cost=0.10234451293945312s\n",
            "completed batch 36 of epoch 151. loss=0.010613372549414635. train batch time cost=0.10206246376037598s\n",
            "completed batch 37 of epoch 151. loss=0.009178615175187588. train batch time cost=0.09750723838806152s\n",
            "completed batch 38 of epoch 151. loss=0.0011077906237915158. train batch time cost=0.09584546089172363s\n",
            "completed batch 39 of epoch 151. loss=0.07257836312055588. train batch time cost=0.09794259071350098s\n",
            "completed batch 40 of epoch 151. loss=0.007743753492832184. train batch time cost=0.09603047370910645s\n",
            "completed batch 41 of epoch 151. loss=0.004964486695826054. train batch time cost=0.09541463851928711s\n",
            "completed batch 42 of epoch 151. loss=0.005391801241785288. train batch time cost=0.09760284423828125s\n",
            "completed batch 43 of epoch 151. loss=0.014110332354903221. train batch time cost=0.09567451477050781s\n",
            "completed batch 44 of epoch 151. loss=0.04402589797973633. train batch time cost=0.09557771682739258s\n",
            "completed batch 45 of epoch 151. loss=0.01781262829899788. train batch time cost=0.0967414379119873s\n",
            "completed batch 46 of epoch 151. loss=0.008968277834355831. train batch time cost=0.09511518478393555s\n",
            "completed batch 47 of epoch 151. loss=0.015590168535709381. train batch time cost=0.0944681167602539s\n",
            "completed batch 48 of epoch 151. loss=0.015641814097762108. train batch time cost=0.10136771202087402s\n",
            "completed batch 49 of epoch 151. loss=0.002288345480337739. train batch time cost=0.10080909729003906s\n",
            "completed batch 50 of epoch 151. loss=0.0044310991652309895. train batch time cost=0.10215353965759277s\n",
            "completed batch 51 of epoch 151. loss=0.15137307345867157. train batch time cost=0.10199189186096191s\n",
            "completed batch 52 of epoch 151. loss=0.00410018116235733. train batch time cost=0.10238313674926758s\n",
            "completed batch 53 of epoch 151. loss=0.006647361908107996. train batch time cost=0.10251283645629883s\n",
            "completed batch 54 of epoch 151. loss=0.007613027933984995. train batch time cost=0.10309410095214844s\n",
            "completed batch 55 of epoch 151. loss=0.08753716945648193. train batch time cost=0.10233664512634277s\n",
            "completed batch 56 of epoch 151. loss=0.0020362399518489838. train batch time cost=0.10313129425048828s\n",
            "completed batch 57 of epoch 151. loss=0.019734784960746765. train batch time cost=0.10380864143371582s\n",
            "completed batch 58 of epoch 151. loss=0.03956429660320282. train batch time cost=0.10322284698486328s\n",
            "completed batch 59 of epoch 151. loss=0.026176568120718002. train batch time cost=0.10302186012268066s\n",
            "completed batch 60 of epoch 151. loss=0.00844205915927887. train batch time cost=0.10199689865112305s\n",
            "completed batch 61 of epoch 151. loss=0.00608806824311614. train batch time cost=0.10207819938659668s\n",
            "completed batch 62 of epoch 151. loss=0.017636699602007866. train batch time cost=0.10278630256652832s\n",
            "completed batch 63 of epoch 151. loss=0.0044693052768707275. train batch time cost=0.10373258590698242s\n",
            "completed batch 64 of epoch 151. loss=0.0024848466273397207. train batch time cost=0.1022789478302002s\n",
            "completed batch 65 of epoch 151. loss=0.002310813870280981. train batch time cost=0.10272407531738281s\n",
            "completed batch 66 of epoch 151. loss=0.020369432866573334. train batch time cost=0.10284614562988281s\n",
            "completed batch 67 of epoch 151. loss=0.0817539244890213. train batch time cost=0.10116291046142578s\n",
            "completed batch 68 of epoch 151. loss=0.01787358708679676. train batch time cost=0.10216331481933594s\n",
            "completed batch 69 of epoch 151. loss=0.02457752637565136. train batch time cost=0.10244917869567871s\n",
            "completed batch 70 of epoch 151. loss=0.02992679551243782. train batch time cost=0.10250306129455566s\n",
            "completed batch 71 of epoch 151. loss=0.02035859040915966. train batch time cost=0.10195803642272949s\n",
            "completed batch 72 of epoch 151. loss=0.06089167669415474. train batch time cost=0.10301637649536133s\n",
            "completed batch 73 of epoch 151. loss=0.04369782656431198. train batch time cost=0.10147809982299805s\n",
            "completed batch 74 of epoch 151. loss=0.124740831553936. train batch time cost=0.10193896293640137s\n",
            "completed batch 75 of epoch 151. loss=0.09465623646974564. train batch time cost=0.10260510444641113s\n",
            "completed batch 76 of epoch 151. loss=0.0476020947098732. train batch time cost=0.10204458236694336s\n",
            "completed batch 77 of epoch 151. loss=0.014582921750843525. train batch time cost=0.10262203216552734s\n",
            "completed batch 78 of epoch 151. loss=0.07119055092334747. train batch time cost=0.10051655769348145s\n",
            "completed batch 79 of epoch 151. loss=0.10579796135425568. train batch time cost=0.10227775573730469s\n",
            "completed batch 80 of epoch 151. loss=0.005008591338992119. train batch time cost=0.10165786743164062s\n",
            "completed batch 81 of epoch 151. loss=0.021160515025258064. train batch time cost=0.10383176803588867s\n",
            "completed batch 82 of epoch 151. loss=0.040088627487421036. train batch time cost=0.10046267509460449s\n",
            "completed batch 83 of epoch 151. loss=0.030824784189462662. train batch time cost=0.10167837142944336s\n",
            "completed batch 84 of epoch 151. loss=0.14418575167655945. train batch time cost=0.10187077522277832s\n",
            "completed batch 85 of epoch 151. loss=0.04120685160160065. train batch time cost=0.10184454917907715s\n",
            "completed batch 86 of epoch 151. loss=0.014025716111063957. train batch time cost=0.10300111770629883s\n",
            "completed batch 87 of epoch 151. loss=0.021957991644740105. train batch time cost=0.1043245792388916s\n",
            "completed batch 88 of epoch 151. loss=0.026114854961633682. train batch time cost=0.10311341285705566s\n",
            "completed batch 89 of epoch 151. loss=0.03612169250845909. train batch time cost=0.10597586631774902s\n",
            "completed batch 90 of epoch 151. loss=0.009236961603164673. train batch time cost=0.10438108444213867s\n",
            "completed batch 91 of epoch 151. loss=0.08693215250968933. train batch time cost=0.10099387168884277s\n",
            "completed batch 92 of epoch 151. loss=0.008396031334996223. train batch time cost=0.10131025314331055s\n",
            "completed batch 93 of epoch 151. loss=0.0456216000020504. train batch time cost=0.10245990753173828s\n",
            "completed batch 94 of epoch 151. loss=0.02495281584560871. train batch time cost=0.10323739051818848s\n",
            "completed batch 95 of epoch 151. loss=0.12411629408597946. train batch time cost=0.10258102416992188s\n",
            "completed batch 96 of epoch 151. loss=0.01273256167769432. train batch time cost=0.10372066497802734s\n",
            "completed batch 97 of epoch 151. loss=0.16398616135120392. train batch time cost=0.10226202011108398s\n",
            "completed batch 98 of epoch 151. loss=0.028143439441919327. train batch time cost=0.10306715965270996s\n",
            "completed batch 99 of epoch 151. loss=0.034580912441015244. train batch time cost=0.10255146026611328s\n",
            "completed batch 100 of epoch 151. loss=0.035479605197906494. train batch time cost=0.10232210159301758s\n",
            "completed batch 101 of epoch 151. loss=0.005367026664316654. train batch time cost=0.10028600692749023s\n",
            "completed batch 102 of epoch 151. loss=0.17516405880451202. train batch time cost=0.09663605690002441s\n",
            "completed batch 103 of epoch 151. loss=0.4251401424407959. train batch time cost=0.1016683578491211s\n",
            "completed batch 104 of epoch 151. loss=0.0740613341331482. train batch time cost=0.1024160385131836s\n",
            "completed batch 105 of epoch 151. loss=0.004917379003018141. train batch time cost=0.10062170028686523s\n",
            "completed batch 106 of epoch 151. loss=0.08467470854520798. train batch time cost=0.10014009475708008s\n",
            "completed batch 107 of epoch 151. loss=0.05961183086037636. train batch time cost=0.10228466987609863s\n",
            "completed batch 108 of epoch 151. loss=0.014579739421606064. train batch time cost=0.10135865211486816s\n",
            "completed batch 109 of epoch 151. loss=0.019111447036266327. train batch time cost=0.10076642036437988s\n",
            "completed batch 110 of epoch 151. loss=0.009232518263161182. train batch time cost=0.10070657730102539s\n",
            "completed batch 111 of epoch 151. loss=0.03288840129971504. train batch time cost=0.10181975364685059s\n",
            "completed batch 112 of epoch 151. loss=0.033468227833509445. train batch time cost=0.10185599327087402s\n",
            "completed batch 113 of epoch 151. loss=0.10285010933876038. train batch time cost=0.10583662986755371s\n",
            "completed batch 114 of epoch 151. loss=0.05314559116959572. train batch time cost=0.10080981254577637s\n",
            "completed batch 115 of epoch 151. loss=0.02400151826441288. train batch time cost=0.1003410816192627s\n",
            "completed batch 116 of epoch 151. loss=0.07878439873456955. train batch time cost=0.10095763206481934s\n",
            "completed batch 117 of epoch 151. loss=0.007740497123450041. train batch time cost=0.10126161575317383s\n",
            "completed batch 118 of epoch 151. loss=0.04656966030597687. train batch time cost=0.10201501846313477s\n",
            "completed batch 119 of epoch 151. loss=0.03943946957588196. train batch time cost=0.1015326976776123s\n",
            "completed batch 120 of epoch 151. loss=0.01782625913619995. train batch time cost=0.10194778442382812s\n",
            "completed batch 121 of epoch 151. loss=0.01627480797469616. train batch time cost=0.10190725326538086s\n",
            "completed batch 122 of epoch 151. loss=0.06448572874069214. train batch time cost=0.09830188751220703s\n",
            "completed batch 123 of epoch 151. loss=0.07595096528530121. train batch time cost=0.09493756294250488s\n",
            "completed batch 124 of epoch 151. loss=0.008929627016186714. train batch time cost=0.10076189041137695s\n",
            "completed batch 125 of epoch 151. loss=0.02171952836215496. train batch time cost=0.10143184661865234s\n",
            "completed batch 126 of epoch 151. loss=0.00010267868492519483. train batch time cost=0.02965688705444336s\n",
            "completed test of epoch 151. loss=0.00010267868492519483. accuracy=0.7129306040938592. train one epoch time cost=27.498796224594116s, test validation time cost=3.9784293174743652\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835, 0.655516724912631, 0.655516724912631, 0.6984523215177234, 0.6939590614078882, 0.7134298552171743, 0.6859710434348477, 0.5906140788816775, 0.707938092860709, 0.7398901647528707, 0.6949575636545182, 0.7129306040938592]\n",
            "completed batch 1 of epoch 152. loss=0.018012115731835365. train batch time cost=0.1011512279510498s\n",
            "completed batch 2 of epoch 152. loss=0.0369931198656559. train batch time cost=0.10441040992736816s\n",
            "completed batch 3 of epoch 152. loss=0.1349785476922989. train batch time cost=0.10191226005554199s\n",
            "completed batch 4 of epoch 152. loss=0.04964488744735718. train batch time cost=0.10359811782836914s\n",
            "completed batch 5 of epoch 152. loss=0.019195985049009323. train batch time cost=0.10390877723693848s\n",
            "completed batch 6 of epoch 152. loss=0.14793692529201508. train batch time cost=0.10387730598449707s\n",
            "completed batch 7 of epoch 152. loss=0.05267927423119545. train batch time cost=0.10924029350280762s\n",
            "completed batch 8 of epoch 152. loss=0.038039430975914. train batch time cost=0.10172700881958008s\n",
            "completed batch 9 of epoch 152. loss=0.04536738991737366. train batch time cost=0.10341167449951172s\n",
            "completed batch 10 of epoch 152. loss=0.010405763983726501. train batch time cost=0.10315585136413574s\n",
            "completed batch 11 of epoch 152. loss=0.062300607562065125. train batch time cost=0.10177063941955566s\n",
            "completed batch 12 of epoch 152. loss=0.06456911563873291. train batch time cost=0.10156559944152832s\n",
            "completed batch 13 of epoch 152. loss=0.08283688127994537. train batch time cost=0.10175466537475586s\n",
            "completed batch 14 of epoch 152. loss=0.08657166361808777. train batch time cost=0.10312867164611816s\n",
            "completed batch 15 of epoch 152. loss=0.014582033269107342. train batch time cost=0.10335159301757812s\n",
            "completed batch 16 of epoch 152. loss=0.01026720367372036. train batch time cost=0.10926032066345215s\n",
            "completed batch 17 of epoch 152. loss=0.007295845076441765. train batch time cost=0.10095500946044922s\n",
            "completed batch 18 of epoch 152. loss=0.029731661081314087. train batch time cost=0.10172581672668457s\n",
            "completed batch 19 of epoch 152. loss=0.018382776528596878. train batch time cost=0.10191535949707031s\n",
            "completed batch 20 of epoch 152. loss=0.018811270594596863. train batch time cost=0.10154175758361816s\n",
            "completed batch 21 of epoch 152. loss=0.02579462341964245. train batch time cost=0.101318359375s\n",
            "completed batch 22 of epoch 152. loss=0.11608419567346573. train batch time cost=0.10143685340881348s\n",
            "completed batch 23 of epoch 152. loss=0.008420152589678764. train batch time cost=0.10325980186462402s\n",
            "completed batch 24 of epoch 152. loss=0.01079073641449213. train batch time cost=0.10292387008666992s\n",
            "completed batch 25 of epoch 152. loss=0.04049825668334961. train batch time cost=0.10167455673217773s\n",
            "completed batch 26 of epoch 152. loss=0.016054823994636536. train batch time cost=0.10205745697021484s\n",
            "completed batch 27 of epoch 152. loss=0.10656620562076569. train batch time cost=0.10093569755554199s\n",
            "completed batch 28 of epoch 152. loss=0.07632170617580414. train batch time cost=0.10281562805175781s\n",
            "completed batch 29 of epoch 152. loss=0.02812805213034153. train batch time cost=0.10259842872619629s\n",
            "completed batch 30 of epoch 152. loss=0.016807138919830322. train batch time cost=0.10292482376098633s\n",
            "completed batch 31 of epoch 152. loss=0.04300832003355026. train batch time cost=0.10155487060546875s\n",
            "completed batch 32 of epoch 152. loss=0.05762118101119995. train batch time cost=0.10233068466186523s\n",
            "completed batch 33 of epoch 152. loss=0.008221794851124287. train batch time cost=0.10356950759887695s\n",
            "completed batch 34 of epoch 152. loss=0.04539738968014717. train batch time cost=0.10159039497375488s\n",
            "completed batch 35 of epoch 152. loss=0.018392285332083702. train batch time cost=0.10066795349121094s\n",
            "completed batch 36 of epoch 152. loss=0.008735593408346176. train batch time cost=0.10240316390991211s\n",
            "completed batch 37 of epoch 152. loss=0.007676903624087572. train batch time cost=0.10101127624511719s\n",
            "completed batch 38 of epoch 152. loss=0.04561673104763031. train batch time cost=0.10082626342773438s\n",
            "completed batch 39 of epoch 152. loss=0.05962991714477539. train batch time cost=0.09999370574951172s\n",
            "completed batch 40 of epoch 152. loss=0.0038396466989070177. train batch time cost=0.10125899314880371s\n",
            "completed batch 41 of epoch 152. loss=0.048287831246852875. train batch time cost=0.10150480270385742s\n",
            "completed batch 42 of epoch 152. loss=0.010774285532534122. train batch time cost=0.1015312671661377s\n",
            "completed batch 43 of epoch 152. loss=0.005336213856935501. train batch time cost=0.10173559188842773s\n",
            "completed batch 44 of epoch 152. loss=0.005775967612862587. train batch time cost=0.10967755317687988s\n",
            "completed batch 45 of epoch 152. loss=0.005387515295296907. train batch time cost=0.10101699829101562s\n",
            "completed batch 46 of epoch 152. loss=0.007553829811513424. train batch time cost=0.10074520111083984s\n",
            "completed batch 47 of epoch 152. loss=0.056927576661109924. train batch time cost=0.10279488563537598s\n",
            "completed batch 48 of epoch 152. loss=0.0628817155957222. train batch time cost=0.10109758377075195s\n",
            "completed batch 49 of epoch 152. loss=0.01663621887564659. train batch time cost=0.10051417350769043s\n",
            "completed batch 50 of epoch 152. loss=0.13696050643920898. train batch time cost=0.10124969482421875s\n",
            "completed batch 51 of epoch 152. loss=0.001595646608620882. train batch time cost=0.10157299041748047s\n",
            "completed batch 52 of epoch 152. loss=0.04842882603406906. train batch time cost=0.10180449485778809s\n",
            "completed batch 53 of epoch 152. loss=0.004250188358128071. train batch time cost=0.10102581977844238s\n",
            "completed batch 54 of epoch 152. loss=0.039788708090782166. train batch time cost=0.10221004486083984s\n",
            "completed batch 55 of epoch 152. loss=0.07021831721067429. train batch time cost=0.10337710380554199s\n",
            "completed batch 56 of epoch 152. loss=0.04704621061682701. train batch time cost=0.10563182830810547s\n",
            "completed batch 57 of epoch 152. loss=0.001980246976017952. train batch time cost=0.10154056549072266s\n",
            "completed batch 58 of epoch 152. loss=0.01096369232982397. train batch time cost=0.1015169620513916s\n",
            "completed batch 59 of epoch 152. loss=0.02483060024678707. train batch time cost=0.10126304626464844s\n",
            "completed batch 60 of epoch 152. loss=0.023392513394355774. train batch time cost=0.10283994674682617s\n",
            "completed batch 61 of epoch 152. loss=0.0031713717617094517. train batch time cost=0.10122966766357422s\n",
            "completed batch 62 of epoch 152. loss=0.010029595345258713. train batch time cost=0.10101532936096191s\n",
            "completed batch 63 of epoch 152. loss=0.01624714583158493. train batch time cost=0.10077738761901855s\n",
            "completed batch 64 of epoch 152. loss=0.024897495284676552. train batch time cost=0.10187506675720215s\n",
            "completed batch 65 of epoch 152. loss=0.07746446132659912. train batch time cost=0.09514856338500977s\n",
            "completed batch 66 of epoch 152. loss=0.10108501464128494. train batch time cost=0.10117340087890625s\n",
            "completed batch 67 of epoch 152. loss=0.0026932409964501858. train batch time cost=0.10016322135925293s\n",
            "completed batch 68 of epoch 152. loss=0.009500902146100998. train batch time cost=0.10024523735046387s\n",
            "completed batch 69 of epoch 152. loss=0.013702023774385452. train batch time cost=0.10062599182128906s\n",
            "completed batch 70 of epoch 152. loss=0.006866091396659613. train batch time cost=0.10264182090759277s\n",
            "completed batch 71 of epoch 152. loss=0.03176058828830719. train batch time cost=0.10152864456176758s\n",
            "completed batch 72 of epoch 152. loss=0.03458394110202789. train batch time cost=0.10281968116760254s\n",
            "completed batch 73 of epoch 152. loss=0.015955153852701187. train batch time cost=0.09570717811584473s\n",
            "completed batch 74 of epoch 152. loss=0.010546987876296043. train batch time cost=0.0968170166015625s\n",
            "completed batch 75 of epoch 152. loss=0.00855298899114132. train batch time cost=0.09741616249084473s\n",
            "completed batch 76 of epoch 152. loss=0.014436370693147182. train batch time cost=0.09627628326416016s\n",
            "completed batch 77 of epoch 152. loss=0.008661755360662937. train batch time cost=0.09531903266906738s\n",
            "completed batch 78 of epoch 152. loss=0.06926332414150238. train batch time cost=0.09646773338317871s\n",
            "completed batch 79 of epoch 152. loss=0.008527714759111404. train batch time cost=0.09610342979431152s\n",
            "completed batch 80 of epoch 152. loss=0.009084268473088741. train batch time cost=0.09922933578491211s\n",
            "completed batch 81 of epoch 152. loss=0.015678009018301964. train batch time cost=0.09392905235290527s\n",
            "completed batch 82 of epoch 152. loss=0.005496322177350521. train batch time cost=0.09575247764587402s\n",
            "completed batch 83 of epoch 152. loss=0.009176522493362427. train batch time cost=0.09544658660888672s\n",
            "completed batch 84 of epoch 152. loss=0.06143661588430405. train batch time cost=0.09572386741638184s\n",
            "completed batch 85 of epoch 152. loss=0.012150329537689686. train batch time cost=0.09964561462402344s\n",
            "completed batch 86 of epoch 152. loss=0.039653435349464417. train batch time cost=0.09691810607910156s\n",
            "completed batch 87 of epoch 152. loss=0.0057949163019657135. train batch time cost=0.09476613998413086s\n",
            "completed batch 88 of epoch 152. loss=0.01966334693133831. train batch time cost=0.09534192085266113s\n",
            "completed batch 89 of epoch 152. loss=0.010962956584990025. train batch time cost=0.0954890251159668s\n",
            "completed batch 90 of epoch 152. loss=0.005238007288426161. train batch time cost=0.09582996368408203s\n",
            "completed batch 91 of epoch 152. loss=0.04684261605143547. train batch time cost=0.09521365165710449s\n",
            "completed batch 92 of epoch 152. loss=0.005887854378670454. train batch time cost=0.09637236595153809s\n",
            "completed batch 93 of epoch 152. loss=0.025193385779857635. train batch time cost=0.095306396484375s\n",
            "completed batch 94 of epoch 152. loss=0.006901770830154419. train batch time cost=0.10131430625915527s\n",
            "completed batch 95 of epoch 152. loss=0.013892083428800106. train batch time cost=0.10130167007446289s\n",
            "completed batch 96 of epoch 152. loss=0.012304496020078659. train batch time cost=0.10125255584716797s\n",
            "completed batch 97 of epoch 152. loss=0.03431076928973198. train batch time cost=0.10300326347351074s\n",
            "completed batch 98 of epoch 152. loss=0.029433270916342735. train batch time cost=0.1019139289855957s\n",
            "completed batch 99 of epoch 152. loss=0.056285351514816284. train batch time cost=0.10245704650878906s\n",
            "completed batch 100 of epoch 152. loss=0.08058207482099533. train batch time cost=0.1080026626586914s\n",
            "completed batch 101 of epoch 152. loss=0.3423336446285248. train batch time cost=0.10247087478637695s\n",
            "completed batch 102 of epoch 152. loss=0.029056847095489502. train batch time cost=0.10406494140625s\n",
            "completed batch 103 of epoch 152. loss=0.0278560109436512. train batch time cost=0.10169363021850586s\n",
            "completed batch 104 of epoch 152. loss=0.02454742230474949. train batch time cost=0.10370421409606934s\n",
            "completed batch 105 of epoch 152. loss=0.009980899281799793. train batch time cost=0.10753154754638672s\n",
            "completed batch 106 of epoch 152. loss=0.023082740604877472. train batch time cost=0.10186481475830078s\n",
            "completed batch 107 of epoch 152. loss=0.007349833380430937. train batch time cost=0.10146236419677734s\n",
            "completed batch 108 of epoch 152. loss=0.020774971693754196. train batch time cost=0.10235595703125s\n",
            "completed batch 109 of epoch 152. loss=0.019111471250653267. train batch time cost=0.10099911689758301s\n",
            "completed batch 110 of epoch 152. loss=0.010357559658586979. train batch time cost=0.10103511810302734s\n",
            "completed batch 111 of epoch 152. loss=0.002498165937140584. train batch time cost=0.1029348373413086s\n",
            "completed batch 112 of epoch 152. loss=0.04318874329328537. train batch time cost=0.1015012264251709s\n",
            "completed batch 113 of epoch 152. loss=0.016668280586600304. train batch time cost=0.1031029224395752s\n",
            "completed batch 114 of epoch 152. loss=0.027242492884397507. train batch time cost=0.10254049301147461s\n",
            "completed batch 115 of epoch 152. loss=0.019243577495217323. train batch time cost=0.10377955436706543s\n",
            "completed batch 116 of epoch 152. loss=0.02545296773314476. train batch time cost=0.10305905342102051s\n",
            "completed batch 117 of epoch 152. loss=0.0025143723469227552. train batch time cost=0.10203003883361816s\n",
            "completed batch 118 of epoch 152. loss=0.013405326753854752. train batch time cost=0.10190200805664062s\n",
            "completed batch 119 of epoch 152. loss=0.0152730792760849. train batch time cost=0.10204744338989258s\n",
            "completed batch 120 of epoch 152. loss=0.02048512175679207. train batch time cost=0.10239601135253906s\n",
            "completed batch 121 of epoch 152. loss=0.008380172774195671. train batch time cost=0.10160207748413086s\n",
            "completed batch 122 of epoch 152. loss=0.004871985409408808. train batch time cost=0.10135650634765625s\n",
            "completed batch 123 of epoch 152. loss=0.012036606669425964. train batch time cost=0.10219264030456543s\n",
            "completed batch 124 of epoch 152. loss=0.01546639483422041. train batch time cost=0.10358667373657227s\n",
            "completed batch 125 of epoch 152. loss=0.06339716166257858. train batch time cost=0.10435080528259277s\n",
            "completed batch 126 of epoch 152. loss=8.073668141150847e-05. train batch time cost=0.02975177764892578s\n",
            "completed test of epoch 152. loss=8.073668141150847e-05. accuracy=0.7383924113829257. train one epoch time cost=27.58855128288269s, test validation time cost=3.982846260070801\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835, 0.655516724912631, 0.655516724912631, 0.6984523215177234, 0.6939590614078882, 0.7134298552171743, 0.6859710434348477, 0.5906140788816775, 0.707938092860709, 0.7398901647528707, 0.6949575636545182, 0.7129306040938592, 0.7383924113829257]\n",
            "completed batch 1 of epoch 153. loss=0.06402335315942764. train batch time cost=0.10049676895141602s\n",
            "completed batch 2 of epoch 153. loss=0.013271626085042953. train batch time cost=0.10060238838195801s\n",
            "completed batch 3 of epoch 153. loss=0.001721997861750424. train batch time cost=0.10393118858337402s\n",
            "completed batch 4 of epoch 153. loss=0.01215590164065361. train batch time cost=0.10207390785217285s\n",
            "completed batch 5 of epoch 153. loss=0.0021956968121230602. train batch time cost=0.10131454467773438s\n",
            "completed batch 6 of epoch 153. loss=0.005350517574697733. train batch time cost=0.10251045227050781s\n",
            "completed batch 7 of epoch 153. loss=0.010724730789661407. train batch time cost=0.10367226600646973s\n",
            "completed batch 8 of epoch 153. loss=0.011360346339643002. train batch time cost=0.10688924789428711s\n",
            "completed batch 9 of epoch 153. loss=0.0053741405718028545. train batch time cost=0.1030430793762207s\n",
            "completed batch 10 of epoch 153. loss=0.06266740709543228. train batch time cost=0.10286664962768555s\n",
            "completed batch 11 of epoch 153. loss=0.05720103532075882. train batch time cost=0.1023702621459961s\n",
            "completed batch 12 of epoch 153. loss=0.005361452698707581. train batch time cost=0.10182785987854004s\n",
            "completed batch 13 of epoch 153. loss=0.016750996932387352. train batch time cost=0.10167837142944336s\n",
            "completed batch 14 of epoch 153. loss=0.0021004003938287497. train batch time cost=0.10147666931152344s\n",
            "completed batch 15 of epoch 153. loss=0.05997298285365105. train batch time cost=0.10296988487243652s\n",
            "completed batch 16 of epoch 153. loss=0.005459927022457123. train batch time cost=0.10246896743774414s\n",
            "completed batch 17 of epoch 153. loss=0.0028142472729086876. train batch time cost=0.10456299781799316s\n",
            "completed batch 18 of epoch 153. loss=0.0011161738075315952. train batch time cost=0.10422849655151367s\n",
            "completed batch 19 of epoch 153. loss=0.0044953166507184505. train batch time cost=0.10269308090209961s\n",
            "completed batch 20 of epoch 153. loss=0.01195202860981226. train batch time cost=0.10310626029968262s\n",
            "completed batch 21 of epoch 153. loss=0.05479344725608826. train batch time cost=0.1028432846069336s\n",
            "completed batch 22 of epoch 153. loss=0.01012672483921051. train batch time cost=0.10279345512390137s\n",
            "completed batch 23 of epoch 153. loss=0.07917101681232452. train batch time cost=0.10222029685974121s\n",
            "completed batch 24 of epoch 153. loss=0.037430111318826675. train batch time cost=0.10526680946350098s\n",
            "completed batch 25 of epoch 153. loss=0.028234336525201797. train batch time cost=0.1020963191986084s\n",
            "completed batch 26 of epoch 153. loss=0.0014737261226400733. train batch time cost=0.1013951301574707s\n",
            "completed batch 27 of epoch 153. loss=0.01141134463250637. train batch time cost=0.10465550422668457s\n",
            "completed batch 28 of epoch 153. loss=0.006015115417540073. train batch time cost=0.1024627685546875s\n",
            "completed batch 29 of epoch 153. loss=0.03516200929880142. train batch time cost=0.10161423683166504s\n",
            "completed batch 30 of epoch 153. loss=0.040301498025655746. train batch time cost=0.10278630256652832s\n",
            "completed batch 31 of epoch 153. loss=0.008897495456039906. train batch time cost=0.10090184211730957s\n",
            "completed batch 32 of epoch 153. loss=0.03082478791475296. train batch time cost=0.10076427459716797s\n",
            "completed batch 33 of epoch 153. loss=0.0389101468026638. train batch time cost=0.10076642036437988s\n",
            "completed batch 34 of epoch 153. loss=0.02205648645758629. train batch time cost=0.10093069076538086s\n",
            "completed batch 35 of epoch 153. loss=0.007967421784996986. train batch time cost=0.10231614112854004s\n",
            "completed batch 36 of epoch 153. loss=0.02330690622329712. train batch time cost=0.10331416130065918s\n",
            "completed batch 37 of epoch 153. loss=0.05354609340429306. train batch time cost=0.09604954719543457s\n",
            "completed batch 38 of epoch 153. loss=0.01531129889190197. train batch time cost=0.09618949890136719s\n",
            "completed batch 39 of epoch 153. loss=0.040695901960134506. train batch time cost=0.09656310081481934s\n",
            "completed batch 40 of epoch 153. loss=0.0015493185492232442. train batch time cost=0.09560298919677734s\n",
            "completed batch 41 of epoch 153. loss=0.013321190141141415. train batch time cost=0.09469342231750488s\n",
            "completed batch 42 of epoch 153. loss=0.02777082286775112. train batch time cost=0.1009988784790039s\n",
            "completed batch 43 of epoch 153. loss=0.008823676966130733. train batch time cost=0.1007530689239502s\n",
            "completed batch 44 of epoch 153. loss=0.07848332822322845. train batch time cost=0.10144901275634766s\n",
            "completed batch 45 of epoch 153. loss=0.006294684950262308. train batch time cost=0.09613895416259766s\n",
            "completed batch 46 of epoch 153. loss=0.013699544593691826. train batch time cost=0.09605622291564941s\n",
            "completed batch 47 of epoch 153. loss=0.005345348734408617. train batch time cost=0.09631729125976562s\n",
            "completed batch 48 of epoch 153. loss=0.016475185751914978. train batch time cost=0.09559869766235352s\n",
            "completed batch 49 of epoch 153. loss=0.011345678940415382. train batch time cost=0.09627580642700195s\n",
            "completed batch 50 of epoch 153. loss=0.019576646387577057. train batch time cost=0.09668374061584473s\n",
            "completed batch 51 of epoch 153. loss=0.10052438825368881. train batch time cost=0.09510374069213867s\n",
            "completed batch 52 of epoch 153. loss=0.01885148696601391. train batch time cost=0.09493494033813477s\n",
            "completed batch 53 of epoch 153. loss=0.07233653962612152. train batch time cost=0.0963442325592041s\n",
            "completed batch 54 of epoch 153. loss=0.006230033468455076. train batch time cost=0.09592509269714355s\n",
            "completed batch 55 of epoch 153. loss=0.012991083785891533. train batch time cost=0.09619665145874023s\n",
            "completed batch 56 of epoch 153. loss=0.006643571425229311. train batch time cost=0.09627628326416016s\n",
            "completed batch 57 of epoch 153. loss=0.04185868799686432. train batch time cost=0.0964651107788086s\n",
            "completed batch 58 of epoch 153. loss=0.009025488048791885. train batch time cost=0.09579825401306152s\n",
            "completed batch 59 of epoch 153. loss=0.0008657032740302384. train batch time cost=0.09480714797973633s\n",
            "completed batch 60 of epoch 153. loss=0.0247829407453537. train batch time cost=0.09448742866516113s\n",
            "completed batch 61 of epoch 153. loss=0.022239865735173225. train batch time cost=0.09603571891784668s\n",
            "completed batch 62 of epoch 153. loss=0.02737252227962017. train batch time cost=0.09631800651550293s\n",
            "completed batch 63 of epoch 153. loss=0.014007780700922012. train batch time cost=0.09677457809448242s\n",
            "completed batch 64 of epoch 153. loss=0.0905454009771347. train batch time cost=0.09722375869750977s\n",
            "completed batch 65 of epoch 153. loss=0.1574239879846573. train batch time cost=0.09497427940368652s\n",
            "completed batch 66 of epoch 153. loss=0.09058810025453568. train batch time cost=0.09530830383300781s\n",
            "completed batch 67 of epoch 153. loss=0.0028823325410485268. train batch time cost=0.10388541221618652s\n",
            "completed batch 68 of epoch 153. loss=0.025720026344060898. train batch time cost=0.10138893127441406s\n",
            "completed batch 69 of epoch 153. loss=0.01653813198208809. train batch time cost=0.10262751579284668s\n",
            "completed batch 70 of epoch 153. loss=0.006568308919668198. train batch time cost=0.10203170776367188s\n",
            "completed batch 71 of epoch 153. loss=0.007576040457934141. train batch time cost=0.10201621055603027s\n",
            "completed batch 72 of epoch 153. loss=0.002678551012650132. train batch time cost=0.10211873054504395s\n",
            "completed batch 73 of epoch 153. loss=0.06146138533949852. train batch time cost=0.10193490982055664s\n",
            "completed batch 74 of epoch 153. loss=0.033554449677467346. train batch time cost=0.10332131385803223s\n",
            "completed batch 75 of epoch 153. loss=0.0042455922812223434. train batch time cost=0.10190582275390625s\n",
            "completed batch 76 of epoch 153. loss=0.006254015490412712. train batch time cost=0.10305643081665039s\n",
            "completed batch 77 of epoch 153. loss=0.003043060889467597. train batch time cost=0.10215091705322266s\n",
            "completed batch 78 of epoch 153. loss=0.017388759180903435. train batch time cost=0.10296463966369629s\n",
            "completed batch 79 of epoch 153. loss=0.0348329171538353. train batch time cost=0.1026918888092041s\n",
            "completed batch 80 of epoch 153. loss=0.011512824334204197. train batch time cost=0.10270953178405762s\n",
            "completed batch 81 of epoch 153. loss=0.0455489307641983. train batch time cost=0.10210180282592773s\n",
            "completed batch 82 of epoch 153. loss=0.015680484473705292. train batch time cost=0.10279369354248047s\n",
            "completed batch 83 of epoch 153. loss=0.02834659442305565. train batch time cost=0.10224390029907227s\n",
            "completed batch 84 of epoch 153. loss=0.03232372924685478. train batch time cost=0.10339713096618652s\n",
            "completed batch 85 of epoch 153. loss=0.003461513202637434. train batch time cost=0.10444474220275879s\n",
            "completed batch 86 of epoch 153. loss=0.003814645577222109. train batch time cost=0.10181307792663574s\n",
            "completed batch 87 of epoch 153. loss=0.025775721296668053. train batch time cost=0.10275769233703613s\n",
            "completed batch 88 of epoch 153. loss=0.0055534690618515015. train batch time cost=0.10161447525024414s\n",
            "completed batch 89 of epoch 153. loss=0.12342546880245209. train batch time cost=0.10214829444885254s\n",
            "completed batch 90 of epoch 153. loss=0.049880482256412506. train batch time cost=0.10211825370788574s\n",
            "completed batch 91 of epoch 153. loss=0.008644363842904568. train batch time cost=0.10217094421386719s\n",
            "completed batch 92 of epoch 153. loss=0.0024905609898269176. train batch time cost=0.10609912872314453s\n",
            "completed batch 93 of epoch 153. loss=0.03990503400564194. train batch time cost=0.10207152366638184s\n",
            "completed batch 94 of epoch 153. loss=0.036480579525232315. train batch time cost=0.10447072982788086s\n",
            "completed batch 95 of epoch 153. loss=0.03615467622876167. train batch time cost=0.10172462463378906s\n",
            "completed batch 96 of epoch 153. loss=0.16227802634239197. train batch time cost=0.10193419456481934s\n",
            "completed batch 97 of epoch 153. loss=0.005048190709203482. train batch time cost=0.1014549732208252s\n",
            "completed batch 98 of epoch 153. loss=0.010658938437700272. train batch time cost=0.10135912895202637s\n",
            "completed batch 99 of epoch 153. loss=0.003107893979176879. train batch time cost=0.10134315490722656s\n",
            "completed batch 100 of epoch 153. loss=0.043592117726802826. train batch time cost=0.1020355224609375s\n",
            "completed batch 101 of epoch 153. loss=0.01649751514196396. train batch time cost=0.10174751281738281s\n",
            "completed batch 102 of epoch 153. loss=0.10185542702674866. train batch time cost=0.10269546508789062s\n",
            "completed batch 103 of epoch 153. loss=0.012353031896054745. train batch time cost=0.10125923156738281s\n",
            "completed batch 104 of epoch 153. loss=0.019024116918444633. train batch time cost=0.10318803787231445s\n",
            "completed batch 105 of epoch 153. loss=0.035950351506471634. train batch time cost=0.10169482231140137s\n",
            "completed batch 106 of epoch 153. loss=0.025735819712281227. train batch time cost=0.10211658477783203s\n",
            "completed batch 107 of epoch 153. loss=0.01676843874156475. train batch time cost=0.10301661491394043s\n",
            "completed batch 108 of epoch 153. loss=0.029759341850876808. train batch time cost=0.10181593894958496s\n",
            "completed batch 109 of epoch 153. loss=0.026965633034706116. train batch time cost=0.1019601821899414s\n",
            "completed batch 110 of epoch 153. loss=0.04084762930870056. train batch time cost=0.10158658027648926s\n",
            "completed batch 111 of epoch 153. loss=0.010269302874803543. train batch time cost=0.10228848457336426s\n",
            "completed batch 112 of epoch 153. loss=0.04685058444738388. train batch time cost=0.10183358192443848s\n",
            "completed batch 113 of epoch 153. loss=0.007360957097262144. train batch time cost=0.10245251655578613s\n",
            "completed batch 114 of epoch 153. loss=0.011835677549242973. train batch time cost=0.1023416519165039s\n",
            "completed batch 115 of epoch 153. loss=0.005901367869228125. train batch time cost=0.10200834274291992s\n",
            "completed batch 116 of epoch 153. loss=0.05892959609627724. train batch time cost=0.10135579109191895s\n",
            "completed batch 117 of epoch 153. loss=0.015293670818209648. train batch time cost=0.10243082046508789s\n",
            "completed batch 118 of epoch 153. loss=0.040866512805223465. train batch time cost=0.09568428993225098s\n",
            "completed batch 119 of epoch 153. loss=0.010974299162626266. train batch time cost=0.09653544425964355s\n",
            "completed batch 120 of epoch 153. loss=0.02225842885673046. train batch time cost=0.09515929222106934s\n",
            "completed batch 121 of epoch 153. loss=0.0028923137579113245. train batch time cost=0.09497976303100586s\n",
            "completed batch 122 of epoch 153. loss=0.0011720017064362764. train batch time cost=0.0958547592163086s\n",
            "completed batch 123 of epoch 153. loss=0.0073933363892138. train batch time cost=0.09608888626098633s\n",
            "completed batch 124 of epoch 153. loss=0.04121578857302666. train batch time cost=0.0965433120727539s\n",
            "completed batch 125 of epoch 153. loss=0.007975609041750431. train batch time cost=0.09637188911437988s\n",
            "completed batch 126 of epoch 153. loss=2.643315565364901e-05. train batch time cost=0.028345584869384766s\n",
            "completed test of epoch 153. loss=2.643315565364901e-05. accuracy=0.7388916625062406. train one epoch time cost=27.50591731071472s, test validation time cost=3.9690611362457275\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835, 0.655516724912631, 0.655516724912631, 0.6984523215177234, 0.6939590614078882, 0.7134298552171743, 0.6859710434348477, 0.5906140788816775, 0.707938092860709, 0.7398901647528707, 0.6949575636545182, 0.7129306040938592, 0.7383924113829257, 0.7388916625062406]\n",
            "completed batch 1 of epoch 154. loss=0.005165893118828535. train batch time cost=0.09536266326904297s\n",
            "completed batch 2 of epoch 154. loss=0.03412025049328804. train batch time cost=0.09458398818969727s\n",
            "completed batch 3 of epoch 154. loss=0.01177215576171875. train batch time cost=0.09480738639831543s\n",
            "completed batch 4 of epoch 154. loss=0.023979026824235916. train batch time cost=0.09500527381896973s\n",
            "completed batch 5 of epoch 154. loss=0.005057754460722208. train batch time cost=0.09516739845275879s\n",
            "completed batch 6 of epoch 154. loss=0.013709982857108116. train batch time cost=0.09472417831420898s\n",
            "completed batch 7 of epoch 154. loss=0.03400981426239014. train batch time cost=0.09601831436157227s\n",
            "completed batch 8 of epoch 154. loss=0.014220752753317356. train batch time cost=0.09584903717041016s\n",
            "completed batch 9 of epoch 154. loss=0.016211766749620438. train batch time cost=0.0958869457244873s\n",
            "completed batch 10 of epoch 154. loss=0.0012704834807664156. train batch time cost=0.09844017028808594s\n",
            "completed batch 11 of epoch 154. loss=0.022684665396809578. train batch time cost=0.0971980094909668s\n",
            "completed batch 12 of epoch 154. loss=0.012694365344941616. train batch time cost=0.09550619125366211s\n",
            "completed batch 13 of epoch 154. loss=0.005389492493122816. train batch time cost=0.09632992744445801s\n",
            "completed batch 14 of epoch 154. loss=0.014086276292800903. train batch time cost=0.09681868553161621s\n",
            "completed batch 15 of epoch 154. loss=0.03272470459342003. train batch time cost=0.09534764289855957s\n",
            "completed batch 16 of epoch 154. loss=0.004187982529401779. train batch time cost=0.09474515914916992s\n",
            "completed batch 17 of epoch 154. loss=0.014933738857507706. train batch time cost=0.09557604789733887s\n",
            "completed batch 18 of epoch 154. loss=0.1842765212059021. train batch time cost=0.09600043296813965s\n",
            "completed batch 19 of epoch 154. loss=0.013208703137934208. train batch time cost=0.09589076042175293s\n",
            "completed batch 20 of epoch 154. loss=0.00575628224760294. train batch time cost=0.09540462493896484s\n",
            "completed batch 21 of epoch 154. loss=0.04060530662536621. train batch time cost=0.09458446502685547s\n",
            "completed batch 22 of epoch 154. loss=0.07644957304000854. train batch time cost=0.09702682495117188s\n",
            "completed batch 23 of epoch 154. loss=0.0016580974915996194. train batch time cost=0.09490108489990234s\n",
            "completed batch 24 of epoch 154. loss=0.06779301911592484. train batch time cost=0.09401226043701172s\n",
            "completed batch 25 of epoch 154. loss=0.008198295719921589. train batch time cost=0.09451675415039062s\n",
            "completed batch 26 of epoch 154. loss=0.0012239281786605716. train batch time cost=0.09500861167907715s\n",
            "completed batch 27 of epoch 154. loss=0.006942973006516695. train batch time cost=0.09655904769897461s\n",
            "completed batch 28 of epoch 154. loss=0.004237450193613768. train batch time cost=0.09654855728149414s\n",
            "completed batch 29 of epoch 154. loss=0.012233918532729149. train batch time cost=0.09559082984924316s\n",
            "completed batch 30 of epoch 154. loss=0.008576828986406326. train batch time cost=0.09441018104553223s\n",
            "completed batch 31 of epoch 154. loss=0.005588713567703962. train batch time cost=0.09482812881469727s\n",
            "completed batch 32 of epoch 154. loss=0.0036832119803875685. train batch time cost=0.09498071670532227s\n",
            "completed batch 33 of epoch 154. loss=0.021376993507146835. train batch time cost=0.09574007987976074s\n",
            "completed batch 34 of epoch 154. loss=0.014364443719387054. train batch time cost=0.09527802467346191s\n",
            "completed batch 35 of epoch 154. loss=0.005970796570181847. train batch time cost=0.09419822692871094s\n",
            "completed batch 36 of epoch 154. loss=0.01712845079600811. train batch time cost=0.09518313407897949s\n",
            "completed batch 37 of epoch 154. loss=0.021950293332338333. train batch time cost=0.09635686874389648s\n",
            "completed batch 38 of epoch 154. loss=0.005790991708636284. train batch time cost=0.09855484962463379s\n",
            "completed batch 39 of epoch 154. loss=0.03351488336920738. train batch time cost=0.09533333778381348s\n",
            "completed batch 40 of epoch 154. loss=0.004078898578882217. train batch time cost=0.0955953598022461s\n",
            "completed batch 41 of epoch 154. loss=0.004915829747915268. train batch time cost=0.09651470184326172s\n",
            "completed batch 42 of epoch 154. loss=0.007146961055696011. train batch time cost=0.09822344779968262s\n",
            "completed batch 43 of epoch 154. loss=0.008193723857402802. train batch time cost=0.09440851211547852s\n",
            "completed batch 44 of epoch 154. loss=0.0057089803740382195. train batch time cost=0.09619665145874023s\n",
            "completed batch 45 of epoch 154. loss=0.15122966468334198. train batch time cost=0.09957122802734375s\n",
            "completed batch 46 of epoch 154. loss=0.0033580479212105274. train batch time cost=0.09984374046325684s\n",
            "completed batch 47 of epoch 154. loss=0.045145340263843536. train batch time cost=0.0994420051574707s\n",
            "completed batch 48 of epoch 154. loss=0.006570778787136078. train batch time cost=0.09812092781066895s\n",
            "completed batch 49 of epoch 154. loss=0.0023837066255509853. train batch time cost=0.10082077980041504s\n",
            "completed batch 50 of epoch 154. loss=0.002103101462125778. train batch time cost=0.1009056568145752s\n",
            "completed batch 51 of epoch 154. loss=0.003976820036768913. train batch time cost=0.09967207908630371s\n",
            "completed batch 52 of epoch 154. loss=0.010323585942387581. train batch time cost=0.09955096244812012s\n",
            "completed batch 53 of epoch 154. loss=0.007442266680300236. train batch time cost=0.10289168357849121s\n",
            "completed batch 54 of epoch 154. loss=0.004004389978945255. train batch time cost=0.10329318046569824s\n",
            "completed batch 55 of epoch 154. loss=0.013620776124298573. train batch time cost=0.1023416519165039s\n",
            "completed batch 56 of epoch 154. loss=0.007488983683288097. train batch time cost=0.10202765464782715s\n",
            "completed batch 57 of epoch 154. loss=0.008020243607461452. train batch time cost=0.101776123046875s\n",
            "completed batch 58 of epoch 154. loss=0.03780761733651161. train batch time cost=0.1026754379272461s\n",
            "completed batch 59 of epoch 154. loss=0.0024888841435313225. train batch time cost=0.10203695297241211s\n",
            "completed batch 60 of epoch 154. loss=0.015699326992034912. train batch time cost=0.1026155948638916s\n",
            "completed batch 61 of epoch 154. loss=0.013061673380434513. train batch time cost=0.10470223426818848s\n",
            "completed batch 62 of epoch 154. loss=0.10699421167373657. train batch time cost=0.10256266593933105s\n",
            "completed batch 63 of epoch 154. loss=0.0022719521075487137. train batch time cost=0.10123419761657715s\n",
            "completed batch 64 of epoch 154. loss=0.0014095597434788942. train batch time cost=0.10103178024291992s\n",
            "completed batch 65 of epoch 154. loss=0.0038809855468571186. train batch time cost=0.10262465476989746s\n",
            "completed batch 66 of epoch 154. loss=0.0010042238282039762. train batch time cost=0.10391998291015625s\n",
            "completed batch 67 of epoch 154. loss=0.00138263835106045. train batch time cost=0.10039973258972168s\n",
            "completed batch 68 of epoch 154. loss=0.03294006362557411. train batch time cost=0.10183143615722656s\n",
            "completed batch 69 of epoch 154. loss=0.002617561724036932. train batch time cost=0.10276269912719727s\n",
            "completed batch 70 of epoch 154. loss=0.007502082735300064. train batch time cost=0.10042619705200195s\n",
            "completed batch 71 of epoch 154. loss=0.06939344108104706. train batch time cost=0.09510421752929688s\n",
            "completed batch 72 of epoch 154. loss=0.004401915241032839. train batch time cost=0.09691309928894043s\n",
            "completed batch 73 of epoch 154. loss=0.005007580388337374. train batch time cost=0.09699010848999023s\n",
            "completed batch 74 of epoch 154. loss=0.004101415630429983. train batch time cost=0.09669184684753418s\n",
            "completed batch 75 of epoch 154. loss=0.013531124219298363. train batch time cost=0.0964198112487793s\n",
            "completed batch 76 of epoch 154. loss=0.002406470011919737. train batch time cost=0.09578466415405273s\n",
            "completed batch 77 of epoch 154. loss=0.01874239556491375. train batch time cost=0.09537839889526367s\n",
            "completed batch 78 of epoch 154. loss=0.00925020594149828. train batch time cost=0.09673762321472168s\n",
            "completed batch 79 of epoch 154. loss=0.0017876646015793085. train batch time cost=0.09674525260925293s\n",
            "completed batch 80 of epoch 154. loss=0.007365384139120579. train batch time cost=0.09562516212463379s\n",
            "completed batch 81 of epoch 154. loss=0.0219743512570858. train batch time cost=0.0957953929901123s\n",
            "completed batch 82 of epoch 154. loss=0.017089521512389183. train batch time cost=0.09563255310058594s\n",
            "completed batch 83 of epoch 154. loss=0.005049549043178558. train batch time cost=0.09738659858703613s\n",
            "completed batch 84 of epoch 154. loss=0.00553819490596652. train batch time cost=0.09655451774597168s\n",
            "completed batch 85 of epoch 154. loss=0.0026744860224425793. train batch time cost=0.09896111488342285s\n",
            "completed batch 86 of epoch 154. loss=0.06826744228601456. train batch time cost=0.09605836868286133s\n",
            "completed batch 87 of epoch 154. loss=0.0026191058568656445. train batch time cost=0.09601831436157227s\n",
            "completed batch 88 of epoch 154. loss=0.0584401860833168. train batch time cost=0.09760451316833496s\n",
            "completed batch 89 of epoch 154. loss=0.004331373143941164. train batch time cost=0.09592127799987793s\n",
            "completed batch 90 of epoch 154. loss=0.030389005318284035. train batch time cost=0.0942070484161377s\n",
            "completed batch 91 of epoch 154. loss=0.006421440746635199. train batch time cost=0.09520244598388672s\n",
            "completed batch 92 of epoch 154. loss=0.002292824676260352. train batch time cost=0.09534144401550293s\n",
            "completed batch 93 of epoch 154. loss=0.011962294578552246. train batch time cost=0.09739160537719727s\n",
            "completed batch 94 of epoch 154. loss=0.010912518948316574. train batch time cost=0.09482264518737793s\n",
            "completed batch 95 of epoch 154. loss=0.018721818923950195. train batch time cost=0.09539532661437988s\n",
            "completed batch 96 of epoch 154. loss=0.019154535606503487. train batch time cost=0.09596872329711914s\n",
            "completed batch 97 of epoch 154. loss=0.05542968958616257. train batch time cost=0.09656906127929688s\n",
            "completed batch 98 of epoch 154. loss=0.007264274172484875. train batch time cost=0.09562134742736816s\n",
            "completed batch 99 of epoch 154. loss=0.004909296985715628. train batch time cost=0.09598994255065918s\n",
            "completed batch 100 of epoch 154. loss=0.0030557680875062943. train batch time cost=0.09498333930969238s\n",
            "completed batch 101 of epoch 154. loss=0.01676536165177822. train batch time cost=0.09638381004333496s\n",
            "completed batch 102 of epoch 154. loss=0.011982905678451061. train batch time cost=0.10245156288146973s\n",
            "completed batch 103 of epoch 154. loss=0.017007976770401. train batch time cost=0.10203957557678223s\n",
            "completed batch 104 of epoch 154. loss=0.02327747642993927. train batch time cost=0.1065053939819336s\n",
            "completed batch 105 of epoch 154. loss=0.02125261351466179. train batch time cost=0.10156679153442383s\n",
            "completed batch 106 of epoch 154. loss=0.011747936718165874. train batch time cost=0.10223507881164551s\n",
            "completed batch 107 of epoch 154. loss=0.0063944929279387. train batch time cost=0.10310626029968262s\n",
            "completed batch 108 of epoch 154. loss=0.09023467451334. train batch time cost=0.10196161270141602s\n",
            "completed batch 109 of epoch 154. loss=0.018073763698339462. train batch time cost=0.10073542594909668s\n",
            "completed batch 110 of epoch 154. loss=0.0068624066188931465. train batch time cost=0.10078215599060059s\n",
            "completed batch 111 of epoch 154. loss=0.008092266507446766. train batch time cost=0.10282063484191895s\n",
            "completed batch 112 of epoch 154. loss=0.02275536209344864. train batch time cost=0.10257792472839355s\n",
            "completed batch 113 of epoch 154. loss=0.007212039548903704. train batch time cost=0.10185837745666504s\n",
            "completed batch 114 of epoch 154. loss=0.013322927057743073. train batch time cost=0.10240340232849121s\n",
            "completed batch 115 of epoch 154. loss=0.020320318639278412. train batch time cost=0.10271644592285156s\n",
            "completed batch 116 of epoch 154. loss=0.009282227605581284. train batch time cost=0.10370707511901855s\n",
            "completed batch 117 of epoch 154. loss=0.010489840991795063. train batch time cost=0.1028890609741211s\n",
            "completed batch 118 of epoch 154. loss=0.0088997483253479. train batch time cost=0.10282444953918457s\n",
            "completed batch 119 of epoch 154. loss=0.02899620123207569. train batch time cost=0.10264897346496582s\n",
            "completed batch 120 of epoch 154. loss=0.03194861114025116. train batch time cost=0.10345768928527832s\n",
            "completed batch 121 of epoch 154. loss=0.011175590567290783. train batch time cost=0.10232114791870117s\n",
            "completed batch 122 of epoch 154. loss=0.014819315634667873. train batch time cost=0.10042190551757812s\n",
            "completed batch 123 of epoch 154. loss=0.0025519607588648796. train batch time cost=0.1009366512298584s\n",
            "completed batch 124 of epoch 154. loss=0.043511152267456055. train batch time cost=0.10103678703308105s\n",
            "completed batch 125 of epoch 154. loss=0.03426146134734154. train batch time cost=0.10199093818664551s\n",
            "completed batch 126 of epoch 154. loss=1.1662184988381341e-05. train batch time cost=0.030149221420288086s\n",
            "completed test of epoch 154. loss=1.1662184988381341e-05. accuracy=0.7159261108337494. train one epoch time cost=27.2172794342041s, test validation time cost=3.8084871768951416\n",
            "[0.46979530703944083, 0.25062406390414377, 0.21168247628557163, 0.2521218172740889, 0.6290564153769346, 0.26010983524712933, 0.5336994508237644, 0.5931103344982526, 0.6774837743384923, 0.6445332001997004, 0.6230654018971543, 0.6969545681477783, 0.6804792810783824, 0.6954568147778333, 0.6819770344483275, 0.6669995007488767, 0.6739890164752871, 0.6774837743384923, 0.6285571642536195, 0.6804792810783824, 0.6839740389415876, 0.671992011982027, 0.6789815277084373, 0.672491263105342, 0.6714927608587119, 0.6784822765851223, 0.6460309535696456, 0.545681477783325, 0.5946080878681977, 0.599600599101348, 0.6440339490763854, 0.670993509735397, 0.6839740389415876, 0.6460309535696456, 0.6160758861707439, 0.6525212181727409, 0.5736395406889665, 0.5991013479780329, 0.670993509735397, 0.5127309036445332, 0.6220668996505242, 0.582126809785322, 0.6070893659510734, 0.5736395406889665, 0.6145781328007988, 0.6195706440339491, 0.5956065901148277, 0.5287069395906141, 0.5986020968547179, 0.5866200698951572, 0.5706440339490764, 0.5756365451822266, 0.5436844732900649, 0.5771342985521717, 0.5896155766350474, 0.5376934598102846, 0.6090863704443334, 0.5411882176734898, 0.5711432850723914, 0.6045931103344983, 0.5631552670993509, 0.4638042935596605, 0.5941088367448827, 0.5486769845232152, 0.5701447828257613, 0.5881178232651023, 0.528207688467299, 0.44283574638042933, 0.5781328007988018, 0.581627558662007, 0.6205691462805791, 0.34648027958062905, 0.562656015976036, 0.7129306040938592, 0.7029455816275587, 0.7129306040938592, 0.6779830254618073, 0.728407388916625, 0.6145781328007988, 0.7124313529705442, 0.7139291063404892, 0.580629056415377, 0.7319021467798302, 0.507738392411383, 0.6470294558162756, 0.690963554667998, 0.709435846230654, 0.7054418372441338, 0.6874687968047928, 0.7209186220668996, 0.7084373439840239, 0.7114328507239142, 0.708936595107339, 0.7214178731902147, 0.6859710434348477, 0.6645032451323015, 0.6919620569146281, 0.6999500748876685, 0.7264103844233649, 0.708936595107339, 0.7343984023964054, 0.7024463305042437, 0.6325511732401398, 0.6440339490763854, 0.72591113330005, 0.7184223664503245, 0.7019470793809286, 0.7119321018472291, 0.6944583125312032, 0.7174238642036944, 0.7139291063404892, 0.6500249625561657, 0.7044433349975038, 0.6759860209685472, 0.7333999001497753, 0.6829755366949576, 0.7024463305042437, 0.7219171243135297, 0.7254118821767349, 0.7264103844233649, 0.7084373439840239, 0.6859710434348477, 0.7299051422865701, 0.6989515726410385, 0.72690963554668, 0.6984523215177234, 0.654018971542686, 0.6944583125312032, 0.5876185721417874, 0.7124313529705442, 0.7224163754368448, 0.6285571642536195, 0.7164253619570644, 0.7338991512730904, 0.7149276085871193, 0.6789815277084373, 0.6290564153769346, 0.7054418372441338, 0.708936595107339, 0.6874687968047928, 0.7004493260109835, 0.655516724912631, 0.655516724912631, 0.6984523215177234, 0.6939590614078882, 0.7134298552171743, 0.6859710434348477, 0.5906140788816775, 0.707938092860709, 0.7398901647528707, 0.6949575636545182, 0.7129306040938592, 0.7383924113829257, 0.7388916625062406, 0.7159261108337494]\n",
            "completed batch 1 of epoch 155. loss=0.002115425420925021. train batch time cost=0.09910368919372559s\n",
            "completed batch 2 of epoch 155. loss=0.013222419656813145. train batch time cost=0.09654641151428223s\n",
            "completed batch 3 of epoch 155. loss=0.01168617233633995. train batch time cost=0.09654879570007324s\n",
            "completed batch 4 of epoch 155. loss=0.009248103015124798. train batch time cost=0.09509634971618652s\n",
            "completed batch 5 of epoch 155. loss=0.015246950089931488. train batch time cost=0.09637999534606934s\n",
            "completed batch 6 of epoch 155. loss=0.012203123420476913. train batch time cost=0.09594130516052246s\n",
            "completed batch 7 of epoch 155. loss=0.0009128172532655299. train batch time cost=0.09518694877624512s\n",
            "completed batch 8 of epoch 155. loss=0.04144350811839104. train batch time cost=0.09474897384643555s\n",
            "completed batch 9 of epoch 155. loss=0.0867781862616539. train batch time cost=0.09453773498535156s\n",
            "completed batch 10 of epoch 155. loss=0.02216690219938755. train batch time cost=0.09609532356262207s\n",
            "completed batch 11 of epoch 155. loss=0.007364887278527021. train batch time cost=0.0946054458618164s\n",
            "completed batch 12 of epoch 155. loss=0.09971098601818085. train batch time cost=0.09538650512695312s\n",
            "completed batch 13 of epoch 155. loss=0.007613660302013159. train batch time cost=0.09490370750427246s\n",
            "completed batch 14 of epoch 155. loss=0.0684981495141983. train batch time cost=0.09541988372802734s\n",
            "completed batch 15 of epoch 155. loss=0.0026926870923489332. train batch time cost=0.10069894790649414s\n",
            "completed batch 16 of epoch 155. loss=0.0022935480810701847. train batch time cost=0.10115289688110352s\n",
            "completed batch 17 of epoch 155. loss=0.008062834851443768. train batch time cost=0.10108685493469238s\n",
            "completed batch 18 of epoch 155. loss=0.0069724260829389095. train batch time cost=0.10132527351379395s\n",
            "completed batch 19 of epoch 155. loss=0.0871836319565773. train batch time cost=0.10194253921508789s\n",
            "completed batch 20 of epoch 155. loss=0.018095238134264946. train batch time cost=0.10150265693664551s\n",
            "completed batch 21 of epoch 155. loss=0.04343543201684952. train batch time cost=0.10162162780761719s\n",
            "completed batch 22 of epoch 155. loss=0.018241792917251587. train batch time cost=0.1011199951171875s\n",
            "completed batch 23 of epoch 155. loss=0.01265711896121502. train batch time cost=0.10195088386535645s\n",
            "completed batch 24 of epoch 155. loss=0.007980970665812492. train batch time cost=0.10362386703491211s\n",
            "completed batch 25 of epoch 155. loss=0.014328696765005589. train batch time cost=0.10228848457336426s\n",
            "completed batch 26 of epoch 155. loss=0.014269073493778706. train batch time cost=0.09940719604492188s\n",
            "completed batch 27 of epoch 155. loss=0.010994017124176025. train batch time cost=0.09420967102050781s\n",
            "completed batch 28 of epoch 155. loss=0.02128610759973526. train batch time cost=0.09615015983581543s\n",
            "completed batch 29 of epoch 155. loss=0.030767321586608887. train batch time cost=0.096923828125s\n",
            "completed batch 30 of epoch 155. loss=0.0014932277845218778. train batch time cost=0.09642243385314941s\n",
            "completed batch 31 of epoch 155. loss=0.00833431351929903. train batch time cost=0.09510064125061035s\n",
            "completed batch 32 of epoch 155. loss=0.008112757466733456. train batch time cost=0.09514617919921875s\n",
            "completed batch 33 of epoch 155. loss=0.07974810153245926. train batch time cost=0.09627199172973633s\n",
            "completed batch 34 of epoch 155. loss=0.028221581131219864. train batch time cost=0.09585428237915039s\n",
            "completed batch 35 of epoch 155. loss=0.04502382129430771. train batch time cost=0.09615087509155273s\n",
            "completed batch 36 of epoch 155. loss=0.013542945496737957. train batch time cost=0.09580540657043457s\n",
            "completed batch 37 of epoch 155. loss=0.0017793766455724835. train batch time cost=0.09588789939880371s\n",
            "completed batch 38 of epoch 155. loss=0.008801693096756935. train batch time cost=0.09711933135986328s\n",
            "completed batch 39 of epoch 155. loss=0.005270813126116991. train batch time cost=0.09578704833984375s\n",
            "completed batch 40 of epoch 155. loss=0.0035090346354991198. train batch time cost=0.09572052955627441s\n",
            "completed batch 41 of epoch 155. loss=0.006693831644952297. train batch time cost=0.09599971771240234s\n",
            "completed batch 42 of epoch 155. loss=0.01662653125822544. train batch time cost=0.09624600410461426s\n",
            "completed batch 43 of epoch 155. loss=0.005376523360610008. train batch time cost=0.09873771667480469s\n",
            "completed batch 44 of epoch 155. loss=0.015683677047491074. train batch time cost=0.0959479808807373s\n",
            "completed batch 45 of epoch 155. loss=0.00043557691969908774. train batch time cost=0.09608101844787598s\n",
            "completed batch 46 of epoch 155. loss=0.008544947020709515. train batch time cost=0.0944676399230957s\n",
            "completed batch 47 of epoch 155. loss=0.0172272901982069. train batch time cost=0.09531402587890625s\n",
            "completed batch 48 of epoch 155. loss=0.06073038652539253. train batch time cost=0.09717416763305664s\n",
            "completed batch 49 of epoch 155. loss=0.009956207126379013. train batch time cost=0.09563922882080078s\n",
            "completed batch 50 of epoch 155. loss=0.008397437632083893. train batch time cost=0.09575057029724121s\n",
            "completed batch 51 of epoch 155. loss=0.0040156058967113495. train batch time cost=0.09724950790405273s\n",
            "completed batch 52 of epoch 155. loss=0.011013606563210487. train batch time cost=0.09641599655151367s\n",
            "completed batch 53 of epoch 155. loss=0.001332229352556169. train batch time cost=0.09582877159118652s\n",
            "completed batch 54 of epoch 155. loss=0.005115747917443514. train batch time cost=0.09508681297302246s\n",
            "completed batch 55 of epoch 155. loss=0.02017388492822647. train batch time cost=0.09553337097167969s\n",
            "completed batch 56 of epoch 155. loss=0.06211869418621063. train batch time cost=0.09613490104675293s\n",
            "completed batch 57 of epoch 155. loss=0.023506121709942818. train batch time cost=0.09717941284179688s\n",
            "completed batch 58 of epoch 155. loss=0.00794596504420042. train batch time cost=0.09596538543701172s\n",
            "completed batch 59 of epoch 155. loss=0.026629900559782982. train batch time cost=0.09442138671875s\n",
            "completed batch 60 of epoch 155. loss=0.0023098192177712917. train batch time cost=0.09512448310852051s\n",
            "completed batch 61 of epoch 155. loss=0.0069038099609315395. train batch time cost=0.09656214714050293s\n",
            "completed batch 62 of epoch 155. loss=0.007914785295724869. train batch time cost=0.09756255149841309s\n",
            "completed batch 63 of epoch 155. loss=0.004112742375582457. train batch time cost=0.09524893760681152s\n",
            "completed batch 64 of epoch 155. loss=0.0017653267132118344. train batch time cost=0.09764242172241211s\n",
            "completed batch 65 of epoch 155. loss=0.0051628341898322105. train batch time cost=0.09822201728820801s\n",
            "completed batch 66 of epoch 155. loss=0.002403953345492482. train batch time cost=0.09566783905029297s\n",
            "completed batch 67 of epoch 155. loss=0.0021458701230585575. train batch time cost=0.09502196311950684s\n",
            "completed batch 68 of epoch 155. loss=0.0011449824087321758. train batch time cost=0.09612512588500977s\n",
            "completed batch 69 of epoch 155. loss=0.022924767807126045. train batch time cost=0.09510183334350586s\n",
            "completed batch 70 of epoch 155. loss=0.1716192364692688. train batch time cost=0.09613776206970215s\n",
            "completed batch 71 of epoch 155. loss=0.028708986937999725. train batch time cost=0.09651470184326172s\n",
            "completed batch 72 of epoch 155. loss=0.0063107917085289955. train batch time cost=0.0974283218383789s\n",
            "completed batch 73 of epoch 155. loss=0.02506503090262413. train batch time cost=0.10202431678771973s\n",
            "completed batch 74 of epoch 155. loss=0.014547368511557579. train batch time cost=0.10228729248046875s\n",
            "completed batch 75 of epoch 155. loss=0.002613110700622201. train batch time cost=0.10262417793273926s\n",
            "completed batch 76 of epoch 155. loss=0.002761112991720438. train batch time cost=0.10110616683959961s\n",
            "completed batch 77 of epoch 155. loss=0.03493465855717659. train batch time cost=0.10131454467773438s\n",
            "completed batch 78 of epoch 155. loss=0.02901070937514305. train batch time cost=0.1003260612487793s\n",
            "completed batch 79 of epoch 155. loss=0.006593761965632439. train batch time cost=0.10301876068115234s\n",
            "completed batch 80 of epoch 155. loss=0.011399398557841778. train batch time cost=0.10379528999328613s\n",
            "completed batch 81 of epoch 155. loss=0.10715068876743317. train batch time cost=0.10239744186401367s\n",
            "completed batch 82 of epoch 155. loss=0.0468498170375824. train batch time cost=0.11119222640991211s\n",
            "completed batch 83 of epoch 155. loss=0.022863416001200676. train batch time cost=0.10221219062805176s\n",
            "completed batch 84 of epoch 155. loss=0.020184023305773735. train batch time cost=0.10171747207641602s\n",
            "completed batch 85 of epoch 155. loss=0.034638773649930954. train batch time cost=0.10235714912414551s\n",
            "completed batch 86 of epoch 155. loss=0.00627535954117775. train batch time cost=0.10086989402770996s\n",
            "completed batch 87 of epoch 155. loss=0.009116447530686855. train batch time cost=0.10132408142089844s\n",
            "completed batch 88 of epoch 155. loss=0.013608245179057121. train batch time cost=0.10128998756408691s\n",
            "completed batch 89 of epoch 155. loss=0.008771531283855438. train batch time cost=0.1024942398071289s\n",
            "completed batch 90 of epoch 155. loss=0.004053822252899408. train batch time cost=0.09845447540283203s\n",
            "completed batch 91 of epoch 155. loss=0.005596408154815435. train batch time cost=0.09485054016113281s\n",
            "completed batch 92 of epoch 155. loss=0.039515405893325806. train batch time cost=0.09460616111755371s\n",
            "completed batch 93 of epoch 155. loss=0.018268700689077377. train batch time cost=0.09563827514648438s\n",
            "completed batch 94 of epoch 155. loss=0.017973264679312706. train batch time cost=0.09593963623046875s\n",
            "completed batch 95 of epoch 155. loss=0.006945164408534765. train batch time cost=0.09556722640991211s\n",
            "completed batch 96 of epoch 155. loss=0.0783194899559021. train batch time cost=0.09533333778381348s\n",
            "completed batch 97 of epoch 155. loss=0.02156669832766056. train batch time cost=0.09620308876037598s\n",
            "completed batch 98 of epoch 155. loss=0.008013278245925903. train batch time cost=0.10315084457397461s\n",
            "completed batch 99 of epoch 155. loss=0.008655022829771042. train batch time cost=0.10187625885009766s\n",
            "completed batch 100 of epoch 155. loss=0.03453892469406128. train batch time cost=0.10080957412719727s\n",
            "completed batch 101 of epoch 155. loss=0.009943013079464436. train batch time cost=0.10073304176330566s\n",
            "completed batch 102 of epoch 155. loss=0.011008957400918007. train batch time cost=0.10112547874450684s\n",
            "completed batch 103 of epoch 155. loss=0.01535420585423708. train batch time cost=0.10152149200439453s\n",
            "completed batch 104 of epoch 155. loss=0.013336196541786194. train batch time cost=0.10101795196533203s\n",
            "completed batch 105 of epoch 155. loss=0.0043796030804514885. train batch time cost=0.10159087181091309s\n",
            "completed batch 106 of epoch 155. loss=0.010749155655503273. train batch time cost=0.10439658164978027s\n",
            "completed batch 107 of epoch 155. loss=0.005461640655994415. train batch time cost=0.10258126258850098s\n",
            "completed batch 108 of epoch 155. loss=0.014798490330576897. train batch time cost=0.10076761245727539s\n",
            "completed batch 109 of epoch 155. loss=0.010769408196210861. train batch time cost=0.0950937271118164s\n",
            "completed batch 110 of epoch 155. loss=0.05362227186560631. train batch time cost=0.09520912170410156s\n"
          ]
        }
      ],
      "source": [
        "# train and test for each epoch\n",
        "epoch = 280\n",
        "batch_size = 64\n",
        "loss_train_history_list = []\n",
        "acc_test_list = []\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "sum_time_cost_train = 0\n",
        "for ep in range(epoch):\n",
        "    i = 0\n",
        "    batch_loss_list = []\n",
        "    t0 = time.time()\n",
        "    while i<len(y_train):\n",
        "        t1 = time.time()\n",
        "\n",
        "        # x_train_tensor_batch = x_train_tensor[i:i+batch_size]\n",
        "        # y_train_tensor_batch = y_train_tensor[i:i+batch_size]\n",
        "        x_train_tensor_batch = x_train[i:i+batch_size]\n",
        "        y_train_tensor_batch = y_train[i:i+batch_size]\n",
        "        x_train_tensor_batch = torch.tensor(x_train_tensor_batch, dtype=torch.float32).to(device)\n",
        "        y_train_tensor_batch = torch.tensor(y_train_tensor_batch, dtype=torch.long).to(device)\n",
        "\n",
        "        # STEP-01: train\n",
        "        model.train()\n",
        "        # predict\n",
        "        y_train_pred = model(x_train_tensor_batch)\n",
        "        # loss\n",
        "        loss = loss_function(y_train_pred, y_train_tensor_batch)\n",
        "        batch_loss_list.append(loss)\n",
        "        # gradient decent\n",
        "        optimizer_function.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_function.step()\n",
        "        i = i+batch_size\n",
        "        t2 = time.time()\n",
        "        print('completed batch {0} of epoch {1}. loss={2}. train batch time cost={3}s'.format(i//batch_size, ep, loss, t2-t1))\n",
        "    t3 = time.time()\n",
        "    sum_time_cost_train += t3-t0\n",
        "\n",
        "    # STEP-02: validation\n",
        "    loss_ave = sum(batch_loss_list)/len(batch_loss_list)\n",
        "    loss_train_history_list.append(loss_ave)\n",
        "\n",
        "    # Test\n",
        "    model.eval()\n",
        "    asum = 0\n",
        "    j=0\n",
        "    with torch.no_grad():\n",
        "        while j < len(y_test):\n",
        "            x_test_batch = x_test[j:j+batch_size]\n",
        "            y_test_batch = y_test[j:j+batch_size]\n",
        "            x_test_tensor_batch = torch.tensor(x_test_batch, dtype=torch.float32).to(device)\n",
        "            y_test_tensor_batch = torch.tensor(y_test_batch, dtype=torch.long).to(device)\n",
        "\n",
        "            y_test_pred_batch = model(x_test_tensor_batch)\n",
        "            y_test_pred_batch = y_test_pred_batch.cpu().detach().numpy()\n",
        "            y_test_pred_batch = numpy.argmax(y_test_pred_batch, axis=1)\n",
        "\n",
        "            for k in range(len(y_test_pred_batch)):\n",
        "                if y_test_pred_batch[k]==y_test_batch[k]:\n",
        "                    asum += 1\n",
        "\n",
        "            j = j+batch_size\n",
        "\n",
        "        t4 = time.time()\n",
        "        acc_test = asum/len(y_test)\n",
        "        print('completed test of epoch {0}. loss={1}. accuracy={2}. train one epoch time cost={3}s, test validation time cost={4}'.format(ep,loss, acc_test, t3-t0, t4-t3))\n",
        "        acc_test_list.append(acc_test)\n",
        "        print(acc_test_list)\n",
        "\n",
        "\n",
        "print(sum_time_cost_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YChX-eoPlQeA"
      },
      "outputs": [],
      "source": [
        "# Define the folder path\n",
        "dir_root = '/content/drive/MyDrive/Colab Notebooks/Results/Task 1/1/280'\n",
        "if not os.path.exists(dir_root):\n",
        "    os.makedirs(dir_root)\n",
        "\n",
        "loss_train_history_list_txt = os.path.join(dir_root, 'loss_train_history_list.txt')\n",
        "acc_test_list_txt = os.path.join(dir_root, 'acc_test_list.txt')\n",
        "\n",
        "# Open file in write mode\n",
        "with open(loss_train_history_list_txt, 'w') as file:\n",
        "    for item in loss_train_history_list:\n",
        "        file.write(str(item) + '\\n')\n",
        "\n",
        "with open(acc_test_list_txt, 'w') as file:\n",
        "    for item in acc_test_list:\n",
        "        file.write(str(item) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz4rQkgEdIUT"
      },
      "outputs": [],
      "source": [
        "final_accuracy = acc_test_list[-1]\n",
        "print(f'Final model accuracy: {final_accuracy:.5f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raYEFCPBbLHi"
      },
      "outputs": [],
      "source": [
        "torch.save(model, os.path.join(dir_root, 'model_resnet18_task1.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkaArQtmGc12"
      },
      "source": [
        "## Plot the training loss curve and accuracy curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwVTaBjFGcO_"
      },
      "outputs": [],
      "source": [
        "loss_train_history_list_txt =os.path.join(dir_root, 'loss_train_history_list.txt')\n",
        "\n",
        "x_list = []\n",
        "loss_train_history_list = []\n",
        "# Open file in write mode\n",
        "i = 0\n",
        "with open(loss_train_history_list_txt, 'r') as fr:\n",
        "    for line in fr:\n",
        "        i += 1\n",
        "        line = line.strip()\n",
        "        token = float( line.split('(')[1].split(',')[0] )\n",
        "        loss_train_history_list.append(token)\n",
        "        x_list.append(i)\n",
        "\n",
        "# Plotting training loss curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x_list, loss_train_history_list, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(dir_root,'training_loss_curve.png'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o0Qz2y9IpSk"
      },
      "outputs": [],
      "source": [
        "acc_test_list_txt = os.path.join(dir_root, 'acc_test_list.txt')\n",
        "x_list = []\n",
        "acc_test_history_list = []\n",
        "# Open file in write mode\n",
        "i = 0\n",
        "with open(acc_test_list_txt, 'r') as fr:\n",
        "    for line in fr:\n",
        "        i += 1\n",
        "        line = line.strip()\n",
        "        token = float( line )\n",
        "        acc_test_history_list.append(token)\n",
        "        x_list.append(i)\n",
        "\n",
        "\n",
        "# Plotting test accuracy curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x_list, acc_test_history_list, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(dir_root,'test_accuracy_curve.png'))\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}