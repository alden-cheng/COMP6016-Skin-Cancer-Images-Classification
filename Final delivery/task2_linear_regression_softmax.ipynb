{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7IvLdJzqJXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97a50e86-b848-458a-c12d-701c8e0ecff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Process"
      ],
      "metadata": {
        "id": "Yk20oUI8quZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "from tqdm import *\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import torch\n",
        "from torchvision.models import resnet18, resnet34, resnet50"
      ],
      "metadata": {
        "id": "T3zlFuc-qxoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_root = '/content/drive/MyDrive/Datasets/HAM10000/ham10000_equalizeHist'\n",
        "label_list = []\n",
        "label_to_image_path_list = {}\n",
        "label_to_int = {}\n",
        "int_to_label = {}"
      ],
      "metadata": {
        "id": "Y5mjSTtGq0S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load meta data\n",
        "cnt = 0\n",
        "for f1 in os.listdir(data_root):\n",
        "    label = f1\n",
        "    label_to_int[label] = cnt\n",
        "    int_to_label[cnt] = label\n",
        "    cnt += 1\n",
        "    label_list.append(label)\n",
        "    f2 = os.path.join(data_root, f1)\n",
        "    label_to_image_path_list[label] = []\n",
        "    for f3 in os.listdir(f2):\n",
        "        f4 = os.path.join(f2, f3)\n",
        "        label_to_image_path_list[label].append(f4)\n",
        "\n",
        "print( 'label_to_int={0}'.format(label_to_int) )\n",
        "print( 'int_to_label={0}'.format(int_to_label) )"
      ],
      "metadata": {
        "id": "161klm8pq3dA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f438142a-1eab-4abe-dbd7-a1247e3732ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_to_int={'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'vasc': 5, 'nv': 6}\n",
            "int_to_label={0: 'akiec', 1: 'bcc', 2: 'bkl', 3: 'df', 4: 'mel', 5: 'vasc', 6: 'nv'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load image to color images\n",
        "label_to_image_arr_list = {}\n",
        "for label in label_to_image_path_list:\n",
        "    label_to_image_arr_list[label] = []\n",
        "    image_path_list = label_to_image_path_list[label]\n",
        "    for image_path in tqdm(image_path_list):\n",
        "        img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "        label_to_image_arr_list[label].append(img)\n",
        "\n",
        "# print load status\n",
        "for label in label_to_image_arr_list:\n",
        "    print('{0}: loaded {1} images, shape={2}'.format(label, len(label_to_image_arr_list[label]),\n",
        "                                                    label_to_image_arr_list[label][0].shape))"
      ],
      "metadata": {
        "id": "cLYaVtyzq7cZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40cb44f3-1ae1-4fa0-b44f-83838e389b56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 327/327 [00:10<00:00, 30.78it/s]\n",
            "100%|██████████| 514/514 [04:58<00:00,  1.72it/s]\n",
            "100%|██████████| 1099/1099 [10:15<00:00,  1.79it/s]\n",
            "100%|██████████| 115/115 [01:06<00:00,  1.74it/s]\n",
            "100%|██████████| 1113/1113 [07:56<00:00,  2.33it/s]\n",
            "100%|██████████| 142/142 [01:04<00:00,  2.21it/s]\n",
            "100%|██████████| 6705/6705 [1:00:11<00:00,  1.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "akiec: loaded 327 images, shape=(450, 600, 3)\n",
            "bcc: loaded 514 images, shape=(450, 600, 3)\n",
            "bkl: loaded 1099 images, shape=(450, 600, 3)\n",
            "df: loaded 115 images, shape=(450, 600, 3)\n",
            "mel: loaded 1113 images, shape=(450, 600, 3)\n",
            "vasc: loaded 142 images, shape=(450, 600, 3)\n",
            "nv: loaded 6705 images, shape=(450, 600, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# images to x_data, y_data\n",
        "x_data = []\n",
        "y_data = []\n",
        "for label in label_to_image_arr_list:\n",
        "    for img in label_to_image_arr_list[label]:\n",
        "        x_data.append( img )\n",
        "        y_data.append( label_to_int[label] )\n",
        "\n",
        "\n",
        "x_data = numpy.array(x_data)\n",
        "y_data = numpy.array(y_data)\n",
        "print( 'x_data.shape={0}, y_data.shape={1}'.format(x_data.shape, y_data.shape) )"
      ],
      "metadata": {
        "id": "UwZCDvz0q_-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear model with softmax"
      ],
      "metadata": {
        "id": "1QFIm_aXrBwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "class_count = len(label_to_image_arr_list)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "GDfL2wdOrJvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear model with softmax\n",
        "class LinearSoftmaxModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearSoftmaxModel, self).__init__()\n",
        "        self.linear = torch.nn.Linear(810000, class_count)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "metadata": {
        "id": "qx1Rj3DnrMEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearSoftmaxModel().to(device)"
      ],
      "metadata": {
        "id": "2vJP2WXjrQau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer_function = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "-8I2-ncYrSSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The training and test sets are split in the ratio of 8:2"
      ],
      "metadata": {
        "id": "KVQmy-SCrVL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RGB image into a vector\n",
        "n_channel = x_data.shape[-1]\n",
        "n_count = x_data.shape[0]\n",
        "width = x_data.shape[1]\n",
        "height = x_data.shape[2]\n",
        "x_data = x_data.reshape( (n_count,n_channel*width*height)  )"
      ],
      "metadata": {
        "id": "07XAZnUmrXc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split( x_data, y_data,\n",
        "                                                    test_size=0.2, random_state=42 )\n",
        "# output\n",
        "print('x_train={0}, x_test={1}, y_train={2}, y_test={3}'.format(\n",
        "    x_train.shape, x_test.shape, y_train.shape, y_test.shape\n",
        ") )"
      ],
      "metadata": {
        "id": "PBxhXqDxrd_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_acc(y_pred, y_true):\n",
        "    asum = 0\n",
        "    for i in range(len(y_pred)):\n",
        "        if y_pred[i]==y_true[i]:\n",
        "            asum += 1\n",
        "    return asum/len(y_pred)"
      ],
      "metadata": {
        "id": "dSDiYq07rfkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and test for each epoch\n",
        "epoch = 10\n",
        "batch_size = 64\n",
        "loss_train_history_list = []\n",
        "acc_test_list = []\n",
        "model.to(device)\n",
        "\n",
        "sum_time_cost_train = 0\n",
        "for ep in range(epoch):\n",
        "    i = 0\n",
        "    batch_loss_list = []\n",
        "    t0 = time.time()\n",
        "    while i<len(y_train):\n",
        "        t1 = time.time()\n",
        "\n",
        "        # x_train_tensor_batch = x_train_tensor[i:i+batch_size]\n",
        "        # y_train_tensor_batch = y_train_tensor[i:i+batch_size]\n",
        "        x_train_tensor_batch = x_train[i:i+batch_size]\n",
        "        y_train_tensor_batch = y_train[i:i+batch_size]\n",
        "        x_train_tensor_batch = torch.tensor(x_train_tensor_batch, dtype=torch.float32).to(device)\n",
        "        y_train_tensor_batch = torch.tensor(y_train_tensor_batch, dtype=torch.long).to(device)\n",
        "\n",
        "        # STEP-01: train\n",
        "        model.train()\n",
        "        # predict\n",
        "        y_train_pred = model(x_train_tensor_batch)\n",
        "        # loss\n",
        "        loss = loss_function(y_train_pred, y_train_tensor_batch)\n",
        "        batch_loss_list.append(loss)\n",
        "        # gradient decent\n",
        "        optimizer_function.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_function.step()\n",
        "        i = i+batch_size\n",
        "        t2 = time.time()\n",
        "        print('completed batch {0} of epoch {1}. loss={2}. train batch time cost={3}s'.format(i//batch_size, ep, loss, t2-t1))\n",
        "    t3 = time.time()\n",
        "    sum_time_cost_train += t3-t0\n",
        "\n",
        "    # STEP-02: validation\n",
        "    loss_ave = sum(batch_loss_list)/len(batch_loss_list)\n",
        "    loss_train_history_list.append(loss_ave)\n",
        "\n",
        "    # Test\n",
        "    model.eval()\n",
        "    asum = 0\n",
        "    j=0\n",
        "    with torch.no_grad():\n",
        "        while j < len(y_test):\n",
        "            x_test_batch = x_test[j:j+batch_size]\n",
        "            y_test_batch = y_test[j:j+batch_size]\n",
        "            x_test_tensor_batch = torch.tensor(x_test_batch, dtype=torch.float32).to(device)\n",
        "            y_test_tensor_batch = torch.tensor(y_test_batch, dtype=torch.long).to(device)\n",
        "\n",
        "            y_test_pred_batch = model(x_test_tensor_batch)\n",
        "            y_test_pred_batch = y_test_pred_batch.cpu().detach().numpy()\n",
        "            y_test_pred_batch = numpy.argmax(y_test_pred_batch, axis=1)\n",
        "\n",
        "            for k in range(len(y_test_pred_batch)):\n",
        "                if y_test_pred_batch[k]==y_test_batch[k]:\n",
        "                    asum += 1\n",
        "\n",
        "            j = j+batch_size\n",
        "\n",
        "        t4 = time.time()\n",
        "        acc_test = asum/len(y_test)\n",
        "        print('completed test of epoch {0}. loss={1}. accuracy={2}. train one epoch time cost={3}s, test validation time cost={4}'.format(ep,loss, acc_test, t3-t0, t4-t3))\n",
        "        acc_test_list.append(acc_test)\n",
        "        print(acc_test_list)\n",
        "\n",
        "print(sum_time_cost_train)"
      ],
      "metadata": {
        "id": "8vkm1I9tR4Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder path\n",
        "dir_root = '/content/drive/MyDrive/Colab Notebooks/Results/Task 2/2/10'\n",
        "if not os.path.exists(dir_root):\n",
        "    os.makedirs(dir_root)\n",
        "\n",
        "loss_train_history_list_txt = os.path.join(dir_root, 'loss_train_history_list.txt')\n",
        "acc_test_list_txt = os.path.join(dir_root, 'acc_test_list.txt')\n",
        "\n",
        "# Open file in write mode\n",
        "with open(loss_train_history_list_txt, 'w') as file:\n",
        "    for item in loss_train_history_list:\n",
        "        file.write(str(item) + '\\n')\n",
        "\n",
        "with open(acc_test_list_txt, 'w') as file:\n",
        "    for item in acc_test_list:\n",
        "        file.write(str(item) + '\\n')"
      ],
      "metadata": {
        "id": "BUUHEtR_R9oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_accuracy = acc_test_list[-1]\n",
        "print(f'Final model accuracy: {final_accuracy:.5f}')"
      ],
      "metadata": {
        "id": "SGlu5Wm9SC8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, os.path.join(dir_root, 'model_resnet18_task2.pth'))"
      ],
      "metadata": {
        "id": "IMJOfyCCSE7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot the training loss curve and accuracy curve"
      ],
      "metadata": {
        "id": "xGYzpbPoscy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_train_history_list_txt =os.path.join(dir_root, 'loss_train_history_list.txt')\n",
        "\n",
        "x_list = []\n",
        "loss_train_history_list = []\n",
        "# Open file in write mode\n",
        "i = 0\n",
        "with open(loss_train_history_list_txt, 'r') as fr:\n",
        "    for line in fr:\n",
        "        i += 1\n",
        "        line = line.strip()\n",
        "        token = float( line.split('(')[1].split(',')[0] )\n",
        "        loss_train_history_list.append(token)\n",
        "        x_list.append(i)\n",
        "\n",
        "# Plotting training loss curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x_list, loss_train_history_list, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(dir_root,'training_loss_curve.png'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M0aCOO6msDLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_test_list_txt = os.path.join(dir_root, 'acc_test_list.txt')\n",
        "x_list = []\n",
        "acc_test_history_list = []\n",
        "# Open file in write mode\n",
        "i = 0\n",
        "with open(acc_test_list_txt, 'r') as fr:\n",
        "    for line in fr:\n",
        "        i += 1\n",
        "        line = line.strip()\n",
        "        token = float( line )\n",
        "        acc_test_history_list.append(token)\n",
        "        x_list.append(i)\n",
        "\n",
        "\n",
        "# Plotting test accuracy curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x_list, acc_test_history_list, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Test Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(dir_root,'test_accuracy_curve.png'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jlkwBjL_sa_O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}